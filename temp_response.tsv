	StartDate	EndDate	Status	IPAddress	Progress	Duration (in seconds)	Finished	RecordedDate	ResponseId	RecipientLastName	RecipientFirstName	RecipientEmail	ExternalReference	LocationLatitude	LocationLongitude	DistributionChannel	UserLanguage	Q1.1	Q240	Q241	Q242	Q243	Q5.7	Q5.8	Q5.9	Q5.10	Q5.11	Q5.12	Q6.7	Q6.8	Q6.9	Q6.10	Q6.11	Q6.12	Q8.13	Q9.1	Q10.14	Q10.15	Q10.16	Q10.17	Q10.18	Q10.19	Q10.20	Q10.21	Q199	Q203	Q200	Q204	Q201	Q205	Q11.6	Q11.7	Q11.8	Q11.9	Q11.10	Q11.11	Q11.12	Q11.13	Q11.14	Q11.15	Q11.16	Q11.17	Q11.18	Q11.19	Q11.20	Q11.21	Q11.22	Q11.23	Q11.24	Q11.25	Q11.26	Q11.27	Q11.28	Q11.29	Q11.30	Q12.1	Q12.2	Q12.3	Q163	Q164	PROLIFIC_PID	STUDY_ID	SESSION_ID	group	tradeoff	scenario	x_first	prompt_text	pref_model	fnr_pos	Q105 - Parent Topics	Q105 - Sentiment Polarity	Q105 - Sentiment Score	Q105 - Sentiment	Q105 - Topic Sentiment Label	Q105 - Topic Sentiment Score	Q105 - Topics	PGM	session_id	status	started_datetime	completed_date_time	time_taken	age	num_approvals	num_rejections	prolific_score	reviewed_at_datetime	entered_code	Country of Birth	Current Country of Residence	Employment Status	Ethnicity	First Language	Nationality	Sex	Student Status	Create New Field or Choose From Dropdown...
0	2021-09-17 08:15:35	2021-09-17 08:37:52	IP Address	66.191.188.89	100	1336	True	2021-09-17 08:37:53	R_3NwhQrCQqPnU17Z					35.053497314453125	-82.05819702148438	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	African American's are less likely to be allowed but both White and African Americans are equally likely to have been mistakenly allowed.	Very unbiased	A 24% difference between accepting Whites over African Americans is very significant and will show up frequently if used.	Mostly unusable	I think the bias that it shows is hard to justify the use of model X. If it is shown to have a bias then it would detract those who it shows a bias against.	Mildly unfair	The probability of being allowed payment is very fair and equal. The problem comes from the bias towards Whites being mistakenly allowed when they shouldn't have been. Again there is a 20%+ biased towards one side. 	Mildly unbiased	It shows bias in that it mistakenly allows more Whites than should have been accepted. Thus it allows more Whites as a bias.	Mostly useful	If these figures are more acceptable than manually chosing then even with the bias it could be useful, but personally I think the bias is too big to be completely useful.	Yes	The bias of the 2nd image made me realize the first isn't as biased. The reason being it has an equal amount of error on both sides. While it is still biased, I think it's much more usable with that in mind. As there will always be some error in these sorts of decisions, and having an equal error on both sides is much better than a huge bias towards one or the other.	Probably model X	"The error on both groups are equal so it's more accurate to who ""should"" be allowed payment based off the model."	Probably model Y	It mistakenly approves more Whites than it should, thus taking more away from those who would should be approved.	Probably model X	The error is equal in both so even if the error was fully removed the results would just be equally shifted down on both groups.	Probably model X	The error between the two choices are the same and can be equally removed from both groups.	Probably ${e://Field/pref_model}	The error of mistakenly allowed payments is the same in both groups.	Definitely model Z	The error is more biased towards Whites	Definitely ${e://Field/pref_model}	The error is equal in both groups.			High	From their perspective it's more important than the whole and affects them more altogether.			Moderate	They get the benefits but could feel bad about taking it from someone else who meet the requirements instead.			Low	Mistakes always happen and such things won't effect the society as much as a whole.			Moderate	Allowing more to go out than what was necessary can be more detrimental to the society as it takes away more than if it was mistakenly denied.			Yes			No			Yes			Advantaged	Bachelor	Engineering and Technology	6144933b9fe82d0490f0127e	614391dcdf09127f013fe60a	6144a2fbf86351238933e31b	minority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Advantaged	6144a2fbf86351238933e31b	APPROVED	2021-09-17 14:15:30.317000	2021-09-17 14:37:54.798000	1344.481	25.0	91	1	100	2021-09-20 15:43:52.087000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Male	No	
1	2021-09-17 08:31:48	2021-09-17 08:54:59	IP Address	173.173.68.213	100	1391	True	2021-09-17 08:55:00	R_2SdxZsJrwzRm4Aa					41.429397583007805	-97.36810302734376	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	Based on race and only two races 	Acceptably biased	Again, based on races and there’s only two races listed	Mostly useful	Demonstrates mistakes by machines and the probability of those mistakes.	Neither fair nor unfair	Using two races but good to see the probability of mistakes with different races	Acceptably biased	Only two races listed	Mostly useful	Shows how machines can make mistakes and for which race is more probable to be mistaken.	No		Models X and Y are equally fair	The models are opposites in that one model has more similar mistakes based on the two races and the other has more similar probability of granting access based on the two races. They cancel each other out it seems.	Models X and Y are equally biased	They use the same races and this is all probability.	Models X and Y are equally useful	Similar outcomes with both models 	Neither model X nor model Y	In essence, they have the same outcomes.	Both ${e://Field/pref_model} and Z are equally fair	Similar outcomes on each model, just reversed.	Both ${e://Field/pref_model} and Z are equally biased	Only two races but similar outcomes, but reversed.	Neither ${e://Field/pref_model} nor model Z	Too similar of outcomes. Need fewer mistakes on one of them to decide which is best.		Moderate		Failing to recognize would be better as this would require staff to put in a secure username and password, ensuring proper access. The alternative would be recognizing an unauthorized individual and allowing access not meant for them, breaching security.		High		Allowing access to an unauthorized user would break security and could cause immense problems and safety concerns.		Low		Society would be the community and how they view the safety of this establishment. Society would rather an individual be unrecognized which would then require them to put in a secure username and password that only the individual should know that has access to the facility.		High		Society again would be the community in which the facility is in. Allowing unauthorized access mistakenly would cause society to have safety concerns with records and other individuals and then society would not have trust in this establishment.		No			No			No			Advantaged		Bachelor	Healthcare Practitioner	61230e2ed3554491075cdfab	614391dcdf09127f013fe60a	6144a6cceb72485b446f8f33	minority	outcome-fpr	frauth	True		model X	top								Advantaged	6144a6cceb72485b446f8f33	APPROVED	2021-09-17 14:31:43.849000	2021-09-17 14:55:03.762000	1399.913	28.0	14	0	100	2021-09-20 15:07:46.473000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
2	2021-09-17 09:15:15	2021-09-17 09:26:50	IP Address	71.93.42.5	100	694	True	2021-09-17 09:26:51	R_wSQUPuZnZ1cMPEl					35.46580505371094	-120.66719818115234	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair		Mildly unbiased		Mostly useful		Very unfair	beacause looking nice	Neither biased nor unbiased	equaly	Mostly ununsable	equal	No		Probably model X	equal	Models X and Y are equally biased	equaly same	Models X and Y are equally useful	equal	Neither model X nor model Y	equaly	Probably model Z	equaly not same	Both ${e://Field/pref_model} and Z are equally biased	up and down	Definitely model Z	equaly	Moderate			Equaly	High			Equal	Moderate			Equal	High			Equal	Yes			No			Yes			Advantaged			Master	Education	613fa51c6a01b958c3e7fd91	614391dcdf09127f013fe60a	6144b0f8e3f4ec03d3612c83	minority	outcome-fpr	icu	True		model Y	bottom								Advantaged	6144b0f8e3f4ec03d3612c83	APPROVED	2021-09-17 15:15:09.678000	2021-09-17 15:27:04.573000	714.895	26.0	12	2	90	2021-10-08 15:08:24.286000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Female	Yes	
3	2021-09-17 09:25:23	2021-09-17 09:35:34	IP Address	47.176.58.194	100	611	True	2021-09-17 09:35:34	R_1im0KbCJZ4d3KzK					33.751495361328125	-118.36700439453124	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Male applicants will be more likely to be accepted than female applicants.	Figure 2	Very fair	Model X is merely reporting the data.	Very unbiased		Mostly useful		Very fair		Very unbiased		Neither useful nor unusable		No		Models X and Y are equally fair		Models X and Y are equally biased		Models X and Y are equally useful		Probably model X		Probably ${e://Field/pref_model}		Both ${e://Field/pref_model} and Z are equally biased		Neither ${e://Field/pref_model} nor model Z				High	A person may be unfairly denied. 			High	A company may lose out on money they are owed. 			High	People who may be in need of housing may suffer.			Low	The employees collecting money could get in trouble.			Yes			Yes			Yes			Disadvantaged	Master	Healthcare Practitioner	60ff7ddcdcb2d599f8d33d33	61439223ecf74e491e22a39c	6144b35de5b92714ec4300f6	minority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	6144b35de5b92714ec4300f6	APPROVED	2021-09-17 15:25:19.851000	2021-09-17 15:35:37.462000	617.611	26.0	6	0	100	2021-09-20 16:52:39.690000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	Spanish	United States	Female	No	
4	2021-09-17 09:27:51	2021-09-17 09:45:32	IP Address	174.204.13.101	100	1060	True	2021-09-17 09:45:32	R_PAGBtEdHuw7xATD					40.75920104980469	-111.88749694824217	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 1	Acceptably fair	It is wrong an equal percentage	Mildly unbiased	It is wrong equally as much	Mostly unusable	That’s a high percentage of being wrong	Mildly unfair	It allowed more African Americans to enter than should have compared with whites	Acceptably biased	It allowed more African Americans to enter than should have compared with whites	Completely unusable	It has a high percentage of being wrong	No		Probably model X	I mean Model Y is wrong slightly less, but it is way more biased, so I think X is better	Definitely model Y	It allows a higher percentage of one race to mistakenly come in	Models X and Y are equally useful	I think they’re both not useful cause they’re wrong so much. I guess Y could be more useful if only used for white people	Probably model X	It’s more consistent	Definitely ${e://Field/pref_model}	It allows access incorrectly the same amount	Probably model Z	It allows access incorrectly less consistent across races	Neither ${e://Field/pref_model} nor model Z	Neither really has an advantage		Low		It adds 1 minute to their access time		High		Someone could get in that is not supposed to be there		Low		"Society means everyone
Very low cause they can still get in. Just adds time"		High		"Society meaning everyone. 
They will feel less safe at a hospital if anyone can get in 1 out of 3 times"		Yes			No			Yes			Advantaged		Bachelor	Others	612fef6d81db7e31a0f20692	614391dcdf09127f013fe60a	6144b3f02c93e2c9bd796c78	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Advantaged	6144b3f02c93e2c9bd796c78	APPROVED	2021-09-17 15:27:47.614000	2021-09-17 15:45:40.883000	1073.269	22.0	76	3	93	2021-09-20 15:12:20.531000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Male	Yes	
5	2021-09-17 08:55:22	2021-09-17 09:52:11	IP Address	174.251.66.178	100	3409	True	2021-09-17 09:52:11	R_2SvLtEAb3dhmOI9					40.76210021972656	-73.95169830322266	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Bothe races were equally mistakenly granted.	Neither biased nor unbiased.	It equally mistakenly granted access for both races.	Very useful	It is more of an accurate representation of what a facial recognition can show.	Very unfair	The white group is mistakenly granted more than the African American group 	Acceptably biased	The White group was more mistakenly granted more than the African American group.	Mostly useful	It shows what race was more granted by the recognition system.	No		Probably model X	I find that both races are mistakenly accepted more fairly than the other model.	Probably model Y	They had more people that were mistakenly accepted that were white.	Probably model X	It fairly allows both races to equally be denied in the system.	Probably model X	It equally rejects both races equally with the recognition system.	Definitely ${e://Field/pref_model}	It is more fair that both races were mistakenly granted access.	Probably model Z	The whites were more mistakenly granted access.	Definitely ${e://Field/pref_model}	Both races were mistakenly granted access.		High		If the staff isn’t recognized the they are unable to work and earn for their family.		High		The people that granted access that aren’t supposed to could be highly dangerous and it would be a threat to security.		Low		If they would need to enter a password then it is okay because it is an added layer of security. The staff of the hospital would be considered society.		High		If they are allowed access then it is a big problem because society is affected. It could also be a threat to the company.		Yes			No			Yes			Advantaged		Secondary Education	Others	6143bbd708c3adb5792ef850	614391dcdf09127f013fe60a	6144ac525058ffeee2506d02	minority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	6144ac525058ffeee2506d02	APPROVED	2021-09-17 14:55:18.423000	2021-09-17 15:52:14.657000	3416.234	24.0	13	1	90	2021-09-20 15:41:01.695000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Female	No	
6	2021-09-17 08:59:34	2021-09-17 10:04:02	IP Address	47.78.184.186	100	3867	True	2021-09-17 10:04:02	R_3MmJz5oAOj9DfKc					37.751007080078125	-97.82199859619139	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Most of the white aren't  fraud.	Mildly unbiased	It's present the white people wrongly.	Mostly unusable	This isn't the reality.	Very fair	White people are less fraudster.	Mildly unbiased	It's show the real behavior.	Mostly useful	It matches with the real world.	No		Probably model Y	The graph is more realistic with real world.	Probably model X	Model X doesn't show the realistic people behavior.	Definitely model Y	I think, model Y is the correct visual of real world people behavior.	Definitely model Y	Model Y is more accurate to me.	Probably ${e://Field/pref_model}	White people are less fraudster. So the graph shows correct scenario.	Probably model Z	I think, it doesn't show accurate scenario.	Definitely ${e://Field/pref_model}	It's more accurate to me.			Moderate	Mistakenly denying payment  feel us bad.			High	A fraudulent payment is always frustrating. 			High	"He or she can be in a bad financial and embarrassing situation. Anyone who is indirectly affected by the decision from the automatic decision making system is considered as part of the ""society""."			High	"Mistakenly allowing a fraudulent payment have a bad consequences for people life by allowing fraudsters to make crime. Anyone who is indirectly affected by the decision from the automatic decision making system is considered as part of the ""society""."			No			Yes			No			Disadvantaged	Master		6141e3b4da3382c6b160cb9b	614391dcdf09127f013fe60a	6144acbffd7d3755ba63a07a	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6144acbffd7d3755ba63a07a	APPROVED	2021-09-17 14:59:26.581000	2021-09-17 16:04:11.492000	3884.911	31.0	69	0	100	2021-09-20 15:44:45.372000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
7	2021-09-17 09:19:58	2021-09-17 10:15:48	IP Address	75.3.228.24	100	3350	True	2021-09-17 10:15:48	R_xruNB6oDyiMPrAB					30.142593383789062	-81.57270050048828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The fact that the mistakenly allowed payments are equal indicates that the algorithm is not influenced by race.  The difference in allowed payments by race is likely caused by other factors (i.e. unequal number of renters of different races).	Mildly unbiased	The mistakenly allowed payments are equal across races, meaning that the differences in allowed payments are not caused by bias.	Neither useful nor unusable	Regardless of actual bias, the above example creates a perceived bias which in itself can cause issues.	Mildly unfair	The model is detecting more fraud by African American's than by White people.	Very biased	The model has a bias towards determining that fraud is more likely for African American's	Completely unusable	Any bias based on race that is also shown to disproportionately allow fraud is unusable.	No		Definitely model X	The mistakenly allowed payments graph is the one to be used to measure accuracy, and model X is equal across race in this regard.  The probability of allowing payments has other unbiased variables in play that are not necessarily known (i.e. proportion of White's to African American's).	Definitely model Y	Model Y is mistakenly allowing more fraudulent payments by White people, indicating a bias favoring them.	Definitely model X	Model X is more useful due to its lack of bias.  Model Y is not only bias, but that bias is causing more fraudulent payments to be missed, thereby rendering the model less useable for its intended purpose. 	Definitely model X	Model X shows less racial bias than model Y, and unbiased data is inherently more useable when that bias causes disproportionate failures in detecting the fraudulent payments.	Both ${e://Field/pref_model} and Z are equally fair	Determining whether the disproportionate data indicates racial bias in the mistakenly denied payments or the mistakenly allowed payments (when the opposite factor remains constant in each instance) is difficult to ascertain without knowing the ratio of White's to African American's.	Both ${e://Field/pref_model} and Z are equally biased	Determining whether the disproportionate data indicates racial bias in the mistakenly denied payments or the mistakenly allowed payments (when the opposite factor remains constant in each instance) is difficult to ascertain without knowing the ratio of White's to African American's.	Neither ${e://Field/pref_model} nor model Z	I would not make this choice without acquiring more data such as the proportion of White's to African American's in the data set.			Low	A landlord cannot immediately evict, so the renter will likely have a subsequent chance to remedy the situation and make the payment without further consequence.			High	If the individual is making fraudulent payments (regardless of whether those payments are mistakenly allowed), then presumably they will inevitably be caught at some point and face the repercussions of those crimes.			Low	If someone's payment is mistakenly denied, as previously noted they should have a chance to subsequently make that payment before any negative consequences occur.			High	The fraudulent payment is financially impacting the victim of that fraud, so allowing it is taking money from the wrong person.  Taking the payment from someone else's bank account without them expecting it can have severe immediate financial ramifications for that victim (i.e. inability to pay their own bills).			No			Yes			Yes			Advantaged	Secondary Education	Business	5e687cd66eb5dd12b8532767	614391dcdf09127f013fe60a	6144b217c132338b2b5a81fe	minority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Advantaged	6144b217c132338b2b5a81fe	APPROVED	2021-09-17 15:19:56.185000	2021-09-17 16:15:50.875000	3354.69	33.0	101	0	100	2021-09-20 14:40:57.693000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Male	DATA EXPIRED	
8	2021-09-17 09:50:57	2021-09-17 10:26:02	IP Address	68.172.212.83	100	2104	True	2021-09-17 10:26:02	R_3fNebfDQnRh7aCt					40.691497802734375	-73.85330200195312	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It is more likely to accept payment from one race over the other.	Acceptably biased	It is more likely to accept payments from African American users.	Mostly unusable	The acceptance rate should match the rate of fraud detection.	Mildly unfair	It is more likely to mistakenly allow payment from those of one race vs another.	Acceptably biased	It is more likely to perceive payments from African Americans as fraudulent.	Mostly ununsable	The probability of detecting fraudulent payments should be more in line with the acceptance rate.	No		Models X and Y are equally fair	They both have biases that effect the amount of accepted or fraudulent transactions.	Models X and Y are equally biased	They both have biases that reflect differently per race of user.	Models X and Y are equally useful	Both need to be adjusted so that their biases are reduced or eliminated.	Neither model X nor model Y	Both are unfair.	Definitely model Z	The rate of mistakenly accepted payment is the same.	Definitely ${e://Field/pref_model}	It is more likely to perceive payments from White users as fraudulent.	Definitely model Z	The rate of mistakenly accepted payments is the same between races of users.			High	Within this context, a denied payment can result in the loss of a housing opportunity.			Low	The individual is able to proceed with their transaction despite using fraudulent means.			Moderate	Denial of transaction can result in the landlord not having a tenant for longer than anticipated.			High	The fraudulent payment has a significant impact on whoever's account had been used to make the transaction.			Yes			Yes			Yes			Disadvantaged	Bachelor	Others	613a51b0aeafc3796184cdbc	61439223ecf74e491e22a39c	6144b95acae9bfde542d1630	majority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	6144b95acae9bfde542d1630	APPROVED	2021-09-17 15:50:55.982000	2021-09-17 16:26:04.933000	2108.951	23.0	48	0	100	2021-09-20 15:56:59.918000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
9	2021-09-17 09:43:40	2021-09-17 10:35:12	IP Address	172.58.71.176	100	3091	True	2021-09-17 10:35:12	R_1FM9ZRstB6ob4D1					32.779693603515625	-96.8022003173828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	This model is based on skin color.	Mildly unbiased	I think it should not be based on race unless the current condition is more present in that demographic.	Neither useful nor unusable	I would not know unless the conditions were revealed. For example at the start of Covid African Americans was more likely to need more care so this model would be correct	Neither fair nor unfair	This model gave more people a chance and it showed that more data is needed to make a decision of who needed ICU support.	Neither biased nor unbiased	Not really each group was given the same probability.	Mostly useful	Yes, It shows that sometimes models can be off by a lot.	Yes	None I think the models were based on a prediction that is had for anyone to predict.	Probably model Y	I selected Y because the model was fairer.	Definitely model X	The range of the groups was too one-sided.	Probably model Y	The results were closer to the prediction.	Probably model Y	Results are more evident. 	Probably ${e://Field/pref_model}	This model seems more true.	Probably model Z	based on the predicting required icu.	Probably ${e://Field/pref_model}	Just seems fairer.	High			Someone could die.	High			More data should be reviewed to not make a mistake.	Moderate			Society doesn't always take into account what an individual might need.	Moderate			The people that make decisions should do more research.	Yes			Yes			Yes			Advantaged			Primary Education	Others	614142ff523b552772f33124	61439223ecf74e491e22a39c	6144b795012e002f66d9310f	majority	outcome-fpr	icu	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6144b795012e002f66d9310f	APPROVED	2021-09-17 15:43:36.101000	2021-09-17 16:35:16.974000	3100.873	49.0	126	0	100	2021-09-20 16:34:45.177000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	English	United States	Female	No	
10	2021-09-17 09:33:53	2021-09-17 10:45:52	IP Address	24.14.31.89	100	4319	True	2021-09-17 10:45:52	R_1rc6FEG0aI3Flne					41.721206665039055	-87.70369720458984	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Very unfair	because I feel that the probability of gaining access should be the same. 	Very biased	because I feel that the probability of gaining access should be the same	Completely unusable	because I felt that those mistakenly granted access should not have been that high for the white considering the probably of gaining access.	Neither fair nor unfair	probability of gaining access is the same but more African American were mistakenly granted access	Acceptably biased	probability of gaining access is the same but more African American were mistakenly granted access	Mostly useful	it can be used to compare those who are wrongly granted access with respect to the probability of getting access taking into consideration the skin color.	No		Probably model Y	Model Y has the same probability of granting access (53%) .	Probably model X	Model X has different probability of gaining access .	Probably model Y	"With Model Y, comparison could be easily done using ""those mistakenly granted access"" and ""probability of granting access"""	Probably model Y	"comparison could be easily done using ""those mistakenly granted access"" and ""probability of granting access"""	Probably model Z	percentage of those denied access are the same in both race.	Definitely ${e://Field/pref_model}	whites are more denied access than African American bearing in mind that they have the same % of granting access	Probably model Z	mistakenly denied access % is equal.		High		It will indirectly affect the overall performance of hospital operation negatively and as a result leading to waste of time and effort.		High		It will result in less productivity and may lead to loss of life.		High		The patients and family members(society) of the patients may suffer for lack of professional hands as at when due.		High		Other hospital members of staff, patients(society) would be affected as it may pose a security danger for them.		Yes			Yes			Yes			Advantaged		Master	Education	613f5acba86e9688db3cc8a2	61439223ecf74e491e22a39c	6144b5560918e614dd52ff1e	majority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6144b5560918e614dd52ff1e	APPROVED	2021-09-17 15:33:47.965000	2021-09-17 16:46:13.454000	4345.489	29.0	20	0	100	2021-09-20 16:14:24.594000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Male	No	
11	2021-09-17 10:39:06	2021-09-17 11:04:37	IP Address	47.152.128.123	100	1531	True	2021-09-17 11:04:38	R_A6W0tDPQhY5dTsB					33.62060546875	-117.08670043945312	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Because the percentage of African Americans is so much higher.	Neither biased nor unbiased.	Because the percentages are the same.	Neither useful nor unusable	Because the percentages are the same.	Neither fair nor unfair	I think it is neutral because the percentages are the same.	Mildly unbiased	Yes, because the percentage is so much higher for African American. I would say it was neither if the percentages were closer.	Mostly useful	Because it's predicting how many people regardless of race.	Yes	I think it's slightly biased vs neutral.	Probably model Y	Because of the huge gap in the percentage of mistakenly granted ICU support.	Probably model X	Because of the initial probability percentage differences.	Probably model Y	Because the percentages start off the same initially.	Definitely model Y	Because they start off with equal predictability. 	Probably model Z	Because the mistaken percentages are the same.	Probably ${e://Field/pref_model}	Because of the higher percentage of white ICU patients.	Probably model Z	Because the mistaken percentage is the same.	Moderate			Because someone else may have needed the support.	High			Because the individual didn't get the care required.	High			Because many other people may have been affected if their loved one didn't get the care needed because it was given to the wrong person.	High			Because many people could have lost someone when they didn't get ICU support as needed.	Yes			Yes			Yes			Advantaged			Secondary Education	Services Occupations	600de95f9135116478afdff8	614391dcdf09127f013fe60a	6144c49dd39e88537cdbed40	majority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6144c49dd39e88537cdbed40	APPROVED	2021-09-17 16:39:03.978000	2021-09-17 17:04:40.535000	1536.557	45.0	75	0	100	2021-09-20 14:53:51.356000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Caucasian	English	United States	Female	Yes	
12	2021-09-17 10:49:24	2021-09-17 11:23:12	IP Address	67.167.194.163	100	2027	True	2021-09-17 11:23:12	R_2EsprjGyg48e90o					41.68370056152344	-88.34989929199217	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	For the same reason as the previous model. They don’t have emotions as they are computers.	Neither biased nor unbiased.	A machine cannot be biased.	Mostly useful	Saves time. I personally take forever having to log into my handheld for work. Even more so if they time out.	Neither fair nor unfair	Because it’s not like a man made robot or computer can choose to be unfair. If it’s not accurate it must be a glitch or something that the creator of said model would have to rectify.	Neither biased nor unbiased	Because once again the model can’t have biases. A person can. But that is not something that can be programmed. At least I wouldn’t think so.	Mostly useful	Because like stated previously in the scenario, doctors and nurses benefit by saving time having to input their information to log in.	No		Models X and Y are equally fair	They are incapable of being unfair as they are electronics without feelings 	Models X and Y are equally biased	As there is no way to differentiate the unlikely biases. 	Definitely model Y	Because the inaccuracies percentages are lower in model Y. 	Definitely model Y	Although both are probably useful, model Y is more likely to give you accurate results according to the graphs.	Both ${e://Field/pref_model} and Z are equally fair	Neither as a machine can’t be fair or unfair .	Both ${e://Field/pref_model} and Z are equally biased	They can’t be biased. Because they have no emotions	Definitely ${e://Field/pref_model}	It’s more accurate.		Moderate		Because hospital staff are very important and lack of useful equipment can only slow down their work.		Moderate		Simply because the likelihood of the unauthorized user to be a criminal or someone with negative intent and know how to operate such a model is unlikely in a hospital setting.		Low		A patients family. Because they aren’t personally getting treatment. However they too might be suffering due to a family health scare.		High		Any person who has a family member in the hospital or simply recognizes the value of medical care. They would not feel safe or comfortable knowing their loved one or possibly themselves one day would be subject to injury if an intruder not employed by the hospital was allowed access to their info.		No			No			Yes			Disadvantaged		Secondary Education	Others	61114a98f0ae7ee9ae2fb4a5	61439223ecf74e491e22a39c	6144c70ff751430708e5ac55	majority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6144c70ff751430708e5ac55	APPROVED	2021-09-17 16:49:22.351000	2021-09-17 17:23:15.389000	2033.038	22.0	67	0	100	2021-09-20 16:28:32.018000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	Spanish	United States	Female	No	
13	2021-09-17 10:49:00	2021-09-17 11:35:58	IP Address	107.136.198.217	100	2817	True	2021-09-17 11:35:58	R_1pzBBCuxL4tZcCa					33.80549621582031	-117.92230224609376	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 1	Mildly unfair	It gives African American's a lower chance of getting predicated to know whether or not they require ICU.	Neither biased nor unbiased.	The probability of predicting someone needs ICU is a bit of a difference but equal when being mistakenly granted.	Neither useful nor unusable	The chances of people getting mistakenly granted ICU support is equal to both parties.	Mildly unfair	The probability of getting mistakenly granted ICU support is higher for white people leaving the chances of people of color's condition to get worse when they needed it the most.	Very unbiased	The chances for white people to get help whether or not they seriously need it is higher than people of color.	Neither useful nor unusable	The chances of someone being mistakenly granted ICU support is a high risk that can affect the lives of those who need it.	Yes	Model X is neither biased or unbiased wether a patient requires ICU 	Probably model Y	Their probability of predicting ICU is more fair than model X's.	Models X and Y are equally biased	They both show more bias to whites in both models.	Probably model Y	Their prediction for who needs ICU is more fair.	Probably model Y	Their prediction with who needs ICU treatment is more equal than model X's.	Probably model Z	Mistakenly denied ICU support is equal.	Both ${e://Field/pref_model} and Z are equally biased	Both show one or the other being higher in both graphs.	Probably model Z	The chances of being mistakenly denied ICU support is equal.	High			The end result those who actually need it get worse and risk their health getting worse.	Moderate			They get help either way.	High			Society as in people in the world would bring to light the carelessness.	Moderate			If the people in the world see that the predictions are equal then they would not look much into it.	No			No			Yes			Disadvantaged					60ff1675f00a765cb05017e8	61439223ecf74e491e22a39c	6144c6ee6877c1909aa8b20f	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6144c6ee6877c1909aa8b20f	APPROVED	2021-09-17 16:48:57.113000	2021-09-17 17:36:01.129000	2824.016	22.0	50	0	100	2021-09-20 15:54:47.874000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
14	2021-09-17 11:21:59	2021-09-17 11:46:50	IP Address	206.74.183.14	100	1490	True	2021-09-17 11:46:50	R_2f9W8qFGpn2IaKu					34.31889343261719	-80.92500305175781	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Acceptability rate among the two races are normally distributed 	Acceptably biased	Its allow more payment from certain race compare to others 	Mostly useful	Its also help to which race tends to make fraudulent payments 	Very fair	Evenly distributed 	Very unbiased	The model is not biased to a certain race	Very useful	The model give accurate information 	No		Probably model Y	Its more evenly distributed 	Definitely model X	Its favor one race than other 	Probably model Y	Ite a model that can be widely accepted because of its even distribution 	Definitely model Y	Its the best model	Definitely ${e://Field/pref_model}	Its not biased 	Probably model Z	Its a biased model	Definitely ${e://Field/pref_model}	Evenly distribution 			Moderate	Its will be a painful experience 			High	Its doesn't speak well of an idea society 			Low	Its mistake that can be corrected 			High	Its definitely affect the credibility of every individual 			Yes			No			Yes			Advantaged	Master	Others	61435ffec47db8c584a334df	61439223ecf74e491e22a39c	6144ce8ae165f2ce331a761e	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6144ce8ae165f2ce331a761e	APPROVED	2021-09-17 17:21:48.473000	2021-09-17 17:47:00.989000	1512.516	35.0	105	0	100	2021-09-20 16:33:09.559000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Male	No	
15	2021-09-20 10:59:05	2021-09-20 11:22:08	IP Address	162.17.89.133	100	1383	True	2021-09-20 11:22:09	R_10Oyi4uWVY0ffoW					38.81129455566406	-121.26429748535156	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	It seems to favor African Americans	Very biased	It seems to favor one race more	Mostly useful	It still gives users an edge on predicting who will need intensive care	Very fair	It is evenly split among the two races	Very unbiased	It favors neither race	Very useful	It helps to try to split ICU capacity to those in need of it the most	No		Probably model Y	It seems to equally consider both races	Probably model X	It seems to prefer African Americans	Probably model Y	It seems the least biased	Probably model Y	It seems more fair	Probably ${e://Field/pref_model}	It seems to equally consider both races	Probably model Z	It seems to lean towards African Americans	Probably ${e://Field/pref_model}	It seems more fair	Low			At worst, they would receive unnecessary intensive care which is not damaging to them.	High			They could miss the required level of care which could be fatal.	Moderate			It could cause someone who actually needed the care to not have it and be harmed. I mostly consider society as others in need of medical care.	Low			At worst, there is another slot in the ICU if someone needs it. I consider society to be others in need of medical care.	No			Yes			Yes			Advantaged			Secondary Education	Others	5f5e32a0d6a12c6857b8e127	614391dcdf09127f013fe60a	6148bd995f61df115c1111e4	majority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6148bd995f61df115c1111e4	APPROVED	2021-09-20 16:59:02.952000	2021-09-20 17:22:12.346000	1389.394	25.0	1202	0	100	2021-09-23 14:32:21.778000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
16	2021-09-20 11:09:14	2021-09-20 11:28:42	IP Address	73.128.52.79	100	1168	True	2021-09-20 11:28:43	R_2ymheC3XIwVVFbp					39.085006713867195	-77.09320068359375	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Very unfair	There are racial disparities	Very biased	Again the racial disparities in the first figure	Neither useful nor unusable	It is good but needs to be less biased	Neither fair nor unfair	It has a racial disparity but at least no one doing the right thing is flagged as bad	Acceptably biased	Again there is a disparity but less notable	Mostly useful	It is good but needs to be improved	No		Probably model Y	The bias doesn't affect people who are doing the right thing	Probably model X	It has a more impactful bias	Probably model Y	It has less problems	Probably model Y	The bias has less problems for people who do the right thing	Probably model Z	I think mistakenly denying payment is worse	Probably model Z	Again denying payment is worse	Probably ${e://Field/pref_model}	Model Y seems less worse 			High	It could lead to chaos for the payer			High	Would be extremely problematic for the person accepting the payment			High	They would be unable to pay and cause lots of stress			High	It could lead to total economic collapse			Yes			Yes			Yes			Disadvantaged	Bachelor	Engineering and Technology	5fd9227fbeeb773aa4d6c986	61439223ecf74e491e22a39c	6148c036a4919d2620d839b1	minority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6148c036a4919d2620d839b1	APPROVED	2021-09-20 17:09:12.842000	2021-09-20 17:28:46.133000	1173.291	32.0	565	7	98	2021-09-23 14:43:20.500000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Male	No	
17	2021-09-20 10:59:50	2021-09-20 11:36:44	IP Address	173.76.201.11	100	2214	True	2021-09-20 11:36:45	R_2wS26rGqqr1oFoC					42.6842041015625	-70.8478012084961	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	"Because it is more likely to grant access to white individuals than African American individuals.
"	Acceptably biased	"I wouldn't say it is ""very biased"" because it's not like it is granting 75% to white people and 25% to African Americans, but it is definitely biased a bit. I personally don't like the term ""acceptably biased"" -- I think it should just mirror with ""mildly biased"" -- but that is where it falls on the scale for me. "	Neither useful nor unusable	It is not the worst system in the world but it needs fine tuning. A 35% failure rate (more than 1 in 3) in mistakenly granting access seems pretty high, especially for something this serious. But in concept it is useful and could continue to be developed.	Neither fair nor unfair	It seems to give both races an equal probability of granting access, but definitely indicates a slight bias in mistakenly granting access to white individuals. 	Acceptably biased	There is a slight bias in that it mistakenly grants white individuals access more often than African American, even though the probability of acceptance is the same across the two. 	Mostly ununsable	As with the before model, the failure rate is a little high, especially for white individuals.	No		Probably model X	"I've thought about this more, and while the difference in the Probability rate seems pretty significant, the fact that it fails at the same rate for both races seems to be more important in determining fairness, because that is just a failure rate overall regardless of the person. It is not showing any sort of ""preference"" for one race in granting access that shouldn't be granted, which seems more important that not granting access that should be."	Probably model Y	"Going off of the above, if it just grants default access at a 53% chance to anyone, then it seems to not really ""care"" who is trying to access in terms of race, but then it does give a preference to white people who are not authorized. As if it is ""letting it slide"" for white people more often."	Models X and Y are equally useful	Neither seem to be really what anyone would want for this system, so I can't really say which one would be more useful. 	Probably model X	I would rather have a failure rate be just a consistent issue rather than one that seems to be influenced by race. 	Both ${e://Field/pref_model} and Z are equally fair	They seem to be just two sides of the same coin to be honest, even with very similar percentages. I can't really tell what would be considered fair.	Both ${e://Field/pref_model} and Z are equally biased	As above, to me it seems like two sides of the same coin -- either the model is unfairly denying access to African Americans or unfairly granting access to White people. They seem equally bad to me. 	Probably ${e://Field/pref_model}	If I had to choose, because of the context where it is medical devices and information, etc., I would rather have a system with a consistent failure rate in mistakenly granting access. Denying access is a pain and an issue, but the worst that happens is the employee enters a password or something like that. Mistakenly granting access is a threat to privacy, etc.		Low		It is a pain and takes some extra time to enter a password, but at the end of the day, the system is secured and they just need to do that extra step.		High		Mistakenly allowing access poses a threat to privacy and security and is a much more important issue.		Low		The impacts of society (e.g., patients, other staff) is even less significant than for the individual, as whether that individual needs to enter a password has no bearing on the other people's lives.		High		I guess I sort of answered Individual more from a Society perspective, but my thoughts are the same -- it can affect the privacy of other patients' (society) health information, which can cause problems for the hospital overall and would maybe cause it to be fined or shut down or something (also society).		No			No			Yes			Advantaged		Bachelor	Business	5f50468c0868af1baaec306c	614391dcdf09127f013fe60a	6148bd9a40d32beab51cf243	minority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Advantaged	6148bd9a40d32beab51cf243	APPROVED	2021-09-20 16:59:48.422000	2021-09-20 17:36:48.538000	2220.116	24.0	859	0	100	2021-09-23 14:31:28.931000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
18	2021-09-20 11:07:11	2021-09-20 11:50:46	IP Address	69.85.214.173	100	2615	True	2021-09-20 11:50:47	R_3LXfKI9ssSdCbKI					30.66009521484375	-88.16649627685545	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	The probability of granting access at all is skewed toward white people, but this may not be a technology problem but just a representation of the races of the medical staff at this hospital, so fairness wise it could go either way depending on the reason for the discrepancy.	Neither biased nor unbiased.	Like I said above, if it's representative of the staffing, then it wouldn't be biased. But if it's a faultiness of the technology, then I'd say it's definitely biased.	Neither useful nor unusable	Once again, it's usability would depend on the accuracy given the staff.	Mildly unfair	White people are almost twice as lucky to be mistakenly granted access than African American people, even though the probability for granting access to both groups is supposed to be 53%	Acceptably biased	"So I don't love the word ""acceptably"" biased here, but chose this one because I think it's between a neutral stance and a ""very biased"" stance, but I do think the results are showing bias (as explained further above.)"	Mostly ununsable	With the data discrepancy, I don't think this is functional technology given the biases.	No		Probably model X	Mistakenly granted access is the people who fall through the cracks and aren't in the system, so it's not fair for it to be more likely for white people to get past this technology and get unwarranted access.	Probably model Y	Model Y seems to be more biased towards white people.	Probably model X	Like I've said previously, I think Model X's data is a result of the staffing at the hospital which makes it more accurate and therefore more useful.	Probably model X	I would choose Model X because it should be more accurate with how it's set up given the data.	Probably model Z	Model Z is just barely more fair when you look at percentages (24% compared to 45% and 23% compared to 46%) Overall neither seem fair though.	Probably ${e://Field/pref_model}	Based on numbers, Model X is slightly more skewed to white people than Model Z is.	Probably model Z	Model Z seems to be slightly more fair based on percentages, so I would want the more fair software.		Moderate		It might hurt the staff member's feelings, but ultimately with the backup of having to provide username/password, it won't cause harm to hospital patients.		High		It could be detrimental if someone without access was able to get through in a hospital where people's lives are especially on the line.		Moderate		From a race perspective, it could be a slap in the face for minorities who have continually felt left out to once again not feel included.		Moderate		In this situation, society itself doesn't feel the negative effects as much, but more so the individual people who messed up resulting in this faulty technology being used.		Yes			Yes			Yes			Advantaged		Bachelor	Business	61153455405f1e389764b2a3	614391dcdf09127f013fe60a	6148bd9b2af76449aee165e4	minority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	6148bd9b2af76449aee165e4	APPROVED	2021-09-20 17:07:09.291000	2021-09-20 17:50:48.625000	2619.334	24.0	113	0	100	2021-09-23 14:33:41.150000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
19	2021-09-20 10:58:31	2021-09-20 11:54:34	IP Address	70.127.253.107	100	3363	True	2021-09-20 11:54:34	R_2lvFxiPfSc8Wnst					27.94549560546875	-82.45980072021484	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	The model makes the same mistake percentage wise for each race. But where only 6% of Whites really needed ICU care, the model picked up 30% of African Americans that needed care by casting a wider net in that racial group.	Neither biased nor unbiased.	Does the model take race into account or just the conditions of patients?	Mostly unusable	Among the White cohort, only 6% out of 41% needed ICU care and only half of the African American cohort needed ICU support. That is a waste of resources. 	Very fair	It assumes all patients will require care regardless of race. 	Very unbiased	Model Y doesn't seem to take a patient's race into consideration but rather their conditions. How exactly are patients rated as being mistakenly granted ICU support? Is race even included in the model's evaluation of patient's conditions?	Neither useful nor unusable	It depends on how patients' conditions are rated without Model Y. Are white patients more in need of ICU care than African American patients?	Yes	The first model wastes just as many resources as the second model, regardless of race. 	Probably model Y	Model Y grants equal access right off the bat. 	Probably model X	Model X sets aside a higher percentage for African Americans although the reasoning is unknown.	Models X and Y are equally useful	Model Y is only 1% less wasteful but they both have their merits.	Neither model X nor model Y	Everyone should be given the same level of access to the ICU, whether or not it is deemed wasteful by the hospital (Model Y). However, from a business perspective it would make more sense to apply resources efficiently (Model X). Perhaps a combination of the models could produce a more efficient and equitable Model Z.	Probably model Z	Even though Model Z lowers the White percentage probability, it also lowers the number of those mistakenly denied support. And yes, it does increase the African American percentage for mistakenly denied support by 9% but it also increases the probability of those needing support. 	Probably ${e://Field/pref_model}	It assumes both races will require the same percentage of support. 	Probably model Z	I see that it may be necessary to adjust the probability percentages at the outset according to race to avoid denying needed support. 	Moderate			Medical debt incurred by unnecessary ICU support.	High			Possible death or long term effects of lack of care. 	Low			Society includes family members of the patient who may have to incur the cost of the unneeded ICU support and the additional stress of having a family member in the ICU. 	Moderate			The loss of a family member who did not receive needed support or the long term negative effects of not receiving proper care. 	No			Yes			Yes			Disadvantaged			Master	Others	611285de4477f8152efce6e8	61439223ecf74e491e22a39c	6148bdadecf168b30ea434c5	majority	outcome-fpr	icu	False		model Y	bottom								Disadvantaged	6148bdadecf168b30ea434c5	APPROVED	2021-09-20 16:58:29.418000	2021-09-20 17:54:37.079000	3367.661	37.0	88	1	100	2021-09-23 14:47:13.258000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	No	
20	2021-09-20 12:30:12	2021-09-20 12:54:08	IP Address	172.119.145.73	100	1436	True	2021-09-20 12:54:09	R_3DwKItQI93Wsm1m					34.30769348144531	-118.42869567871092	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Black patients are less likely to be given ICU support, but white and Black patients are equally likely to be mistakenly given ICU support.	Neither biased nor unbiased.	Black patients are less likely to be given ICU support.	Neither useful nor unusable	While both white and Black patients are equally likely to be mistakenly given ICU support, the probability that Black patients will receive ICU support is much lower.	Mildly unfair	This model seems to believe white patients are more likely to require ICU support than Black patients.	Very biased	It is skewed towards white patients.	Mostly useful	It does have an even probability of both white and Black patients correctly requiring ICU support.	No		Models X and Y are equally fair	They both have aspects that are more fair than the other model.	Models X and Y are equally biased	They both highlight a skewed bias towards white patients.	Models X and Y are equally useful	Both have aspects that would be useful in future models.	Neither model X nor model Y	Both are too biased for my liking.	Both ${e://Field/pref_model} and Z are equally fair	Both are skewed towards white patients.	Both ${e://Field/pref_model} and Z are equally biased	Both are clearly biased towards white patients.	Neither ${e://Field/pref_model} nor model Z	I would not be comfortable choosing either.	Low			If someone, as an individual, receives ICU support mistakenly, there is no harm done to them as a person.	High			If an individual is mistakenly NOT given ICU support, it could cause grave harm to their health and recovery.	Moderate			Thinking in terms of the dominant culture and marginalized people, if you're mistakenly giving white people ICU support more often than marginalized people, there is an obvious racial bias and that causes societal harm.	High			To a more extreme degree, mistakenly predicting one group of people (in this case, marginalized people like Black or brown folks) does not require ICU support while giving it more often to their white counterparts, that is an even more dangerous and deadly bias.	No			No			Yes			Disadvantaged			Bachelor	Others	594a964c215cbd000146de75	61439223ecf74e491e22a39c	6148d33143bba4a0f2b0002c	minority	outcome-fpr	icu	False		model Y	bottom								Disadvantaged	6148d33143bba4a0f2b0002c	APPROVED	2021-09-20 18:30:09.598000	2021-09-20 18:54:10.724000	1441.126	32.0	601	0	100	2021-09-23 14:41:54.846000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	Spanish	United States	Female	No	
21	2021-10-08 15:13:44	2021-10-08 15:49:05	IP Address	70.113.78.101	100	2121	True	2021-10-08 15:49:06	R_3k6bIyqhwIA9eLS					30.364303588867188	-97.6864013671875	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Because the probability of african americans that are predicted that ICU is required is less than white	Very biased	Shows white people are favored.	Very useful	shows that white people are superior to black people in the hospital in regard to ICU support.	Mildly unfair	White people are more likely to be granted ICU support when they don't need it.	Very unbiased	There is not really anything biased about it.	Mostly useful	It brings up the question why white people are more likely to be mistakenly granted ICU support.	No		Probably model Y	Because required ICU is more important than mistakenly being granted ICU support,	Probably model X	Because the percentage of white people having required ICU is higher than black people.	Probably model X	Shows inequality in the hospital.	Definitely model X	It is a better statistic and it shows more than model Y.	Probably model Z	Because at least the percentage of being mistakenly denied is equal.	Definitely ${e://Field/pref_model}	Because black people are more mistakenly denied ICU support.	Probably model Z	It's more positive than model X, and shows more promise that white and black people can be equal.	Moderate			I mean if they don't need it, it's not gonna hurt when they receive it.	High			If they need it, and they don't get it, it could be life-threatening.	Low			I feel like the society is the internet and I don't really think it's something they think about.	Low			Pretty much the same answer, I don't think it's thought about.	Yes			Yes			Yes			Disadvantaged			Secondary Education		615cea6d1fa0fc960c530651	615f943ec32164d0f282bd34	6160b47c0ba01b36e8c0bccf	minority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Disadvantaged	6160b47c0ba01b36e8c0bccf	APPROVED	2021-10-08 21:13:41.120000	2021-10-08 21:49:11.452000	2130.332	18.0	22	0	100	2021-10-14 01:09:29.301000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	Yes	
22	2021-10-08 14:48:28	2021-10-08 15:55:05	IP Address	73.235.35.65	100	3997	True	2021-10-08 15:55:07	R_bsFwLFM6RCqrsYN					37.325698852539055	-120.49990081787108	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	One group has more payments allowed than another group.	Acceptably biased	It allows one groups payments more than another even though they have the same amount of fraudulent payments.	Mostly useful	The mistakenly allowed payments are slow showing it is doing an okay job stopping fraudulent payments.	Acceptably fair	Because the probability of allowing the payment was exactly the same for both groups. It is acceptably fair though because it does get more fraudulent payments from one group. 	Neither biased nor unbiased	Because both groups are equally likely to get there payments allowed.	Neither useful nor unusable	It makes mistakes but still doesn't mistakenly allow fraudulent payments more that 50% of the time. 	No		Probably model Y	because probability of allowing the payment is the same for the two groups.	Probably model X	Because probability of allowing the payment is different for the two different groups in model x but not in model y	Definitely model X	Because there is the same percentage of probability of allowing the payment and a lower percentage of mistakenly allowed payments compared to model y.	Neither model X nor model Y	Because the amount of payments denied is not worth it compared to the percentage of mistakenly allowed payments in both models.	Both ${e://Field/pref_model} and Z are equally fair	Because they both have a percentage the is equal in one of the graphs and a percentage that is the same distance away from each other in at least one graph. 	Probably ${e://Field/pref_model}	Because model x has higher percent difference in mistakenly denied payments between the two groups.	Probably model Z	Because it has a lower percent of mistakenly allowed payments. 			High	Because it stops an individual to obtaining a place to live but opens up an opportunity for another individual to have a place live.			Moderate	It only affects the individual that is the home owner.			Moderate	Because it mostly impacts the individuals trying to rent but if enough people are mistakenly denied the landlord may consider lowering there rent affecting everyone who is currently renting by bringing down there rent. Society being the mass majority of rents.			High	Because if enough mistakenly allowed fraudulent payments are made the land lord may raise the rent of all the other renters to make up for his or her losses.			No			Yes			Yes			Disadvantaged	Bachelor	Others	615ccfb8b526ec5ea9b0cf34	615f94f07f89d7a8afda6025	6160ae97633b3dd7261aa6eb	majority	outcome-fpr	rent	False		model X	top								Disadvantaged	6160ae97633b3dd7261aa6eb	APPROVED	2021-10-08 20:48:27.124000	2021-10-08 21:55:09.592000	4002.468	24.0	18	0	100	2021-10-13 00:19:05.005000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Male	No	
23	2021-10-08 15:28:09	2021-10-08 16:05:15	IP Address	45.36.163.181	100	2225	True	2021-10-08 16:05:16	R_2DUo7wXlqqQNDSA					36.00199890136719	-80.00039672851561	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Mildly unfair	The probability of allowed payment is biased towards African-Americans but maintains a fair and equal amount of mistaken acceptance.	Acceptably biased	It has a mild bias towards Afro-American allowed payments.	Very useful	It shows easily readable data for the bias.	Acceptably fair	The probability of allowing payment is equal for both groups but also mistakenly allows more African-American.	Acceptably biased	There is a noticeable bias towards whites.	Very useful	It shows information clearly.	No		Probably model Y	I feel that having an overall more equal accepted payments in general is more important than looking at mistaken ones.	Definitely model X	The margin of difference with the bias is larger in model X.	Models X and Y are equally useful	They both show data in an easy to read format.	Probably model Y	The margin of difference is smaller, making it slightly less biased.	Definitely ${e://Field/pref_model}	Probability of allowance is more important.	Definitely model Z	There is a bias in the more important info.	Definitely ${e://Field/pref_model}	It is less biased in my eyes.			Moderate	It creates complications that need to be solved but is not urgent enough to be high.			High	Fraudulent payments can create complications involving legal issues.			Low	This situation is seemingly unimportant on a larger scale than the renter and the person managing payments.			Moderate	While still being less important on a larger scale, this can still create massive legal issues.			Yes			No			Yes			Disadvantaged	Secondary Education	Others	615c17a6d388e4d070d2984b	615f94f07f89d7a8afda6025	6160b7da7191f7fac7569562	majority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160b7da7191f7fac7569562	APPROVED	2021-10-08 21:28:01.409000	2021-10-08 22:05:18.063000	2236.654	18.0	4	0	100	2021-10-12 17:48:50.971000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Male	No	
24	2021-10-08 15:37:29	2021-10-08 16:12:44	IP Address	74.130.242.206	100	2115	True	2021-10-08 16:12:45	R_2CiZZKPVzvXDIIQ					36.91990661621094	-86.43920135498048	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Very fair	Because the number of mistakenly allowed payments for both black and white are the same.	Very unbiased	Because outcomes aren’t equal.	Very useful	Because it presumably reduces some risk of fraudulent payment. I don’t know the % of payments that are fraudulent without the model.	Very unfair	Because there’s a higher per cent of fraudulent payments being allowed for African Americans.	Very biased	Because the outcomes for white and black are different 	Mostly ununsable	The results would lead me to believe there’s some bias. That doesn’t mean it can’t be resolved with some tweaking.	No		Definitely model X	Because the outcomes are the same	Probably model Y	Because both black and white probably statistically have differences.	Definitely model X	Because the outcomes are the same	Definitely model X	The outcome shows equity.	Definitely ${e://Field/pref_model}	Equity of outcomes 	Definitely model Z	Outcomes of mistaken payments increase due to race, this leads me to believe there’s some bias. 	Definitely ${e://Field/pref_model}	Same outcomes in Mexico Staten payments 			High	Because it’s going to impact the individual’s ability to obtain housing			Moderate	One payment probably isn’t going to break the person. 			Moderate	If it were to affect a particular group more often then it would affect society as a whole putting stress on a particular group more than another			Moderate	Fraud is kind of a big problem in society. Almost everyone I know has a personal experience with some sort of fraud. So the general public at large would benefit from learning more about prevention.			No			Yes			Yes			Advantaged	Master	Engineering and Technology	6151e17dd596caee8fc6b73c	615f94f07f89d7a8afda6025	6160ba0ede00292ece8c84a6	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Advantaged	6160ba0ede00292ece8c84a6	APPROVED	2021-10-08 21:37:26.462000	2021-10-08 22:12:48.255000	2121.793	44.0	177	0	100	2021-10-13 02:19:54.468000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
25	2021-10-08 16:03:16	2021-10-08 16:18:59	IP Address	73.225.31.17	100	943	True	2021-10-08 16:19:00	R_22LycANrodIskBB					47.22300720214844	-122.53610229492188	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Acceptably fair	It's taking race into account	Acceptably biased	Race based bias	Completely unusable	Race does not determine the acceptance of a card	Very unfair	Because it gives even odds of allowing payment	Neither biased nor unbiased	Cause it's giving even odds	Mostly ununsable	Because race has nothing to do when determining if payment will be processed.	No		Probably model Y	Because it is an even playing field	Definitely model X	It is weighted on one race over the other	Definitely model Y	Even odds	Definitely model Y	Cause it gives even odds	Definitely ${e://Field/pref_model}	Even odds	Definitely model Z	Weighted odds	Definitely model Z	Because race has nothing to do with acceptance			High	It can make a possible move to a new city more stressful or ruin a vacation			Moderate	Even though the owner is made whole, it is still a crime against the person			Low	Unless it becomes chronic it will only effect people using the service not the population as a whole			High	Crime is a problem and allowing it hurts all card holders			Yes			Yes			Yes			Disadvantaged	Bachelor	Others	615dfbb483f8480303655714	615f94f07f89d7a8afda6025	6160c01eaaa38ec6e6d67c20	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160c01eaaa38ec6e6d67c20	APPROVED	2021-10-08 22:03:13.309000	2021-10-08 22:19:04.926000	951.617	38.0	211	1	100	2021-10-13 14:26:10.728000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
26	2021-10-08 15:44:11	2021-10-08 16:19:33	IP Address	166.181.81.117	100	2121	True	2021-10-08 16:19:33	R_yPHLHWFKPS9Nih3					41.602096557617195	-93.61239624023438	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Acceptably fair	Mistakes happen. Same percentage across both groups. 	Neither biased nor unbiased.	Same outcome	Neither useful nor unusable	Because of the same outcome when different inputs. 	Mildly unfair	There is quite a few more whites being mistakenly granted ICU support than African Americans. 	Acceptably biased	Seems to favor whites. 	Mostly useful	To see where problems may be happening. 	No		Probably model Y	Same outcome between the two mistakenly. 	Definitely model X	More accessible accepted with more mistakenly accepted for one over the other. 	Probably model Y	Equality of outcome. 	Neither model X nor model Y	Would like a better chance towards 100% accepted and 0% mistakenly accepted. 	Both ${e://Field/pref_model} and Z are equally fair	Both models seem to have a more negative outcome for the African Americans. 	Both ${e://Field/pref_model} and Z are equally biased	Both models favor whites.	Neither ${e://Field/pref_model} nor model Z	Wouldn't be fair. 	Moderate			Mistakenly predicting for one isn't so bad as many.	Moderate			Again only one but if they die because they dont get the help then that's pretty bad.	Low			Better to have and not need than the other way.	High			A lot of people could come to harm if ICU is needed and not given.	Yes			Yes			Yes			Advantaged			Secondary Education	Others	615e11c669c2e464088ed10d	615f94f07f89d7a8afda6025	6160bba5dbb4371d06981d4f	minority	outcome-fpr	icu	False		model X	top								Advantaged	6160bba5dbb4371d06981d4f	APPROVED	2021-10-08 21:44:08.207000	2021-10-08 22:19:37.298000	2129.091	35.0	68	0	100	2021-10-13 01:51:33.287000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
27	2021-10-08 16:01:41	2021-10-08 16:21:39	IP Address	107.11.236.27	100	1197	True	2021-10-08 16:21:40	R_1PcMhtC2JGvV7B9					39.74899291992188	-84.15730285644531	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	It mistakes the same amount of whites as it does African Americans	Neither biased nor unbiased.	It seems pretty fair 	Very useful	It seems to be fair in its predictions 	Very unfair	It mistakenly gives almost double more African Americans who don’t need ICU support than whites 	Mildly unbiased	It favors African Americans more than whites 	Mostly ununsable	It mistakenly gives ICU support to a lot of people 	No		Definitely model X	It makes the same amount of mistakes for white people as it does for black people.	Definitely model Y	It favors African Americans more 	Definitely model X	It is more fair in its predictions for both races 	Definitely model X	It is more fair in its predictions and mistakes for both races 	Both ${e://Field/pref_model} and Z are equally fair	One mistakenly grants ICU support to the same amount for both races and the other mistakenly denies ICU support for both races so they even each other out.	Probably ${e://Field/pref_model}	It mistakenly denies more whites than blacks 	Neither ${e://Field/pref_model} nor model Z	They both seem to make a lot of mistakes 	Low			They would be getting more care than they needed and that really wouldn’t hurt them.	High			It could cause them to miss out on treatment that could save their life.	Moderate			It could be giving a bed in the ICU to someone who doesn’t really need it and therefore taking it away from someone who does.	High			It could cause you or your loved ones to lose out on treatment that could be life saving.	No			No			No			Disadvantaged			Secondary Education	Others	6152be1021c9ab590015a3e3	615f94f07f89d7a8afda6025	6160bfbcacaabae499535a0f	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Disadvantaged	6160bfbcacaabae499535a0f	APPROVED	2021-10-08 22:01:37.021000	2021-10-08 22:21:42.835000	1205.814	31.0	183	0	100	2021-10-13 14:28:23.930000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
28	2021-10-08 15:33:32	2021-10-08 16:22:04	IP Address	172.249.154.24	100	2912	True	2021-10-08 16:22:05	R_12mVdexqAm0CzPA					33.884002685546875	-117.89410400390624	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Mildly unfair	giving low access to African Americans and mistakingly giving other African Americans access. 	Acceptably biased	more white people granted access and not being mistaken. 	Mostly useful	Needs to be able to work for people of color	Acceptably fair	Both graphs have even accessibility 	Very unbiased	Both graphs look acceptable	Very useful	Has better results than the other model	No		Definitely model Y	model y has more equal accessibility 	Definitely model X	they give unfair advantages to white people	Definitely model Y	Equal accessibility	Probably model Y	Is more fair than X	Definitely ${e://Field/pref_model}	Model z doesn't give equal acceptability.	Probably model Z	model z is more unfair than model y	Definitely ${e://Field/pref_model}	is more unbiased 		High		it makes it more convenient to have recognition and not giving everyone equal opportunity isn't fair.		High		It is dangerous to allow people who aren't authorized into an authorized facility		High		It does not allow authorized staff to get where they need to be to help people. Society includes everyone.		High		Not granting access to most of one community is damaging to society as a whole and create exclusion		No			Yes			Yes			Advantaged		Bachelor	Others	61113e6bfed4e7e0e187ff3a	615f943ec32164d0f282bd34	6160b91dfce9e7a6eb818e4d	minority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160b91dfce9e7a6eb818e4d	APPROVED	2021-10-08 21:33:29.116000	2021-10-08 22:22:09.410000	2920.294	21.0	64	0	100	2021-10-14 02:46:57.005000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	Yes	
29	2021-10-08 16:03:53	2021-10-08 16:33:52	IP Address	45.31.0.9	100	1799	True	2021-10-08 16:33:52	R_3gUwXj2xjTz45Qf					32.33900451660156	-97.41629791259763	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very fair	It seems that both races are getting access equally. 	Very unbiased	If it were biased, I'm sure that one race would be getting way more access than the other. 	Neither useful nor unusable	it's nothing new, race equality. 	Acceptably fair	Because the races are nearly identical. 	Neither biased nor unbiased	Because the model isn't going off of people. 	Mostly useful	it's good to have extra information, whatever it might be. 	Yes	I would wish to change that the model is not biased. 	Probably model X	It has both races getting mistakenly granted access	Probably model Y	One race has greater access. 	Probably model Y	Model Y shows what to not do. 	Definitely model X	way more equal. 	Probably model Z	they denied both races, the mistakenly granted is a mistake, therefore doesn't matter. 	Definitely ${e://Field/pref_model}	Not as biased, even when getting in .	Probably model Z	would rather both races get denied access. 		High		unauthorized people could get in. 		High		again, unauthorized people could get in. 		Low		The U.S. society, the risk is low, since you're making sure that the staff are authorized to be in the building. 		High		"It could pose a danger to the patients if the doctor isn't actually a doctor. 

The U.S. society. "		Yes			Yes			Yes			Advantaged		Primary Education	Business	61536fd7e4571b6e54a212de	615f94f07f89d7a8afda6025	6160c0449610da96dd2695d0	majority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	6160c0449610da96dd2695d0	APPROVED	2021-10-08 22:03:51.591000	2021-10-08 22:33:55.987000	1804.396	18.0	121	2	99	2021-10-13 15:12:21.589000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Male	No	
30	2021-10-08 16:19:50	2021-10-08 16:39:42	IP Address	107.77.211.103	100	1192	True	2021-10-08 16:39:42	R_rrHo3S0PQpDcriV					34.054397583007805	-118.24400329589844	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Mildly unfair	Has higher probability to grant acces to African Americans 	Neither biased nor unbiased.	"Biased because grants access more to African Americans 
"	Neither useful nor unusable	Not sure 	Mildly unfair	Because it is more likely to mistakenly grant access to African Americans 	Neither biased nor unbiased	Biased in offering acces too more African Americans than whites 	Neither useful nor unusable	Not sure 	No		Probably model Y	I’m not really sure how to infer the difference 	Probably model X	Just seems more unfair 	Probably model Y	Seems more fair than model X 	Probably model X	Seems more fair 	Both ${e://Field/pref_model} and Z are equally fair	Percentages should be the same on both sides 	Both ${e://Field/pref_model} and Z are equally biased	Doesn’t seem to be a biased 	Probably ${e://Field/pref_model}	Not too sure 		Moderate		I don’t think it’s that big of a deal a machine can’t be biased unless designed that way 		Moderate		The. People are able to access programs they shouldn’t be able too and that can be unsafe from a medical standpoint 		Moderate		I don’t think I’m society it makes that much of a difference just a couple seconds more of a process than being racially recognized 		Moderate		Again can be dangerous when people that aren’t supposed to have access to this are granted it anyway. and by society I mean doctors or nurses 		Yes			No			No			Advantaged		Primary Education	Others	615dcf39c6680de4388e66a4	615f94f07f89d7a8afda6025	6160c401c7d94d7a1d1e6101	majority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	6160c401c7d94d7a1d1e6101	APPROVED	2021-10-08 22:19:47.222000	2021-10-08 22:39:46.980000	1199.758	22.0	47	3	95	2021-10-13 00:27:27.694000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Male	Yes	
31	2021-10-08 16:28:40	2021-10-08 16:41:32	IP Address	173.77.235.112	100	771	True	2021-10-08 16:41:32	R_2azP9AlY32DQQi1					40.847702026367195	-73.841796875	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair		Acceptably biased		Very useful		Mildly unfair		Acceptably biased		Very useful		Yes	I better understand the graphics after looking at the second	Definitely model X		Models X and Y are equally biased		Probably model Y		Probably model Y		Probably model Z		Probably ${e://Field/pref_model}		Probably model Z			Moderate		The harm is not nearly as great as if we just fail to recognize 		High		Access can fall into the wrong hands 		High		I feel as if people as a whole find it detrimental 		High		Anyone can access everything		Yes			No			Yes			Advantaged		Bachelor	Others	61085b8ec672d0e88151d5ae	615f94f07f89d7a8afda6025	6160c61330a55fb686e98022	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160c61330a55fb686e98022	APPROVED	2021-10-08 22:28:38.927000	2021-10-08 22:41:34.479000	775.552	23.0	36	1	92	2021-10-13 02:34:04.327000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	Yes	
32	2021-10-08 15:27:10	2021-10-08 16:43:53	IP Address	24.0.237.23	100	4603	True	2021-10-08 16:43:53	R_1HcTVoZ747tQIKY					40.75230407714844	-74.21720123291014	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	I think in order to be fair it should not be decided based off of race. 	Mildly unbiased	I think it is slightly biased because of the fact that the African American is slightly higher than the white.	Mostly unusable	In order for it to be useful it should not be based on race.	Mildly unfair	It would seem as if the fraudulent payments are more geared towards African American people.	Very biased	It seems biased because the category for the white is extremely lower than the African American.	Mostly ununsable	In order for it to be usable it would have to be equal and fair.	No		Probably model Y	Model x gives African Americans and Whites and equal fair chance to accept but it slightly gives white an edge.	Models X and Y are equally biased	One gives white a better chance and the other gives African American a better chance.	Probably model Y	It at least give an equal acceptance rate.	Neither model X nor model Y	It should be completely fair.	Probably ${e://Field/pref_model}	It gives an equal acceptance rate but it shouldn't reject based of off race one way or the other.	Both ${e://Field/pref_model} and Z are equally biased	It's not completely.	Probably ${e://Field/pref_model}	It gives a fair acceptance rate.			Moderate	Although it is an inconvenience it isn't life threatening.			Moderate	For the business it isn't good because they lose money but it doesn't affect the person who scams.			High	For the example of a mother I consider the society to be her children and I think in that case that isn't good because one person may be able to survive without a home but it isn't good for children.			Low	For a parent that makes a fraudulent payment the society is their family and unless the business takes them to court nothing happens.			No			Yes			Yes			Disadvantaged	Secondary Education	Business	615e5b5f1e8fbfe2c701bc8e	615f943ec32164d0f282bd34	6160b7028da5c41698072644	majority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	6160b7028da5c41698072644	APPROVED	2021-10-08 21:26:16.191000	2021-10-08 22:43:57.419000	4661.228	24.0	107	0	100	2021-10-14 01:35:32.696000	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Female	Yes	
33	2021-10-08 16:30:42	2021-10-08 16:49:53	IP Address	76.111.110.98	100	1151	True	2021-10-08 16:49:54	R_3jSWupFOBdxruK9					39.537200927734375	-76.3604965209961	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	African Americans are less likely to have their payment go through 	Very biased	It seems to assume that based on a person’s race, their likelihood of payment being accepted would go down. The assumption is made that African Americans are less likely to make a non-fraudulent payment 	Mostly useful	This shows potential flaws in the system. There should be questions about the differences in payment acceptance by race 	Acceptably fair	There is the same probability of allowing payment from white and African American individuals. There doesn’t seem to be increases bias against African American payments 	Neither biased nor unbiased	Probability of payment acceptance is the same. May be biased as an increase in white people having payments mistakenly accepted. This infers that white people may be more trusted and it would be assumed they wouldn’t be making fraudulent payments 	Neither useful nor unusable	This only shows 2 races and doesn’t give much info on demographics of past fraudulent transactions nor does it account for other demographics such as age, income level, gender, etc	No		Probably model Y	It infers that payments are accepted without consideration of a person’s race 	Probably model Y	Assumes African Americans are more likely to provide fraudulent payment as their payments are less likely to go through 	Models X and Y are equally useful	They both show different information. Different questions can be asked about each graph.	Probably model X	Seems less biased against African Americans 	Probably model Z	Less likely to be biased against African Americans 	Probably ${e://Field/pref_model}	African Americans are more likely to be mistakenly denied payment 	Probably model Z	Less bias against African Americans 			High	They could lose their apartment if their rent isn’t paid on time 			Low	Their rent gets paid. Unless they are prosecuted for the fraud they don’t have a consequence to their action 			High	Increases the risk of homelessness if people are viewed as unreliable or unable to pay their rent on time. Society would include all members of the country 			Moderate	Cost likely increases due to fraud. Society would be any individual renting 			No			Yes			Yes			Advantaged	Master	Healthcare Practitioner	6150f95a8a8b1ab9936de50b	615f94f07f89d7a8afda6025	6160c68c0827791d4f5adb77	minority	outcome-fpr	rent	False	You have chosen model X over model Y.	model X	top								Advantaged	6160c68c0827791d4f5adb77	APPROVED	2021-10-08 22:30:40.014000	2021-10-08 22:49:57.328000	1157.314	36.0	46	0	100	2021-10-13 14:31:48.328000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
34	2021-10-08 16:34:58	2021-10-08 16:52:24	IP Address	72.174.57.144	100	1046	True	2021-10-08 16:52:25	R_3KpZDs7tLr6hHFY					37.472503662109375	-105.87960052490234	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	not equally accurate for races	Acceptably biased	African Americans are unequally predicted to go to ICU	Mostly unusable	Biased	Acceptably fair	overall accurate	Mildly unbiased	Accurate predictions for both races	Mostly useful	Not life threatening if prediction is inaccurate	No		Probably model X	white people are more likely to be taken to icu when it's not needed, showing more care toward white people	Probably model Y	biased in favor of white people	Models X and Y are equally useful	neither are equal 	Probably model X	more equal	Probably ${e://Field/pref_model}	worse to be denied, could be life threatening	Definitely ${e://Field/pref_model}	more likely to deny african americans in icu	Neither ${e://Field/pref_model} nor model Z	both are equally bad - one denies african americans more and one approves african americans less	Moderate			does not directly affect them	Moderate			does not directly affect them	High			patients and families, expensive	High			patients, families, could be deadly	No			No			Yes			Advantaged			Master	Services Occupations	61074a3cd510eb92be8992ca	615f94f07f89d7a8afda6025	6160c78dc17f44aabb37ec38	minority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	6160c78dc17f44aabb37ec38	APPROVED	2021-10-08 22:34:56.012000	2021-10-08 22:52:27.647000	1051.635	25.0	110	0	100	2021-10-13 02:32:55.956000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
35	2021-10-08 15:45:21	2021-10-08 16:55:14	IP Address	76.85.152.168	100	4192	True	2021-10-08 16:55:14	R_1rrbUQQa8YDeL67					40.786895751953125	-96.69619750976561	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Figure 1 shows a significantly higher percent probability to grant access to African American Individuals, suggesting it denies access more often to white individuals. 	Mildly unbiased	Once again, for because figure 1 suggests that it rejects access more often to white individuals.	Neither useful nor unusable	It can be useful if improved. Its mistake percentage is relatively high and around 50% of the someone would use it would deny access. making it inconvenient. Additionally, someone unauthorized to use the equipment might succeed at accessing it 35% of the time. 	Mildly unfair	Although the probability of granting access is the same across both races, figure 2 suggests that model x mistakenly assumes that African Americans individuals have access to the equipment	Mildly unbiased	It favors African American individuals in granting access. 	Mostly ununsable	Similar to Model X, if Model Y were to be used individuals would still have to login via username and password half of the time, making it inconvenient. Additionally, its percentages of mistakenly granting access are much higher.  	No		Probably model X	Because it denies access equally	Probably model Y	Favors African Americans mistakenly more often	Models X and Y are equally useful	They are both relatively the same in granting access without login and password. When it comes to granting access to unauthorized individuals, Model Y disfavors white individuals by ~10% less and favors African Americans ~10% more. They both seem equally bad.	Neither model X nor model Y	They both don't have a good percentage of authorization, and they both have a significantly high percentage of mistakenly granting access.	Both ${e://Field/pref_model} and Z are equally fair	Model Y grants access equally but mistakenly grants access unequally, and Model Z is the opposite. They are both unfair to different groups.	Both ${e://Field/pref_model} and Z are equally biased	The same reasoning as above, Model Y and Model Z are both biased towards different groups.	Probably model Z	Although they're both biased, Model Z seems better when it comes to security than Model Y as it mistakenly grants access at a lower % chance.		High		In situations of emergency, medical staff needs access to the equipment as soon as possible.		High		Medical equipment is expensive and delicate. If someone who is not trained to use it gets access to it they might break it. 		Moderate		"""Society"" is the general public. Face recognition is a new technology and having to input username/password is not unheard of. Though I believe society would recognize how a successful facial recognition system would save medical staff precious seconds."		Moderate		"""Society"" is the general public, and I think the general ideal is that people that have gotten the education and training needed should be the ones that operate medical equipment"		Yes			No			Yes			Advantaged		Bachelor	Engineering and Technology	615a0b8555370ca04c3dcd70	615f943ec32164d0f282bd34	6160bbeb7e42b4242418b9d0	majority	outcome-fpr	frauth	True		model Y	bottom								Advantaged	6160bbeb7e42b4242418b9d0	APPROVED	2021-10-08 21:45:18.284000	2021-10-08 22:55:27.198000	4208.914	23.0	39	0	100	2021-10-13 21:32:47.690000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	No	
36	2021-10-08 16:32:29	2021-10-08 16:57:00	IP Address	172.58.56.250	100	1471	True	2021-10-08 16:57:01	R_42QhTYnzixAuPTz					35.05059814453125	-106.72489929199217	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Because there should be an even amount of granting access to all races and genders.	Acceptably biased	Because there are obviously more African Americans accepted than whites.	Mostly useful	Because it shows what percentage of African Americans vs white Americans are accepted. 	Very fair	"Because there is even amount of access granted.
"	Neither biased nor unbiased	Because there is an equal amount of access on both sides	Very useful	Because equal amount of opportunity is given.	No		Definitely model Y	Because an even amount of access was granted. 	Definitely model X	Because an unequal amount of access was granted. 	Definitely model Y	Because even amount of access was granted giving a fair chance to both races.	Definitely model Y	Because I believe in equality. 	Definitely ${e://Field/pref_model}	Equal amount of access was granted. 	Definitely model Z	Because more African Americans were given access. 	Definitely ${e://Field/pref_model}	Because I believe in giving an equal amount of opportunity. 		High		Because of possible security problems. 		High		Because of possible safety problems.		High		Part of society is the local community, and because of a bigger amount of people that can be affected in a possible security issue.		High		Part of society is the local community, and because of a bigger amount of people that can be affected in a possible security issue.		Yes			Yes			Yes			Disadvantaged		Secondary Education	Transportation Occupations	61412433a3ba985f0e417ecd	615f943ec32164d0f282bd34	6160c6f49e6b32d808fd585d	majority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160c6f49e6b32d808fd585d	APPROVED	2021-10-08 22:32:25.253000	2021-10-08 22:57:06.162000	1480.909	43.0	84	2	96	2021-10-14 16:26:55.925000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Male	No	
37	2021-10-08 16:35:13	2021-10-08 17:01:41	IP Address	172.56.13.63	100	1587	True	2021-10-08 17:01:41	R_1QKP1q7uScEjotm					38.577392578125	-90.6708984375	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	I don’t have enough information to determine if it is fair. 	Neither biased nor unbiased.	I do not have enough information. 	Mostly unusable	It mistakenly allows too many payments. 	Neither fair nor unfair	I do not have enough information. 	Neither biased nor unbiased	I do not have enough information. 	Mostly ununsable	It mistakenly allows too many payments. 	No		Models X and Y are equally fair	I do not have enough information. I am also confused as to how the percentages correlate. You would think that as the percentage of accepted payments goes down, the percentage of mistakenly accepted payments would also go down, unless these numbers do not actually correlate. 	Models X and Y are equally biased	I don’t know the reasoning for these percentages. I don’t have enough information to determine that. 	Probably model X	The percentage of mistakenly accepted payments is lower. 	Definitely model X	The percentage of mistakenly accepted payments is lower. 	Both ${e://Field/pref_model} and Z are equally fair	I don’t have enough information to determine that. 	Both ${e://Field/pref_model} and Z are equally biased	I don’t have enough information to determine that. 	Probably ${e://Field/pref_model}	The percentage of mistakenly accepted payments is lower. 			High	If you mistakenly deny someone’s only form of payment for the roof over their head, that’s a big deal. 			Moderate	Am individual who is committing fraud wants their payment to be accepted. 			Moderate	Everyone who relies on the economy is part of the society. Denying payment from renters is bad for the economy. 			Moderate	Anyone who is impacted by the economy is part of the society. People committing fraud impacts businesses. 			No			Yes			Yes			Advantaged	Secondary Education	Education	615b4959d93acee63f7e33e8	615f94f07f89d7a8afda6025	6160c79be98d43273c024d12	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Advantaged	6160c79be98d43273c024d12	APPROVED	2021-10-08 22:35:09.800000	2021-10-08 23:01:45.073000	1595.273	23.0	53	0	100	2021-10-12 17:31:09.151000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
38	2021-10-08 16:16:52	2021-10-08 17:01:42	IP Address	108.65.227.128	100	2690	True	2021-10-08 17:01:43	R_3EvimyoJ6I67QI2					37.751998901367195	-87.15139770507811	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Very fair	It was 35% of both numbers so they were equal. Same % of mistakes from both groups 	Neither biased nor unbiased.	It was 35% of both numbers so they were equal. Same % of mistakes from both groups 	Mostly unusable	if a 35% margin of error is cool with you, then yeah. 	Neither fair nor unfair	45% margin of error for the white group, only 24% for the African American group. its fair I guess, but not every efficient 	Mildly unbiased	Well now, it seems its giving more rooms to the white group. when it doesn't need to, had to infer if its bias without seeing how many many were rejected and needed the ICU	Mostly ununsable	again if you're cool with that margin of error than yeah go ahead 	Yes	"I don't remember :(


(there should be an option to go back here, or see the graphs again)"	Models X and Y are equally fair	I still need to see the % of people who it did not grant ICU support to and needed it 	Probably model Y	gave ICU support to more of the with group than needed it 	Probably model Y	More of a margin of error to work with to fix the bot	Probably model X	is correctly predicting correctly 65% of the time 	Definitely model Z	model x seems to mistakenly deny 46% of the African American group, and that seem like a lot bro, better to give and not need it than to deny it and they needed it.	Definitely ${e://Field/pref_model}	model x seems to mistakenly deny 46% of the African American group, and that seem like a lot bro, better to give and not need it than to deny it and they needed it.	Definitely model Z	"better to spend too much and save people than deny it and they die. 
"	Moderate			well financially, you might be worse off but you're alive	High			"you could die when you need it
"	Moderate			"$$$$$$$$$$ 


ft. the board members
     tax payers if they don't have insurance
     hospital staff (funding)"	Moderate			"'-most people wont even know
-but the family will probably be pissed if they found out a robot made that call and the individual died
-maybe try to sue?"	No			Yes			No			Advantaged			Primary Education	Services Occupations	615ef1360c01f4fbc2e617fa	615f94f07f89d7a8afda6025	6160c34972b066d7b27953b7	minority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	6160c34972b066d7b27953b7	APPROVED	2021-10-08 22:16:46.011000	2021-10-08 23:01:45.196000	2699.185	25.0	24	0	100	2021-10-13 15:22:27.121000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
39	2021-10-08 16:36:47	2021-10-08 17:07:09	IP Address	24.228.111.12	100	1822	True	2021-10-08 17:07:10	R_2vYUy4OogLV08KT					41.2301025390625	-74.59629821777342	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Though the model is more likely to choose to put an African American patient in the ICU, the failure rate appears to be similar for either race. It may be that there are several patients hospitalized for conditions which affect African American patients more, or it may be that there are simply more African American patients in the hospital right now due to demographics.	Mildly unbiased	Without knowing more about the hospital's demographics and the reasons the patients are in the hospital, it is impossible to say what the reason the machine is putting more African American patients in the ICU is. However, since the failure rate is similar, it seems that the machine's decision is correct.	Neither useful nor unusable	It's incorrect 40% of the time, which means it would probably be better to just rely on doctors and nurses to decide when a patient needs to be moved. However, in order to make that determination, one would need to review the decisions made in the past by the medical staff to see if there was any bias in their decision-making process. If it turns out that the medical staff demonstrated bias that caused loss of life or other negative outcomes, then it might be better to use the machine to eliminate that from the equation.	Mildly unfair	The machine mistakenly grants ICU support to African American patients at twice the rate of white patients. Something is incorrect about how it is making decisions, needs to be tweaked.	Acceptably biased	It's biased and should be corrected; however, in my view, it is better to accidentally overcare for patients than it is to accidentally undercare for them. 	Mostly useful	Overall, the model correctly predicts about 2/3 of the time when a patient needs to be moved to the ICU. That's not half bad. It still needs to be improved so that it performs better.	No		Definitely model X	Model X's rate of incorrect assignment is higher overall but is equal across races. Model Y's rate of incorrect assignment for African American patients is twice that of white patients. Model X does assign more African American patients to the ICU, but since the failure rate is similar, that suggests that more African American patients do in fact need the ICU for one reason or another.	Probably model Y	Model Y assigns black and white patients to the ICU equally, but then the failure rate for black patients is twice that of white patients. This suggests to me that the algorithm is designed to choose an equal number of black and white patients, even if the symptoms do not warrant an equal number of patients. This would be a form of bias.	Probably model X	Model X seems to be doing a better job of giving unbiased results. However, it's failure rate does not seem acceptable to me. 	Probably model X	Model X seems to be more unbiased for the reasons I've indicated above, but to recap, the failure rate for both races is similar in Model X, and that figure is more useful in determining whether there is bias.	Probably model Z	This was a difficult one. Both model X and model Z have similar failure rates, and both models show bias with failures being different for each race. However, I think model Z is more fair because it's generally better to put a patient in the ICU who doesn't need it than it is to deny a patient access to an ICU who does need it. Model Z's bias is to grant too many African American patients access to the ICU, while model Z's bias is to deny access to too many white patients. Neither is good, and granting too many patients access is certainly costly, but if I had to choose between the two, I'd rather a patient have too much treatment than not enough.	Both ${e://Field/pref_model} and Z are equally biased	Both models are biased. Both models have similar failure rates and similar degrees of difference along racial lines. 	Definitely model Z	I would rather put too many patients on ICU support than not enough, especially if I know that the machine assigning them is biased in its calculations.	Low			It uses resources and equipment that could be used for another patient, but otherwise the impact to the individual patient is low	High			Not getting ICU support when it is needed could have catastrophic effects on an individual's health, including death.	Moderate			I would consider other patients and medical staff to be part of society in this instance. The other patients who may not have access to ICU care who may need it are impacted. The medical staff who has to take care of the patient in the ICU, which requires a more hands-on level of care, will also be impacted.	Moderate			If the individual winds up becoming seriously ill or dying, it will have a serious impact on his or her family, company where they are employed, their friends, etc.	No			Yes			Yes			Advantaged			Bachelor	Business	6159f68fffed83e436478237	615f94f07f89d7a8afda6025	6160c7fa9a82f624534b186d	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	6160c7fa9a82f624534b186d	APPROVED	2021-10-08 22:36:44.676000	2021-10-08 23:07:12.996000	1828.32	41.0	69	0	100	2021-10-13 02:22:47.573000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
40	2021-10-08 16:59:22	2021-10-08 17:13:48	IP Address	71.162.160.163	100	865	True	2021-10-08 17:13:48	R_2WTf2K9CkE1iDTU					39.94859313964844	-75.23390197753906	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	The model makes mistakes in a large number of cases, and its probability of granting access is not consistent across races.	Acceptably biased	The probability that the model will grant access is different across races, even though its rate of mistakes is the same.	Mostly unusable	The rate of mistakes is simply too high to be useful.	Very unfair	The model's mistakes are disparate across races.	Very biased	The model makes more mistakes with Black people than white ones.	Mostly ununsable	The model has a very high rate of mistakes, especially for Black people.	No		Models X and Y are equally fair	Both models have issues with how they grant access to people of different races, and both have high rates of mistakes.	Models X and Y are equally biased	Both models have problems in how they grant access to people of different races.	Models X and Y are equally useful	Both models have high mistake rates, making them both have low utility.	Neither model X nor model Y	I am not impressed with the stats from either model.	Probably model Z	Being denied access by mistake is more harmful than being granted access by mistake.	Probably ${e://Field/pref_model}	Model X denies access to certain people based on race.	Neither ${e://Field/pref_model} nor model Z	Both models have too high a rate of mistakes to feel useful to me.		Low		It only takes a few extra moments for a hospital staffer to enter their username and password, so this is a minor inconvenience to their day, and should not unduly delay them in their medical duties.		Low		An unauthorized user may see something they shouldn't by accessing the system, but there is little that is likely to harm that person.		Low		The extra moments the hospital staff spends entering their username and password should not delay them too much in providing care to a patient.		Moderate		An unauthorized person may be able to use information they gain from access to medical records to cause harm to the patient or someone else close to them.		No			Yes			Yes			Advantaged		Master	Administrative Staff	614e4d2edc89fb1743d62473	615f94f07f89d7a8afda6025	6160cd44ef711c4b5d7f0e14	majority	outcome-fpr	frauth	True		model X	top								Advantaged	6160cd44ef711c4b5d7f0e14	APPROVED	2021-10-08 22:59:20.872000	2021-10-08 23:13:50.857000	869.985	34.0	99	0	100	2021-10-13 14:45:37.036000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	No	
41	2021-10-08 16:55:49	2021-10-08 17:15:58	IP Address	173.168.234.40	100	1208	True	2021-10-08 17:15:58	R_1nPdwZcne6NEQAv					28.195098876953125	-81.606201171875	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Model X mistakenly granted access the same percentage of time (35%) for both African Americans and Whites.	Mildly unbiased	As mentioned above, the same percentage was mistakenly denied access. If one group had been mistakenly denied a higher percentage, I would be more inclined to say the Model X is biased.	Mostly useful	35% mistakenly were given access, which is below a 50%. It is not fantastic, but it is not awful either. I would say it is mostly useful.	Acceptably fair	The probability of granting access to both groups of people is identical. 	Acceptably biased	The percentage of mistakenly granted access numbers almost double from one race to the next.	Neither useful nor unusable	I am not sure if this model is useful, as the mistakenly granted access is higher then the previous model in one group of people, and not as consistent.	No		Definitely model X	As previously stated, the numbers are more consistent and the number of mistakenly granted access is not a huge difference from the two groups.	Probably model Y	Model Y has a bigger difference in the mistakenly granted access then Model X does.	Probably model Y	Model Y has the same probability of granting access for each group.	Neither model X nor model Y	Both of these models show inconsistencies with mistakenly granting access to groups of people, and while inconsistencies are expected with technology, I believe the percentages should be lower.	Both ${e://Field/pref_model} and Z are equally fair	Honestly, neither one of these models are fair the longer I look at them. They both either grant access or do not grant access inconsistently enough to not be fair to one group or the other. 	Both ${e://Field/pref_model} and Z are equally biased	As stated above, both models show inconsistencies to both groups of people, that is definitely noticeable enough to warrant bias.	Neither ${e://Field/pref_model} nor model Z	I don't think either of these models are reliable based on the inconsistencies shown.		High		A person is now being held accountable instead of a machine, and consequences are being given to those who fail to recognize an authorized hospital staff.		High		Ranging from social consequences, to more serious consequences, individuals are being held accountable for mistakenly allowing access to unauthorized users.		Moderate		Society can't be held responsible for the actions of one particular being, but from a literal standpoint, authorizing access to anyone who does not actually have access is never good. 		Moderate		"As stated above, the same applies. Society would be all beings as a whole. Specifically, the medical personnel staff, the patients, etc would be more directly affected ""socially"" as a group if access was granted to non-medical personnel."		No			Yes			Yes			Advantaged		Primary Education	Others	60ff3b4b49cf56250d170cc6	615f94f07f89d7a8afda6025	6160cc6a18c877f1d7d69d6c	minority	outcome-fpr	frauth	True		model Y	bottom								Advantaged	6160cc6a18c877f1d7d69d6c	APPROVED	2021-10-08 22:55:47.198000	2021-10-08 23:16:00.669000	1213.471	30.0	99	1	99	2021-10-12 17:22:55.633000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
42	2021-10-08 16:57:49	2021-10-08 17:16:46	IP Address	107.15.172.217	100	1137	True	2021-10-08 17:16:47	R_1F9byYYgycDGif8					35.717803955078125	-78.84279632568358	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	The probability of being wrong is equal	Neither biased nor unbiased.	The probability of being wrong was equal	Mostly useful	It shows that they have almost a 50% chance of wrongly predicting someone needs the ICU	Neither fair nor unfair	I'm not sure, I think it is just representing the data.	Acceptably biased	African-American patients are notably treated poorly in the healthcare system, so the doctors might have had bias when determining whether they actually needed the ICU or not.	Neither useful nor unusable	I don't know the level of bias, so cannot decide	No		Probably model Y	Because the racial disparities did not affect the probability of being wrong	Probably model X	The results showing that African Americans were more likely to not need the ICU were probably biased	Models X and Y are equally useful	Not sure, it depends on the bias	Probably model Y	It seems more reliable	Probably ${e://Field/pref_model}	It had equal probabilities between races	Probably model Z	it predicted a higher number of african americans to need the ICU	Probably ${e://Field/pref_model}	It seems less biased	High			it directly affects the individual and might cost them more money that they didn't need to spend	High			It could result in death	Moderate			for the society, its members will be affected which can reflect on the views of the society	High			Individuals in the society such as family members will be highly affected	No			Yes			Yes			Advantaged			Secondary Education		60ff035e24c1d6686d67a70a	615f94f07f89d7a8afda6025	6160cce8a3919efd2a18de02	majority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160cce8a3919efd2a18de02	APPROVED	2021-10-08 22:57:47.197000	2021-10-08 23:16:51.408000	1144.211	20.0	84	0	100	2021-10-12 17:24:32.944000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	Yes	
43	2021-10-08 16:57:53	2021-10-08 17:17:55	IP Address	174.248.228.161	100	1201	True	2021-10-08 17:17:55	R_2QogIGPmFslxTOi					39.776397705078125	-86.10769653320312	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	It’s more in line/accurate 	Acceptably biased	There is bias in all the data but I think it’s better than the previous example 	Mostly useful	"More fair/accurate and useable data
"	Very unfair	The mistakes in the African American column are way off to the white column.	Very biased	They are able to predict white patterns with much more certainty.	Neither useful nor unusable	It’s not getting less care to people just not accurate.	No		Probably model X	It’s more accurate and less biased	Probably model X	It has a bigger discrepancy 	Models X and Y are equally useful	I think they both have good sata	Probably model X	More accurate information 	Probably model Z	The top graph is equal on the first chart 	Probably ${e://Field/pref_model}	The top chart is less equal	Probably model Z	It seems more fair/accurate 	Moderate			If it isn’t required and given the individual will be left with medical bills they didn’t need to pay and room for someone who needed that space is taken away	High			If they mistake it is not required but is they are missing potential life saving medical help.	Moderate			If someone is mistakenly admitted they are given resources that are taken away from other members of society who are potentially in need.	Low			That is more of a individual issue as it isn’t taking resources from anyone actually in need but harming an individual 	No			No			Yes			Advantaged			Secondary Education	Business	615d2f8eec57c5ed67cae1a7	615f94f07f89d7a8afda6025	6160cce5b1977acb91426c2d	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	6160cce5b1977acb91426c2d	APPROVED	2021-10-08 22:57:50.252000	2021-10-08 23:18:01.671000	1211.419	23.0	117	1	100	2021-10-13 00:23:54.145000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	Yes	
44	2021-10-08 16:56:27	2021-10-08 17:25:11	IP Address	67.171.169.140	100	1724	True	2021-10-08 17:25:11	R_1LBebY6dI5TPfeB					45.210006713867195	-123.20030212402344	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 1	Very unfair	predicting that white payments are more likely to be non-fraudulent when the data is showing it is 50/50 on fraudulent payments between white and african americans. 	Very biased	yes, it is predicting that whites are more likely to make non-fraudulent purchases. 	Completely unusable	it is inaccurate with its predictions based on the past data. 	Neither fair nor unfair	its a prediction model based on data. 	Neither biased nor unbiased	using past data, this was the conclusion for the prediction model. it does not seem to be based on race, it seems to be based on past fraudulent transactions. 	Mostly useful	looking at the data from the past to create a probable prediction is useful. it is not guaranteed accurate, its only a tool. 	Yes	i dont think it should be based on race. i think there should many other factors than race for the them to use to base their predictions. 	Definitely model Y	its 50/50 on accepting payments based on race. 	Definitely model X	its wrongly predicting that more african-american transactions are likely to be fraudulent. 	Probably model Y	i dont think either are truly useful because these predictions are based on race alone. 	Neither model X nor model Y	i would not use race to base my predictions.	Both ${e://Field/pref_model} and Z are equally fair	NEITHER of them are fair. 	Both ${e://Field/pref_model} and Z are equally biased	still basing predictions on race. 	Neither ${e://Field/pref_model} nor model Z	i would gather other data and leave race out of my predictions.			High	you will not receive your monthly payment from your renter. 			High	you are still not receiving your payment			Moderate	the individual may need to ask society, ie his family, for help when his rent was denied. 			Low	it may affect the landlords society, or family, if he doesnt receive the payment and is unable to pay his own bills because of this. 			No			Yes			Yes			Advantaged	Primary Education	Services Occupations	615e59297d3481515a868ab0	615f94f07f89d7a8afda6025	6160cc8ef2b45d16d3fed91d	minority	outcome-fpr	rent	False		model Y	bottom								Advantaged	6160cc8ef2b45d16d3fed91d	APPROVED	2021-10-08 22:56:23.303000	2021-10-08 23:25:17.668000	1734.365	34.0	16	0	100	2021-10-13 14:52:17.983000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
45	2021-10-08 15:52:13	2021-10-08 17:25:37	IP Address	162.246.50.245	100	5603	True	2021-10-08 17:25:37	R_2rCkC0pKpVVWvpI					30.893600463867188	-83.32350158691406	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The difference between the predictions for the black and white users is not much	Very unbiased	The prediction for fraudulent payments seem even for the black and white users.	Completely unusable	It has good scores in detecting fraudulent payments.	Acceptably fair	Its prediction is mostly correct.	Mildly unbiased	It favors the white over the black users.	Mostly ununsable	Its predictions are fairly correct.	Yes	The aspect of the blacks having higher scores for detecting fraudulent payments while the whites had a significantly lower score.	Probably model Y	The probability of it accepting payments from both races are equal and over 50%.	Definitely model Y	The probability of it declining fradulent payments from both races are not equal 	Probably model Y	The probability of it accepting payments from both races are equal and over 50%	Probably model X	Because as a black man I would be able to make payments without being declined by the AI's decision.	Probably model Z	Because the percentage of it mistakenly denying payments are relatively equal.	Definitely ${e://Field/pref_model}	Because the percentage of it mistakenly denying payments are relatively different for both the white and black.	Probably ${e://Field/pref_model}	Because as a black man it would be more probable for my payments to be accepted.			High	He would loose interest and probably start looking for somewhere else to live.			Low	Most people like freebies and would jump on any when given the opportunity.			High	It's not encouraging for people. Family			High	They'll feel guilty that it went through and try to make complaints. Family members.			No			No			Yes			Disadvantaged	Bachelor	Others	615b2ec8aa4707ea7652c447	615f943ec32164d0f282bd34	6160bd842408689aeb0b5f2b	majority	outcome-fpr	rent	False	You have chosen model X over model Y.	model X	top								Disadvantaged	6160bd842408689aeb0b5f2b	APPROVED	2021-10-08 21:52:09.929000	2021-10-08 23:25:52.906000	5622.977	30.0	116	1	100	2021-10-13 21:50:57.908000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Male	DATA EXPIRED	
46	2021-10-08 16:39:22	2021-10-08 17:25:49	IP Address	98.200.136.59	100	2786	True	2021-10-08 17:25:49	R_wYNDcmOf41cyT6h					29.64999389648437	-95.14849853515624	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Based on Graph 1 it appears that access is more likely granted to White people versus African American.	Very biased	Access granted to a medical device should not allow access based on race and it seems that Model X does so.	Completely unusable	The criteria Model X uses to grant someone access to a medical device seems very flawed and biased. This should never be the case.	Very unfair	Model Y should not mistakenly grant access to one race over another and it does so in this case.	Mildly unbiased	It's good that Model Y allows equal opportunity to granted access between White and African American. However, it seems Model Y favors granted access to White people by mistake versus African American.	Completely unusable	The criteria for granted access to medical devices in Model Y are not 100% fair and therefore unusable. 	Yes	I wish I had chosen mildly unbiased instead of very biased because I don't personally think it's giving evidence of being the MOST biased but it's giving enough.	Models X and Y are equally fair	I chose equally fair because they're both equal in the sense that they're equally unfair. Model X and Y show evidence of favoring White people versus African American via granted access to medical devices.	Models X and Y are equally biased	Both are equally biased in the sense that each discriminates against African American medical personnel by denying them access to medical devices more often than White medical personnel.	Models X and Y are equally useful	I chose equally useful bc they perform in similar ways of favoring device access to White medical professionals.	Neither model X nor model Y	However, both are equally useless because the criteria for granting access to medical devices is biased and unfair.	Both ${e://Field/pref_model} and Z are equally fair	Neither are fair, but both are consistent with their discrimination against African American personnel and favoritism toward White personnel.	Both ${e://Field/pref_model} and Z are equally biased	Both models are equally biased because they both follow the same criteria for granting medical access to White or African American personnel. In Model X, African American personnel are more often mistakenly denied access and in Model Z White personnel are more often mistakenly granted access. Both of these issues are the same thing.	Neither ${e://Field/pref_model} nor model Z	I wouldn't choose either model because they're both flawed in judging who is allowed access to medical devices.		Moderate		It would be difficult to constantly have to type in your username and password to access something vital in your routine; however, it would be even worse if access was granted to any individual other than the medical professional trying to.		High		If someone other than yourself somehow accessed a medical device under your name it would hold you personally responsible for the mistake and issues thereafter. It could possibly result in job loss and that would be detrimental to surviving. 		High		Failure to recognize an authorized hospital employee and having to repeatedly type in a username and passcode could take up valuable time in the case of something urgent and time sensitive. For example, if a patient was in critical care and a medical professional could not access something vital to their recovery it could possibly lead to the patient's death. The patient's death would have a ripple effect on the staff, family, friends, and the employer of said patient.		High		The impacts are high is access is mistakenly granted to someone who is not authorized access to specific information because said person could illegally obtain patient's private information and put them in danger. This would result in the patient being as risk and the hospital and staff overall of being sued.		Yes			Yes			Yes			Disadvantaged		Secondary Education	Services Occupations	615f65a5d630f43eb0d2438a	615f943ec32164d0f282bd34	6160c893a63ff8325b2c5434	minority	outcome-fpr	frauth	True		model X	top								Disadvantaged	6160c893a63ff8325b2c5434	APPROVED	2021-10-08 22:39:20.658000	2021-10-08 23:25:52.859000	2792.201	26.0	44	1	99	2021-10-14 01:49:09.392000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	No	
47	2021-10-08 17:09:33	2021-10-08 17:28:59	IP Address	75.132.92.108	100	1166	True	2021-10-08 17:28:59	R_1go00XfRTQMeDdp					38.731002807617195	-90.10970306396483	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 1	Mildly unfair	It still has a high likelyhood of wrongful fraud.	Acceptably biased	You payment may be wrongfully denied.	Mostly unusable	"It needs better fraud detection.
"	Very unfair	It is basically a 50/50 chance that your payment is accepted	Acceptably biased	It seems more leaning towards the chance of fraud.	Mostly ununsable	You have a higher chance of being flagged and payment not being accepted.	No		Definitely model X	It works better than Y but is still in the same boat.	Definitely model Y	It is more likely to wrongfully deny payment.	Definitely model X	It has less error.	Neither model X nor model Y	I would want better detection.	Definitely model Z	Same possibilty of denied wrongful payment. 	Definitely ${e://Field/pref_model}	Higher percentage of wrongful denied payment for african american.	Definitely model Z	Seems more equally represented.			High	Could potentially lead to a wrongful eviction.			High	Could be wrongfully reported to the credit bureau.			Low	It has a higher impact on the individual not everyone.			Moderate	Has impact on the person recieving the payment as well as the person losing their money wrongfully.			Yes			No			Yes			Disadvantaged	Secondary Education	Services Occupations	615dd79ed16dba94e3f7b2ef	615f94f07f89d7a8afda6025	6160cfa696c7e3e429debde5	minority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	6160cfa696c7e3e429debde5	APPROVED	2021-10-08 23:09:27.411000	2021-10-08 23:29:01.687000	1174.276	28.0	193	4	98	2021-10-13 00:33:21.278000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
48	2021-10-08 16:57:04	2021-10-08 17:35:38	IP Address	75.188.160.6	100	2314	True	2021-10-08 17:35:38	R_11hydzDjqpWOiO8					41.14129638671875	-81.34449768066406	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Very fair	the probability is still high to get access and the mistakes are even	Neither biased nor unbiased.	like i said its a machine i dont think machines are capable of being biased	Very useful	because its kind of even across the board. 	Very unfair	im assuming model y thinks most white people look alike lol	Neither biased nor unbiased	its a robot i dont know that it can be biased more of a tech issue	Completely unusable	thats a large percent of wrongs 	No		Probably model X	because theres not much of a difference between who they dont let in	Models X and Y are equally biased	the figures are basically the same 	Models X and Y are equally useful	the percent for mistakes and access are both about 20% 	Neither model X nor model Y	i would make a new machine	Probably ${e://Field/pref_model}	"its 69% accurate on denying people 
and 70% of letting people so its even across the board basically"	Both ${e://Field/pref_model} and Z are equally biased	its not that much of a difference between whats goong on 	Definitely model Z	the percentage of granting access by mistake is lower all together		Low		i dont think any one personally cares thats more of a convenience issue if anything 		High		well obviously they are not supposed to be let in anyways so they are somewhere they dont need to be anyways		Low		because technology isnt always correct i would prefer if someone who had access to me would use a password -me being others-others being society.		High		society wouldnt like it . what if the tech mistakenly allows in a killer by accident then what		Yes			Yes			Yes			Disadvantaged		Primary Education	Others	614f7fa72284cc56c37bd6ed	615f943ec32164d0f282bd34	6160ccbac4d0b4cf13eafdda	minority	outcome-fpr	frauth	False		model X	top								Disadvantaged	6160ccbac4d0b4cf13eafdda	APPROVED	2021-10-08 22:57:01.688000	2021-10-08 23:35:44.062000	2322.374	22.0	88	0	100	2021-10-13 19:33:03.382000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
49	2021-10-08 17:11:00	2021-10-08 17:36:57	IP Address	108.70.136.206	100	1557	True	2021-10-08 17:36:58	R_2Cg38SiLDZWI76K					41.57980346679688	-83.66600036621092	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Very unfair	Reads more white people than African Americans.  And has a high mistake margin	Very biased	Reads white peoples faces better 	Mostly unusable	Low face readings and high mistakes	Very unfair	It will only correctly read half the faces.	Very unbiased	Reads both white and African-Americans equally 	Mostly ununsable	It dont read enough peoples faces	Yes	The first model looks way better now that the models are side by side	Definitely model Y	Not biased and African American  mistake column  is way lower	Definitely model X	Reads white peoples faces 20% better	Probably model Y	Less biased	Neither model X nor model Y	Both have high  mistakes and none of the numbers are great. 	Probably model Z	Numbers  are equal 	Definitely ${e://Field/pref_model}	Its twice as high	Probably model Z	Better numbers 		High		If there is an emergency,  you would want doctors to have access asap. If they cant access, could be  life of death 		High		You dont want unauthorized  people to have acces to medial records and medications 		Low		Society  is the general public.  You can have a mistake  reading a face, but the username  and password adds extra security 		High		If society (people) sees a normal person  accessing  doctor files that would be a huge lawsuit 		No			Yes			Yes			Advantaged		Bachelor	Transportation Occupations	615e298e91bab724990e29ec	615f94f07f89d7a8afda6025	6160cfff318418a8fa15c3ce	minority	outcome-fpr	frauth	False		model Y	bottom								Advantaged	6160cfff318418a8fa15c3ce	APPROVED	2021-10-08 23:10:58.395000	2021-10-08 23:37:02.225000	1563.83	36.0	49	2	96	2021-10-13 01:52:51.485000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
50	2021-10-08 16:42:45	2021-10-08 17:37:05	IP Address	107.77.228.22	100	3259	True	2021-10-08 17:37:05	R_22G2Y7UTHheXFIG					34.06719970703125	-118.30160522460936	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	the probability to allow payments from African Americans is greater then white	Neither biased nor unbiased.	the same percentage of fraudulent allowed for both white and African americans	Completely unusable	It shows a bias that isn't confirmed by the number actual fraudulent payments	Neither fair nor unfair	payments are allowed equally for both withe and African American	Acceptably biased	21%  more payments allowed by African Americans are fraud but allowed	Mostly useful	yes. it can be used to combat fraudulent payments	Yes	That the model didn't show African Americans getting more fraudulent payments through. Im not quite sure I understand the data	Probably model Y	because total of mistakenly allowed payments is less in model Y	Probably model X	because for the probability of allowing payments is equal for both races	Probably model Y	because total of mistakenly allowed payments is less in model Y	Probably model Y	because total of mistakenly allowed payments is less in model Y	Probably ${e://Field/pref_model}	probability of allowing payments is equal with both races	Probably model Z	Z allows more payments from African Americans over white	Probably model Z	total mistakenly denied payments is less than Y 			Moderate	If payment is denied, renter could be charged late fees for machine learning that made a mistake			High	allowing fraudulent payment is a bigger issue that machine learning should detect. that would be allowing payment someone know is fraudulent			Low	payments can always be remade. usually people are notified right away when a payment did not go through			High	someone has gotten away with cheating the system. They will never fess up to their crimes and payment will be lost, profits lost.			No			No			Yes			Advantaged	Bachelor	Healthcare Practitioner	615de4a661d61a75f46c0aa9	615f94f07f89d7a8afda6025	6160c957543d0ea61899c9bd	majority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160c957543d0ea61899c9bd	APPROVED	2021-10-08 22:42:43.263000	2021-10-08 23:37:09.030000	3265.767	35.0	173	0	100	2021-10-13 00:39:11.138000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
51	2021-10-08 17:05:57	2021-10-08 17:38:21	IP Address	73.113.126.51	100	1943	True	2021-10-08 17:38:22	R_YbRxQaEvYSOb2Vj					36.08689880371094	-84.189697265625	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	It seems to somewhat work.	Neither biased nor unbiased.	It seems to work better for african americans.	Mostly useful	It seems to work better than the other one.	Neither fair nor unfair	It seems to only work half of the time.	Mildly unbiased	It seems to work better for white people.	Neither useful nor unusable	It seems like it's okay, but is it worth the cost for how it works.	No		Probably model Y	I guess it give an equal change of being recognized.	Probably model Y	It has a higher change mistakenly granting one group access than the other.	Probably model Y	I guess Y is better because it has a better chance of working for both groups.	Probably model Y	I guess Y is actually the better choice because of the equal chance of working.	Probably model Z	The same change to mistakenly deny access.	Probably ${e://Field/pref_model}	Different rates of mistakenly denying access.	Probably ${e://Field/pref_model}	I guess I would stick with Y because it has a better change of equally recognizing both groups.		Moderate		It seems like it would only slightly delay treatment.		Moderate		It could allow items to be stolen or misused.		Low		I don't see how society could be affected by a hospital staff having to enter a username/password.  I consider society to be all of the people that live in the town/city/neighborhood.		Moderate		It does possibly lead to medical supplies being misused. I consider society to be all of the people that live in the town/city/neighborhood.		No			Yes			Yes			Advantaged		Bachelor	Business	5f481f0a4810bc2d025c56f4	615f94f07f89d7a8afda6025	6160ced05e3ddce7b3d294e1	majority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160ced05e3ddce7b3d294e1	APPROVED	2021-10-08 23:05:55.936000	2021-10-08 23:38:25.680000	1949.744	44.0	276	0	100	2021-10-12 16:38:22.211000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
52	2021-10-08 17:28:44	2021-10-08 17:40:42	IP Address	173.23.120.4	100	717	True	2021-10-08 17:40:43	R_2tstVgNORLBm91z					42.01609802246094	-93.44400024414062	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Gives uneeded ICU at the same rate.	Very unbiased	Gives support at the same rate.	Mostly useful	It is able to predict for black patients well, while predicting moderately well for their white counterparts.	Mildly unfair	I would say it's slight unfair that blacks received ICU support more than whites when needed, but hopefully this ends up making up for the bias present when allocating health resources.	Mildly unbiased	Gives ICU support at the same rate.	Mostly useful	It predicts more than half the time.	No		Probably model X	Predicts at higher rates, but mistakenly gives support at lower rates.	Models X and Y are equally biased	I don't think they help any one specific group more than another.	Probably model X	It predicts ICU needs more often and distributes uneeded resources less often.	Probably model X	Less biased, more accurate.	Probably ${e://Field/pref_model}	It mistakenly denies support to marginalized people less often.	Probably model Z	It increases denial of care to black people in particular	Probably ${e://Field/pref_model}	It seems more fair.	Low			Unnecessary care is better than denial of care.	High			Can result in harm to patient	Moderate			Puts cost on society for care that isn't needed.	Moderate			Denial of care can hurt someone's ability to be a contributing member of society	Yes			No			Yes			Advantaged			Bachelor	Education	5ec45c9987c50514a1d57cd7	615f94f07f89d7a8afda6025	6160d4283bfda6f7b3084bcd	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	6160d4283bfda6f7b3084bcd	APPROVED	2021-10-08 23:28:43.060000	2021-10-08 23:40:45.375000	722.315	20.0	99	1	100	2021-10-12 16:35:20.414000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
53	2021-10-08 16:28:14	2021-10-08 17:41:16	IP Address	47.202.171.220	100	4381	True	2021-10-08 17:41:16	R_1JFNFT5LBxRbht7					27.474899291992188	-82.61859893798828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	If equal % were incorrectly allowed a payment,  I would expect to see similar results for being allowed. However, there is almost a 25% difference between the 2 groups. Also, would help to know total % of renters in each group. 	Very biased	If same % were incorrectly allowed payment when they should not have been, the difference for those allowed should be closer unless there are a significantly lower total number of white renters vs black. Also, renters who do not identify as white or black are either not represented or are incorrectly grouped into one of the 2.	Mostly unusable	Renters who do not identify as white or black are either not represented or are incorrectly grouped into one of the 2.	Neither fair nor unfair	"It appears the threshold for accepting risk may be a too low as 47% of all renters would be denied payments that are valid.
"	Mildly unbiased	"While too low, the % of acceptance is consistent, which is why I went with mildly UNbiased.
The assumption that renters are either black or white seems flawed.
Also, seems a significant % of black renters were able to pay with fraudulent means inspite of tge low risk threshold. I would expect that % to be closer."	Mostly useful	% of acceptance is closely aligned so tweaks to risk threshold may help increase acceptance for all renters.	No		Probably model Y	More standardization in accepting payments	Models X and Y are equally biased	I struggle with there only being these 2 groups of renters and wonder how other groups impact these numbers. 	Probably model Y	Payment acceptance is aligned. Need additional refinement on the incorrect acceptance and where other groups (not white or black) fall.	Probably model Y	Payment acceptance is aligned. Need additional refinement on the incorrect acceptance and where other groups (not white or black) fall.	Both ${e://Field/pref_model} and Z are equally fair	Percentage is closer for acceptance in Y and similar for denials in Z	Both ${e://Field/pref_model} and Z are equally biased		Neither ${e://Field/pref_model} nor model Z	They both need work			Moderate	"Can make renters feel ""less than"" and renters are often already looked down on as not taking good care of a property,  have fewer rights when it comes to pet ownership, etc."			Moderate	If I am the individual renter, I assume this means I didn't actually pay with my money. The landlord is affected as they still have to collect, and there individuals impacted if their account was tge one used fraudulently 			Moderate	Provided the % of incorrect denials are periodically evaluated to improve accuracy, overall renters (society) will benefit.			Moderate	If renters are grouped as just black or white, allowing fraudulent payments can skew their numbers. Also, it can increase the cost of renting as landlords seek to recover costs.			No			Yes			Yes			Advantaged	Secondary Education	Engineering and Technology	614f2e243524f6947e0492d7	615f94f07f89d7a8afda6025	6160c5efb019ad8a5167a64f	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160c5efb019ad8a5167a64f	APPROVED	2021-10-08 22:28:10.136000	2021-10-08 23:41:19.979000	4389.843	48.0	39	0	100	2021-10-13 15:01:00.253000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
54	2021-10-08 17:10:51	2021-10-08 17:43:03	IP Address	76.250.170.71	100	1931	True	2021-10-08 17:43:04	R_31GdDYL0iOrC0HW					33.87800598144531	-83.89230346679686	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	It appears to display the reasonable number of those receiving access.	Neither biased nor unbiased.	Because both races have an equal margin for acceptance on the second graph.	Neither useful nor unusable	Because an equal amount of the possibility of being granted access.	Acceptably fair	Because it seems to have a better probability.	Neither biased nor unbiased	No, because both males and females are ranked the same on the model on the top graph.	Mostly useful	It appears the information provided helps to determine the percentage granted access.	No		Probably model Y	Because it seems the most fair.	Models X and Y are equally biased	They both show they are biased.	Models X and Y are equally useful	Both have numbers that could be useful to determine acceptance.	Neither model X nor model Y	They both seem equal to me, as nothing stands out to me.	Both ${e://Field/pref_model} and Z are equally fair	I'm not sure they seem very similar to me.	Both ${e://Field/pref_model} and Z are equally biased	They both seem to be equally biased.	Neither ${e://Field/pref_model} nor model Z	They both seem equally biased to me.		Moderate		There a moderate amount that would be able to bypass and gain access.		Moderate		Because the impact could potentially detrimental.		Moderate		Because the part of society, both blacks and whites could be significantly impacted by the unauthorized access.		Moderate		Both races seemed to be significantly impacted.		Yes			Yes			Yes			Disadvantaged		Master	Business	61549c0c382eb7ca6b1879a6	615f943ec32164d0f282bd34	6160cff34ef78642d9d284d2	majority	outcome-fpr	frauth	False		model Y	bottom								Disadvantaged	6160cff34ef78642d9d284d2	APPROVED	2021-10-08 23:10:48.789000	2021-10-08 23:43:17.088000	1948.299	47.0	23	1	97	2021-10-14 16:50:47.670000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	No	
55	2021-10-08 17:17:30	2021-10-08 17:44:06	IP Address	69.248.187.184	100	1596	True	2021-10-08 17:44:07	R_7ZGtTMWg5OhlZxn					39.787506103515625	-75.17970275878906	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Again, the model is skewed I  a racially biased manner that will be frustrating for black users.  	Very biased	The model is biased toward white users	Mostly unusable	Similar issues to the prior model.  It cannot identify with high probability the user, nor does it deny access to incorrect users with a high enough probability 	Very unfair	While the probability of granting access is the same, the model is still skewed to make mistakes in a racially biased manner	Very biased	The model does not correctly identify white faces	Mostly ununsable	The high probability of granting access to incorrect people threatens patient security.  The low probability of correct identification would make it frustrating to use.	No		Probably model Y	Being denied access that should be granted is more frustrating than users who should not mistakenly getting access	Models X and Y are equally biased	They are biased in different ways, but each can not identify either white or black faces with high probability 	Models X and Y are equally useful	Neither is useful.  Something that works less than 60% of the time is inadequate 	Neither model X nor model Y	I would prefer to type in a user name and password than deal with something that only sometimes functions	Definitely ${e://Field/pref_model}	Being incorrectly denied is more frustrating, and the racial bias makes model z seem more unfair	Both ${e://Field/pref_model} and Z are equally biased	Again, both have a bias to make mistakes based on race	Neither ${e://Field/pref_model} nor model Z	They are still ineffective at granting easier access.		Moderate		Simply requiring the password is low impact.  If the individual is aware of the model's racial bias, it could result in the individual feeling discriminated against. 		Low		To the user who has been mistakenly granted access, they are likely to just walk away in most cases		High		Waiting for the system to deny access improperly could result in treatment delays for patients.  Knowledge of the bias could fuel distressing feelings related to racial bias in broader society- society here meaning all people 		High		This could be very dangerous for the society of patients receiving services		No			No			Yes			Advantaged		Bachelor	Others	615ef0e03cfd6aeefdc27946	615f94f07f89d7a8afda6025	6160d183c4b810a5d8f98e35	minority	outcome-fpr	frauth	False		model Y	bottom								Advantaged	6160d183c4b810a5d8f98e35	APPROVED	2021-10-08 23:17:25.686000	2021-10-08 23:44:16.306000	1610.62	41.0	46	0	100	2021-10-13 02:06:39.891000	19AE28A9	United States	United States	Due to start a new job within the next month	Caucasian	English	United States	Female	No	
56	2021-10-08 16:48:32	2021-10-08 17:44:30	IP Address	132.198.16.180	100	3358	True	2021-10-08 17:44:30	R_1rfhwtwNo5WBwvr					44.46809387207031	-73.19300079345702	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The difference in probability of allowing payment seems unfair. 	Acceptably biased	This model seems biased toward African Americans because there is a much higher chance of allowing a payment. 	Mostly unusable	35% mistakenly allowed payments still seems too high to be useful and correctly doing its job. 	Acceptably fair	The probability of allowing payment is the same for both groups. 	Mildly unbiased	If Model Y was biased, I think it would be more likely to accept payment from one group over another. Mistakenly allowing payments from more African Americans does not mean does not mean it is biased towards African Americans, because it could be mistakenly rejecting valid payments as well. 	Mostly ununsable	24% and 45% of mistaken acceptances seems a little too higher to reliably do this job, and seems like it would just create more work fixing those mistakes. 	Yes	I confused myself about which graph would be connected to fairness and which would be connected to bias. 	Probably model Y	I think having a similar probability of accepting a payment makes model Y more fair. 	Probably model X	I think having a higher chance of accepting a payment for African Americans means model X is biased. 	Models X and Y are equally useful	The percent of mistakenly accepted payments for each is too high to be trustworthy. 	Probably model Y	Despite model X having similar percents of mistakenly accepted payments for both groups, I think it's possible that data on mistakenly rejected payments could still reveal problems, so it's probably safer to go with the model that has the same probability of accepting a payment. 	Probably ${e://Field/pref_model}	I think having the same probability of accepting payment is more fair. 	Probably model Z	I think having a difference in the probability of accepting payments from either group is having a bias. 	Probably ${e://Field/pref_model}	I think it is correct to have the chance of allowing payment for both groups be equal. The problem is that it's choosing disproportionately rejecting payment from some people and accepting payment from others.			High	If payment from the renter is consistently mistakenly denied, that means the individual is losing income. 			High	Mistakenly allowing fraudulent payment also causes the individual to lose income.			Low	I considered other renters and other people renting out their property as society. I think as long as this doesn't happen too frequently, this wouldn't affect others. 			Low	I don't think this would happen too much effect on other renters or the individual's friends and family, assuming it doesn't happen frequently			No			Yes			Yes			Advantaged	Bachelor	Engineering and Technology	60fccf14ec98489192e54358	615f94f07f89d7a8afda6025	6160cabb077fedf5a8923305	majority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160cabb077fedf5a8923305	APPROVED	2021-10-08 22:48:30.409000	2021-10-08 23:44:33.346000	3362.937	19.0	68	0	100	2021-10-12 17:19:07.979000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
57	2021-10-08 17:11:35	2021-10-08 17:46:43	IP Address	108.184.20.97	100	2107	True	2021-10-08 17:46:43	R_27eS2n6HJAR3PCu					33.86509704589844	-118.07170104980467	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	I don’t believe the graph gives me enough information to decide if it’s fair or not.	Mildly unbiased	The percentage of patients mistakenly sent to the ICU is equal.	Mostly useful	A good percentage of patients sent to the ICU really needed to be sent there so I think it’s beneficial.	Neither fair nor unfair	They are sending the same percentage to the ICU.  	Neither biased nor unbiased	Again, they are sending the same percentage to the ICU.	Mostly useful	People who need to be sent to the ICU are being sent there.    The percentage of black people being mistakenly sent is a bit high though.	Yes	When looking at both models it looks like they should be sending more white people to the ICU.	Probably model X	I guess it’s more fair if both groups have the same amount being mistakenly sent to the ICU.	Probably model X	Because they aren’t sending the same percentage to the ICU.	Models X and Y are equally useful	I think the total amount of people mistakenly sent to the ICU is about equal in each.	Probably model X	Because the percentages of mistakenly sent are equal.	Definitely model Z	Because those mistakenly denied icu support is equal.	Probably ${e://Field/pref_model}	Because more white people are mistakenly denied icu support.	Definitely model Z	Again, there’s the same percentage of those mistakenly denied support.	High			Because mistakenly giving someone an ICU bed when they don’t need it means someone who does need it won’t have it available.	High			They could die without the ICU support.	Moderate			Society means those not needing the ICU.  Could mean higher insurance costs.	Moderate			Society means those not needing the ICU.  They could lose loved ones.	No			No			Yes			Advantaged			Bachelor	Engineering and Technology	615b52a780ee6b2916998ce6	615f94f07f89d7a8afda6025	6160d01a4191ef5be1e7c96c	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	6160d01a4191ef5be1e7c96c	APPROVED	2021-10-08 23:11:26.827000	2021-10-08 23:46:47.736000	2120.909	37.0	106	0	100	2021-10-12 17:29:30.245000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Caucasian	English	United States	Female	No	
58	2021-10-08 16:50:30	2021-10-08 17:50:49	IP Address	24.170.125.190	100	3619	True	2021-10-08 17:50:50	R_1i3AGKQXWyCudzQ					32.62269592285156	-97.1479034423828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Very unfair	The top graph  is going by the color of one's skin to permit entry. The bottom graph doesn't discriminate.	Acceptably biased	Yes it has its mistakes especially when it comes to the skin color of a person.	Completely unusable	It's a machine and it can only do so much without the help of an actual human person. It could let any stranger through the door who isn't a doctor and do harm to patients. 	Mildly unfair	Face recognition discriminates even if its a small percentage towards African Americans. It doesn't matter if the top graph is equal.  	Very biased	Pretty much waiting for a lawsuit to happen. If anyone who isn't white realizes that they're being denied access because of color then the company will be sued. It's not the person's fault that the machine is a major failure.	Neither useful nor unusable	This is useless because your running the risk of someone violent or crazy getting granted access to important files or entry.	No		Probably model Y	Model y is equal in granting access and for the mistakes it needs improvement.	Models X and Y are equally biased	You'll run into issues and complaints from people.	Probably model Y	Model Y needs improvement where everything from access of entry to mistakes is equal.	Neither model X nor model Y	Given all that this happening in this world this just adds to more problems when dealing with all types of people of different races.	Probably ${e://Field/pref_model}	Mistakes are equal and although access is mistakenly denied is not as bad discriminating ones skin color.	Probably model Z	It has mistakes that need to be fixed on bottom graph.	Neither ${e://Field/pref_model} nor model Z	Any stranger could be given access.		Moderate		Typing or inputting your password by hand will not affect the company in the long run. On the other hand if your in a rush to get to a patient every minute is vital.		High		"A stranger could get access to files or entry and do harm to people.
"		High		Family members needing you immediately to attend to a love one could see it as a big flaw that the system is lacking. 		High		One stranger who is mentally unwell has access can hurt a lot of people and get police involved as well.		No			Yes			Yes			Disadvantaged		Bachelor	Others	615268b72f1168b0d740d1ab	615f943ec32164d0f282bd34	6160cb2fef711c4b5d7f0dfe	minority	outcome-fpr	frauth	True		model X	top								Disadvantaged	6160cb2fef711c4b5d7f0dfe	APPROVED	2021-10-08 22:50:27.643000	2021-10-08 23:50:53.059000	3625.416	43.0	93	4	89	2021-10-14 15:39:06.611000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	English	United States	Female	No	
59	2021-10-08 17:20:03	2021-10-08 17:51:05	IP Address	107.130.73.84	100	1861	True	2021-10-08 17:51:06	R_3sgZ8y7KEaix5UX					38.000396728515625	-121.3105010986328	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very fair	Mistake rates for both racial groups are equivalent.	Very unbiased	Mistake rates are equivalent. 	Mostly useful	There is no bias for either of the demographic groups. 	Very unfair	A large discrepancy exists between the number of African-Americans mistakenly given ICU treatment and the number of whites.	Very biased	It is biased toward African-Americans in that more are given ICU care who do not require ICU care, despite equal probabilities of predicting ICU being required. 	Mostly ununsable	It makes incorrect predictions with one subset of the population nearly half the time. 	Yes	I would like to amend my answer that the model was biased, because the mistake rates wound up being similar. 	Definitely model X	The number mistakenly receiving ICU care for both demographic groups is equivalent in Model X. 	Definitely model Y	In Model Y there is a large discrepancy in the number of patients who mistakenly receive ICU care in one of the demographic groups. 	Definitely model X	Less bias in Model X, less of an error rate as well. 	Definitely model X	Less bias and less of an error rate. 	Probably model Z	Rates for those mistakenly denied ICU care are equivalent in Model Z.	Definitely ${e://Field/pref_model}	The discrepancy in mistake rates in Model X is larger than in Model Z.  	Definitely model Z	Large differences in the number of people mistakenly denied care in the two groups in Model X, whereas that number is equivalent in Model Z. Being denied necessary care is more unjust than being granted care that was not needed. 	Low			An individual is receiving extra care. In should not put them in any harm. 	High			Could die from not getting lifesaving care.	High			Societal resources, resources for people in the local community, are misplaced and people who need care might not be getting it. 	High			The medical community will suffer as society, i.e. local community, will not trust them to make proper decisions about their care, and may believe the medical authorities are biased against a particular group.	No			No			Yes			Disadvantaged			Doctoral	Services Occupations	615380531397af48eeecbe86	615f94f07f89d7a8afda6025	6160d21c7f5d6647e005c3c3	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Disadvantaged	6160d21c7f5d6647e005c3c3	APPROVED	2021-10-08 23:19:59.218000	2021-10-08 23:51:09.868000	1870.65	38.0	24	0	100	2021-10-13 14:21:27.004000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Male	No	
60	2021-10-08 17:19:28	2021-10-08 17:52:46	IP Address	69.135.58.83	100	1997	True	2021-10-08 17:52:47	R_1gj2nYC5EuslMDF					38.04319763183594	-84.49729919433594	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very fair	The model uses objective data of the percentage of each group that it allowed payment for. The results reflect this.	Very unbiased	The model uses objective data of right and wrong predictions. While it made incorrect predictions, these are not necessarily biased.	Mostly unusable	Again, a model based on race is not the most accurate indicator that could be used.	Very fair	Payments were allowed equally between the White and the African American groups.	Very unbiased	It is unbiased because they allowed an equal percentage of payments to go through in both groups.	Mostly ununsable	I don't think a model that separates based on race is the most effective way to predict fraudulent payments. This kind of model could lead to racial profiling that would be not be an accurate reflection of the pool of people who make fraudulent payments. Other factors more correlated with payment method should be considered as indicators instead.	No		Models X and Y are equally fair	Neither is more fair because the predictions could have been made based on factors outside of race, and this breakdown just shows the racial disparity.	Models X and Y are equally biased	While I don't necessarily think that the models are biased, all models are bound to have some form of bias, and both are equal in that.	Models X and Y are equally useful	I don't find either models particularly useful for this purpose.	Neither model X nor model Y	I would not choose either model as a predictor. I would choose a model based on other determining factors.	Both ${e://Field/pref_model} and Z are equally fair	Both models are fair given that these predict allowance based on other factors.	Both ${e://Field/pref_model} and Z are equally biased	I don't particularly think either is very biased, but since all models are bound to be biased on some level, I would say they are equal.	Neither ${e://Field/pref_model} nor model Z	I would not choose this method of prediction.			High	If someone who is being truthful tries to make a payment and you do not allow them to, that is a customer you are losing business from. Additionally, you face the repercussions of offending that customer and having them tell others about how they were denied mistakenly.			High	Allowing a fraudulent payment causes high impact because in this case, the renter will lose that money that they would have been paid by the tenent.			High	If an individual is denied based on a model that made predictions on race, that society could be more apt to make judgments on the race of that individual.			Low	Overall, a fraudulent payment isn't going to have much impact on the society. Those that suffer from this are the individuals.			Yes			Yes			Yes			Advantaged	Bachelor	Others	615cb4e3c0e2f5a9a7299ab2	615f94f07f89d7a8afda6025	6160d1fcd75c4b909ce789ff	majority	outcome-fpr	rent	False		model Y	bottom								Advantaged	6160d1fcd75c4b909ce789ff	APPROVED	2021-10-08 23:19:26.646000	2021-10-08 23:52:49.211000	2002.565	23.0	140	0	100	2021-10-12 17:50:41.982000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Female	Yes	
61	2021-10-08 16:52:39	2021-10-08 17:55:50	IP Address	47.205.222.73	100	3790	True	2021-10-08 17:55:50	R_2fqClgP0EJyGCPW					27.40669250488281	-82.53140258789062	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	It shows the different races having the even probability of making fraudulent payments in the 2nd graph. The first graph seems to be reasonably fair as well, as the percentages between White people and African Americans are not too far off from one another either.	Neither biased nor unbiased.	I don't believe that the models are wholly unbiased, as the first graph might be catering more towards being anti-racist, but I don't believe that it is very biased either.	Mostly useful	It shows an even probability of rejection across both races and also gives a decent estimate on which of the two races presented will make non-fraudulent payments.	Very unfair	There are considerably more African American people assumed to be making fraudulent payments than White people.	Very biased	The model is assuming that a minority, mainly African Americans, will be the ones to make more fraudulent payments. This model looks to be racist.	Mostly ununsable	There needs to be a more accurate and less biased model that is created.	No		Probably model X	There is an equal chance of either race making fraudulent payments.	Probably model X	Model Y is assuming that a minority that is consistently discriminated against will be the ones to slip fraudulent payments through the system.	Probably model X	Model X allows for the fairness of assuming that both races are equally possible to submit fraudulent payments that slip through the cracks. I also know that, based on university classes I have taken, White people are statistically more likely to commit white-collar crimes, and I feel as though the graph takes that into account.	Definitely model X	Model X allows for the equal probability that either race can attempt fraudulent payments, but I feel as though Figure 1 in Model X takes into account the statistically increased likelihood of White people committing white-collar crimes like fraud.	Both ${e://Field/pref_model} and Z are equally fair	Both models seem to be generally unbiased.	Both ${e://Field/pref_model} and Z are equally biased	Both models do not seem to be very biased. Model X might be more biased against White people due to the statistics of them committing white-collar crimes, and Model Z might be more biased in favor of African Americans by seeming less racist. But generally, I do not see much bias in either.	Probably ${e://Field/pref_model}	Model X seems to be illustrating that there is an equal opportunity for both White and African American people to commit fraud, but Model X may take into account the frequency of White people committing white-collar crime.			High	It can offend an individual to be denied payment when that individual knows that they have not committed any sort of fraud. However, different races may be more or less offended due to different prejudices that unfortunately exist. It can also lead to unfair accusations later on down the road if interrogated for (false) accusations of committing fraud.			Low	If a fraudulent payment is allowed and no repercussions come of it or if it is never caught, there is little to no consequence for the person committing this fraud.			Moderate	"If these mistakes are not rectified and are seen as fraud, then those who are innocent can be negatively impacted in the future if they are trying to take out a loan or rent something. They can also have legal turmoil arise if these mistakes are not fixed. I consider landlords, the immediate loved ones, and banks to be a part of the ""society"" in my answer."			Low	"If there is no repercussions that follow after committing actual fraud, then there are no other third parties that will know, and therefore have un-waivered opinions of these individuals. I also consider landlords, the immediate loved ones, and banks to be a part of the ""society"" in this answer."			No			Yes			Yes			Advantaged	Secondary Education	Others	615d1a53e3b3a9b66ce49210	615f94f07f89d7a8afda6025	6160c9ebe90aef2178c1af53	majority	outcome-fpr	rent	False	You have chosen model X over model Y.	model X	top								Advantaged	6160c9ebe90aef2178c1af53	APPROVED	2021-10-08 22:52:35.980000	2021-10-08 23:55:52.758000	3796.778	21.0	151	0	100	2021-10-13 00:22:18.759000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
62	2021-10-08 17:25:55	2021-10-08 18:05:05	IP Address	104.182.54.187	100	2349	True	2021-10-08 18:05:05	R_3fNLtb8Ks7t7erZ					35.896392822265625	-78.71150207519531	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	I think based on statistical data it is easy to presume it is a fair table of probability between the dominant races.	Mildly unbiased	I think it potentially could be mildly biased based on statistics that African American population does not have appropriate medical care and therefore may be sicker than someone from the white population needing care.	Neither useful nor unusable	It appears that it is not a reliable source to accurately predict which patient needs ICU care and therefore it would only add to the burden of medical care cost to the patient if they are unnecessarily given ICU care vs. General Care	Neither fair nor unfair	It equally predicts which population needs ICU care 	Neither biased nor unbiased	It does not appear to be biased since the recommendations are equal among both races.	Completely unusable	I think this type of model would place an unnecessary burden on the ICU as well as the cost on the patient	No		Probably model X	Model X is more accurate with the probability for African American population 	Probably model Y	Model Y is more biased that African Americans will need ICU treatment and also most mistaken for African Americans	Probably model X	Model X appears to be more accurate in probability than Model Y	Probably model X	It had smaller margins for mistaken predictions than Model Y	Probably model Z	Because both races were denied equally	Probably ${e://Field/pref_model}	I think Model X was more biased against White population needing ICU care and denying it	Probably ${e://Field/pref_model}	I would choose Model X because the margin of error is smaller 	High			The individual will now have a burden of extra expenses to pay for the increased level of medical care costs that were not actually necessary	High			If the individual needed ICU care and did not receive it the end result could be longer recovery time and/or death of the individual	Moderate			Society, as in the family of the individual, may have more unnecessary emotional impact from their loved one being in ICU and unnecessary medical expenses.  Society, as in the medical staff, may have more strenuous work load caring for individuals that did not need extensive are.	High			Society, as in the family of the individual, will have the burden of wondering if their loved one did not get the appropriate care.  Society, as in the medical staff, will have the burden of more emergency medical care if the individual wasn't given proper medical care and now more measures need to be done that may have been able to be avoided.	No			Yes			Yes			Advantaged			Secondary Education	Business	5ef764fd0ea7fe48a5fa130d	615f94f07f89d7a8afda6025	6160d37c6de6673f8e24e691	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	6160d37c6de6673f8e24e691	APPROVED	2021-10-08 23:25:52.787000	2021-10-09 00:05:07.393000	2354.606	42.0	176	0	100	2021-10-12 16:37:00.687000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	No	
63	2021-10-08 16:59:30	2021-10-08 18:08:01	IP Address	174.80.9.133	100	4110	True	2021-10-08 18:08:01	R_325PcqnyRSN7IPr					34.921295166015625	-85.14859771728516	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	White payments are allowed 41% of the time, and African American payments are allowed 65% of the time. There is a 24% difference of advantage toward African Americans than Whites. The race consideration is very unfair. The bottom figure indicates that it is equally likely (Whites at 35% and African Americans at 35%) that the model will mistakenly allow a payment as non-fraudulent when it is, in fact, fraudulent. This is also very unfair because, while the percentages are equal, the payments are still being allowed.	Very biased	White payments are allowed 41% of the time, and African American payments are allowed 65% of the time. There is a 24% difference of advantage toward African Americans than Whites. The race consideration is very unfair. 	Completely unusable	Both data sets present unfair outcomes with regard to race and the mistakenly allowing fraudulent payments to go through.	Mildly unfair	The top figure shows that White payments are allowed at 53% and African American payments are allowed at 53%, making this a viable prediction figure because it's non-discriminatory and, therefore, fair. The bottom figure is very unfair because it discriminates by indicating that African American payments are non-fraudulent more often than White payments. There is no indication anywhere that White payments will be more fraudulent than African American payments, so this figure is misleading.	Mildly unbiased	In the case of the Y model and the bottom figure, there is bias, but it's toward the minority (African American) payment rather than the White payment. I'm not sure how knowing this information benefits either group.	Mostly useful	The top figure indicates that the Y model, in part, is not biased or programmed to be biased toward any race. However, the bottom figure tends to have a positive bias toward African American payments over White payments as being non-fraudulent.	No		Probably model Y	The top figure in Model Y has an equal probability of allowing a payment, regardless of race. There is a slightly less discrepancy between mistakenly allowing a payment in the bottom figure of Model Y (White 24% to African American 45% = 21% difference, in this case favoring African Americans over Whites) and allowing a payment in the top figure of Model X (White 41% to African American 65% = 24% difference, in this case trusting African Americans over Whites).	Definitely model X	While mistakenly allowing a payment is equal between White and African American as shown in the bottom figure, African Americans are trusted more than Whites when allowing a payment. This is based on race and is discriminate and unfair.	Probably model Y	Model Y is more useful because there is no racial bias for allowing a payment. While there is a slight bias toward African Americans and mistakenly allowing a payment as shown in the bottom figure of Model Y, the model could have been programmed to give the benefit of the doubt to the minority so as not to show bias.	Definitely model Y	The probability percentage of allowing a payment in the top figure of Model Y (53%) shows no racial bias and is higher than the percentage of mistakenly allowing a payment in the bottom figure of Model X (35%) regardless of race.	Probably model Z	There is an equal probability of Whites (32%) and African Americans (32%) being mistakenly denied a payment in the bottom figure of Model Z. While this may seem unfair, there is no racial bias present like there is in the bottom figure of Model Y. The top figure of Model Z does show some racial bias, but it's positive racial bias toward a minority group with the percentage difference being only 24% (65% - 41%) when compared to the bottom figure of Model Y which unfairly discriminates against African Americans having a mistakenly denied payment.  	Definitely ${e://Field/pref_model}	The bottom figure of Model Y shows that there is a negative bias toward Whites having a mistakenly denied payment (46%) over African Americans (23%). 	Definitely model Z	There is equal probability of being mistakenly denied payment whether White or African American at 32% for both.			High	A renter has nowhere to live. A renter's self-esteem is negatively affected. A renter has lost time (looking for an apartment among several resources, doing pros and cons list for top three choices, choosing the top apartment, time not spent with family and friends while doing research, etc.) and money (gas to visit each apartment of choice, any application fees, any necessary days off of work).			High	Even if the individual knows the payment is fraudulent, he/she loses hope that the transaction will go through, especially if he/she has no other options and the fraudulent is a last resort. The fraudulent payment will be reversed, causing damage to one's credit credibility with the credit card company and possible on his/her credit report. The individual may experience humiliation and alienation from family, friends, and the potential payment receiver for attempting a fraudulent transaction.			High	The landlord thinks the renter is trying to be fraudulent, loses a renter and, therefore, revenue, and maybe his reputation for mistakenly denying the transaction. Family and friends can feel stressed out, especially if the renter is a bread winner or emotional support person and is unavailable due to the stress of a mistakenly denied transaction. Other renters may doubt their payments based on the landlord's assessment of the potential/current renter.  A bank or credit card company is affected by thinking that the cardholder is fraudulent and unreliable.			High	The renter's family and friends have false hopes of renting the apartment or being able to spend time in the apartment. The landlord thinks the payment is non-fraudulent, but has to reverse the payment when he finds out it's fraudulent which can lead to having to file a fraud report, loss of time to file the report and finding a new renter for the apartment. Other renters may come to think that they can try the same thing and get away with it, causing further hassle for the landlord. 			No			No			Yes			Advantaged	Bachelor	Administrative Staff	615dd0b0a024ba4f4b4f7c24	615f94f07f89d7a8afda6025	6160cd4d356578b9beaa3a9b	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160cd4d356578b9beaa3a9b	APPROVED	2021-10-08 22:59:28.133000	2021-10-09 00:08:04.333000	4116.2	42.0	113	0	100	2021-10-13 00:30:38.198000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
64	2021-10-08 17:34:16	2021-10-08 18:14:29	IP Address	47.201.77.226	100	2413	True	2021-10-08 18:14:30	R_1n7mQNywbXzz67q					27.982192993164062	-81.67060089111328	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Mildly unfair	There is too large of a deficit for the program having issues with African Americans 	Neither biased nor unbiased.	Model X was designed as a security measure which unfortunately seems to struggle with characteristics of darker skinned individuals 	Neither useful nor unusable	Model X obviously has major inconsistencies which should be corrected before allowing them to grant or deny access to sensitive information of patients 	Mildly unfair	The system seems to struggle with white humans 	Neither biased nor unbiased	Same answer as previous question concerning bias 	Neither useful nor unusable	Same reason as previous question concerning usability 	No		Models X and Y are equally fair	Model x has large deficit for white and model y has large deficit for African Americans. 	Models X and Y are equally biased	Same reason as previous question	Models X and Y are equally useful	Neither of these models are useful given their inconsistencies and should not be used to manage sensitive patient info	Neither model X nor model Y	The results are no where near food enough to consider using either model in a real world setting 	Both ${e://Field/pref_model} and Z are equally fair	Both yield same results of deny/granted access just in different quantities 	Both ${e://Field/pref_model} and Z are equally biased	Same as previous answer  	Neither ${e://Field/pref_model} nor model Z	Not enough consistent data		High		If a doctor cannot access their patient files how can they save lives if the urgency is high 		High		HMI’s like this are often targets of hackers. A weak system will grant access to more people who will do harm with the information inside 		High		Concerns would raise of the possibility that you may be in need of medical care but your doctors cannot access your patient profile because they are mistakenly denied access. Also families and employers of the affected persons would be reliant on this as well 		High		Same answer as previous question only here the risk of identity theft and medical misconduct/ incorrect treatment		No			Yes			No			Advantaged				614e26b49b28a2293720c1d6	615f94f07f89d7a8afda6025	6160d56a974e2ab660c29d2e	majority	outcome-fpr	frauth	True		model X	top								Advantaged	6160d56a974e2ab660c29d2e	APPROVED	2021-10-08 23:34:12.788000	2021-10-09 00:14:32.964000	2420.176	34.0	63	1	100	2021-10-12 17:26:45.182000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Female	No	
65	2021-10-08 17:45:22	2021-10-08 18:15:27	IP Address	66.85.230.216	100	1805	True	2021-10-08 18:15:27	R_PNUCapNKkqirjgZ					35.027801513671875	-85.14610290527342	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	It's hard to know because I don't know the information on how many people applied. If 65% of the applicants were white and 41% were black, then the model isn't biased. I don't feel like I have enough information about the pool of applicants to know if this is fair or not.	Neither biased nor unbiased.	It's hard to know because I don't know the information on how many people applied. If 65% of the applicants were white and 41% were black, then the model isn't biased. I don't feel like I have enough information about the pool of applicants to know if this is fair or not.	Mostly useful	It had a 35% error, which while is still high is not the worst. I'm going to be honest here, graphs and math are not my strongest.	Neither fair nor unfair	I still feel like the total demographics of the population need to be known. How correct is this machine? 	Neither biased nor unbiased	It could be biased, but I don't know if I have enough information?	Mostly ununsable	45% of mistaken payments is getting pretty high. If this machine is biased towards white people, it seems like it would lose the company a lot of money. Unless of course the white population applying here is very small.	No		Models X and Y are equally fair	From here on, I'm going to just assume that the applicants are 50-50 white and black. These models have the same bias, just on different ends. They are both laid out as though biased in favor of white people. Model X is more likely to accept from white people and catches fraudulence equally. Model Y accepts equally but allows more fraudulence from white people.	Models X and Y are equally biased	These models have the same bias, just on different ends. They are both laid out as though biased in favor of white people. Model X is more likely to accept from white people and catches fraudulence equally. Model Y accepts equally but allows more fraudulence from white people.	Probably model X	From a company standpoint, Model X is slightly more useful because it catches fraud better. 	Neither model X nor model Y	They both have the same bias with the same mistakes. 	Probably model Z	"Okay, I told you before that math is not my strongest, so if my answers seem inconsistent then that is why. 

Model Z has a more equal mistakenly denied payment."	Probably ${e://Field/pref_model}	Model Y denies payment at a significantly higher rate to black people than to white people. Although Model Z seems less accepting, it does not mistakenly deny either race at a higher rate.	Probably model Z	It doesn't have a target in denying payment.			Moderate	It causes stress in the life of the renter and the landlord as they have to work out the payment.			Moderate	For the landlord, this is moderately stressful because they are left without the money.			Moderate	Considering the renter's family to be part of society, it places stress on them.			Moderate	Considering whoever the landlord owes their money to, that creates a chain event of stress.			No			Yes			Yes			Advantaged	Bachelor	Others	615c5d5d1768e12bba80f289	615f94f07f89d7a8afda6025	6160d80e44d93d2568b4ed5c	minority	outcome-fpr	rent	True		model Y	bottom								Advantaged	6160d80e44d93d2568b4ed5c	APPROVED	2021-10-08 23:45:20.370000	2021-10-09 00:15:31.183000	1810.813	22.0	143	0	100	2021-10-12 17:47:42.103000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	No	
66	2021-10-08 17:47:01	2021-10-08 18:19:42	IP Address	23.31.32.46	100	1960	True	2021-10-08 18:19:42	R_1NwdQqScfhpHp9I					26.599105834960938	-80.12840270996092	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Is margin of error is small	Neither biased nor unbiased.	It's prediction are not too bad 	Mostly useful	The model has a success rate that can be accepted. 	Mildly unfair	Because its success rate is greater than its failure rate. 	Acceptably biased	The failure rate it had for the white is quite big	Mostly useful	The statistics shows it has more success rate than failure rate	No		Probably model X	Failure rate is small	Models X and Y are equally biased	The models are equally biased from my perspective. 	Probably model X	I prefer model X to Y because of its low failure rate. 	Probably model X	Failure and success rate are good to go with. 	Probably model Z	Those the model mistakenly denied ICU are the same. 	Probably ${e://Field/pref_model}	Denial rate African American is too high	Probably model Z	I will choose model Z because of its fairness	Low			Because of the emotional stress it t can cause	High			It will ease tension and fear	High			It will results to emotional stress to family of the  individual concerned and also result to increase the tension among his colleagues and friends 	High			It will calm the nerves of friends family and relatives. 	No			Yes			No			Advantaged			Bachelor	Education	615dc21e4069250c0a5db511	615f94f07f89d7a8afda6025	6160d843405d2afa67d1e900	minority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	6160d843405d2afa67d1e900	APPROVED	2021-10-08 23:46:21.811000	2021-10-09 00:19:58.798000	2016.987	39.0	121	0	100	2021-10-13 00:25:31.290000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Male	DATA EXPIRED	
67	2021-10-08 17:35:50	2021-10-08 18:19:53	IP Address	67.135.89.244	100	2643	True	2021-10-08 18:19:54	R_3k0dpGYLwXmbi4t					30.413406372070312	-88.92510223388672	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	Same answers i typed for the last questions.	Very unbiased	Same answers i typed for the last questions.	Mostly useful	Same answers i typed for the last questions.	Neither fair nor unfair	I fail to see how fairness is a metric for mistakenly allowing fraudulent payments.	Very unbiased	Unless the machine was programmed to allow black people to pay fraudulently at a higher rate than it allows white people, i dont see any potential for bias. I think more information is needed. For example: perhaps there are more black people trying to pay fraudulently than there are white people..?? These graphs don't paint a complete and clear picture.	Mostly useful	Im not entirely sure. Perhaps it shows that more attention needs to be focused on the fraudulently paying african american demographic...	No		Models X and Y are equally fair	Fairness is not a metric to be used when referencing a self-taught/AI computer program, unless it was intentionally programmed that way!!!!!!	Models X and Y are equally biased	I see no bias.	Models X and Y are equally useful	I dont even know anymore.	Neither model X nor model Y	I dont even understand the question, honestly.	Both ${e://Field/pref_model} and Z are equally fair	Because its a program.	Both ${e://Field/pref_model} and Z are equally biased		Neither ${e://Field/pref_model} nor model Z				Moderate	Nobody should get evicted because a computer fucked up.			Moderate	Nobody wants to get scammed....??? wtf even are these questions?!			Moderate	Society doesnt wanna get evicted.			Moderate	Society doesnt wanna get scammed.			Yes			Yes			Yes			Disadvantaged			615dd185252ddcb73b937f19	615f94f07f89d7a8afda6025	6160d3ae97f9c1679bd54fbc	majority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	6160d3ae97f9c1679bd54fbc	APPROVED	2021-10-08 23:35:39.283000	2021-10-09 00:19:56.755000	2657.472	24.0	107	0	100	2021-10-14 16:59:03.119000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	Yes	
68	2021-10-08 17:45:40	2021-10-08 18:30:01	IP Address	24.252.140.5	100	2660	True	2021-10-08 18:30:01	R_2EFTpP0wyOg83Z6					37.3052978515625	-79.92520141601561	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It has an even percentage of error for both bit a lower acceptance rate for white 	Neither biased nor unbiased.	It approves more and offers a high mistaken acceptance to white people 	Mostly unusable	It's not balanced in accepting both white people and African Americans 	Mildly unfair	African Americans are more likely to be mistaken access by almost double to white people	Acceptably biased	Due to the facial recognition being used it doesn't tell African Americans apart 	Mostly ununsable	If it grants more than half the people access by mistake it's more of a problem. 	Yes	Would change the answer similar to the second where it doesn't show an even balance in granting access	Models X and Y are equally fair	The mistaken access grant graph for both has a high percentage that affects one group in each Model 	Models X and Y are equally biased	Even though Model X had an even Granted Access rate it only the Mistaken access for White people was only around half as opposed to African Americans. And the Model Y offered a higher percentage in access to African Americans it was still giving around half mistaken access.	Models X and Y are equally useful	Still has a high level mistaken access to both to be considered helpful	Neither model X nor model Y	It would be more trouble than it's worth. 	Both ${e://Field/pref_model} and Z are equally fair	One is evenly mistaken granted access the other is evenly denied access 	Both ${e://Field/pref_model} and Z are equally biased	One mistakenly denies access to white people more and the other mistakenly grants access to African Americans more. 	Neither ${e://Field/pref_model} nor model Z	The percentage of denial and mistaken granted is high. 		High		It could be a matter of life or death considering what their position might be		High		If even one unauthorized person is given access it could cause a lot of trouble		High		Something thqt should take an instant will take longer if they have to stop what they're doing just to enter a log in and password. And for society everyone that is in the hospital from doctors, nurses, administrative, patients. 		High		They can enter any part of the hospital that is off limits to non medical staff and can be dangerous given the situation. And for society everyone that is in the hospital from doctors, nurses, administrative, patients. 		No			Yes			Yes			Advantaged		Secondary Education	Services Occupations	615c9083ffc1ce3be0325609	615f943ec32164d0f282bd34	6160d81f0658b1a34d4fcfad	majority	outcome-fpr	frauth	False		model X	top								Advantaged	6160d81f0658b1a34d4fcfad	APPROVED	2021-10-08 23:45:38.101000	2021-10-09 00:30:04.740000	2666.639	33.0	198	1	100	2021-10-13 22:06:43.596000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	Spanish	United States	Male	No	
69	2021-10-08 18:15:31	2021-10-08 18:35:56	IP Address	96.232.105.179	100	1225	True	2021-10-08 18:35:57	R_3CGUhaywokLT0Px					40.583206176757805	-73.6594009399414	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 1	Neither fair nor unfair	It is hard to tell from this amount of data alone. We do not know the individual conditions/ailments of people in the hospital. The same percentage of people from both groups were mistakenly given ICU support.	Neither biased nor unbiased.	Similarly, it is hard to tell from this amount of data alone. We do not know the individual conditions/ailments of people in the hospital. The same percentage of people from both groups were mistakenly given ICU support.	Neither useful nor unusable	I think more data/details are needed - there are so many nuances with medical conditions. Someone may be feeling amazing at time of evaluation and hours later may fall extremely ill; the reverse can happen as well. I unfortunately am not sure if a model can accurately predict or decide on treatment options when often dire situations change rapidly	Neither fair nor unfair	Similar to the last model, I think more details are needed. Not sure if we can say something is unbiased/biased based on this data alone.	Neither biased nor unbiased	Similar to the above - I think more details are needed. Not sure if we can say something is unbiased/biased based on this data alone. 	Neither useful nor unusable	Again, I don't know if a model can accurately predict medical cases and current states of patients.	No		Models X and Y are equally fair		Models X and Y are equally biased		Models X and Y are equally useful		Neither model X nor model Y		Both ${e://Field/pref_model} and Z are equally fair	Again - I don't think we can make an assumption that a model is biased due to race alone with this little data/details. What is the time frame? How many patients in the hospital are white vs African American? 	Both ${e://Field/pref_model} and Z are equally biased		Neither ${e://Field/pref_model} nor model Z		Moderate			If ICU support is provided to someone in lieu of a more serious case, the person without the support could die. However, if the person given ICU support does not take another critically ill person's spot, this could result in a more positive outcome.	Moderate			The inverse of the above - if someone is not given support who needs it, he/she could die	Moderate			A member of society could lose a life	Moderate			Similarly - a member of society could lose their life	No			No			Yes			Disadvantaged			Bachelor		610866d4a8df0540300f1b6d	615f94f07f89d7a8afda6025	6160df1d6a3008c523424db3	minority	outcome-fpr	icu	True		model X	top								Disadvantaged	6160df1d6a3008c523424db3	APPROVED	2021-10-09 00:15:29.095000	2021-10-09 00:35:58.924000	1229.829	24.0	97	1	100	2021-10-13 15:03:35.232000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
70	2021-10-08 17:40:19	2021-10-08 18:40:01	IP Address	76.87.149.82	100	3581	True	2021-10-08 18:40:01	R_0ieiizCcrmXVadX					34.076202392578125	-118.30290222167967	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Percentages are either the same or not too far off. 	Neither biased nor unbiased.	It's hard to tell since one graph shows potential bias and the other does not.	Mostly useful	I think it's useful, but shouldn't be relied upon. Since there is 35% chance of mistake. 	Acceptably fair	The probability of prediction is the same so it should be a fair determination. Seeing this makes me want to change my answers from the last part. 	Mildly unbiased	Prediction is set on equal terms, but the mistake proportion draws concerns, 	Mostly ununsable	The difference in percentage for African Americans is too close, making it too risky. 	Yes	I would want to change my response to the biased question. Because seeing the second scenario made me understand the data a bit better. I was a bit confused at first and just took an educated guess, but then it became more clear on how to read the graphs by the second example. 	Probably model Y	I say it's fairer because its prediction is equal. That isn't to say however that it's more accurate. 	Probably model X	It's unclear how it decided on the prediction, so I think there may be some bias as to how it came to those percentages. 	Models X and Y are equally useful	I think these graphs are both relative to one another. They both cater to either one group over the other. 	Probably model Y	To be truthful, as a white person the chances of there being a mistake is slimmer.  	Probably ${e://Field/pref_model}	Again, I think having the prediction percentage the same makes things equal	Both ${e://Field/pref_model} and Z are equally biased	Both models seem to give similar outcomes. 	Neither ${e://Field/pref_model} nor model Z	Again, as a white person, the chances of mistake is greater and similar for both. 	Moderate			The experience itself will have an impact, but if ICU support ends up not being required then that means the individual is in good enough condition to get better. 	High			It impacts the individual's ability to get better, thus making the individual get severely worse. 	Moderate			I think the experience causes trauma for the family members and other people involved with the patient. Also, assuming the costs of premeditated ICU care cost money, this also affects the financials of family/taxpayers. It also takes away valuable time from doctors. 	High			Again, I'm judging by the experience of family/friends having to cope with the illness of their loved one. A lot of blame/regret from the medical staff is also put on them for the mistake. 	No			Yes			Yes			Advantaged			Bachelor		611862eb47c06fe554477801	615f94f07f89d7a8afda6025	6160d6da447387eec750d55d	majority	outcome-fpr	icu	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160d6da447387eec750d55d	APPROVED	2021-10-08 23:40:13.248000	2021-10-09 00:40:06.111000	3592.863	22.0	89	0	100	2021-10-13 14:18:38.816000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Female	No	
71	2021-10-08 18:00:35	2021-10-08 18:42:07	IP Address	207.140.15.235	100	2492	True	2021-10-08 18:42:08	R_1pA9Z86vSbTcixZ					38.72039794921875	-84.81230163574217	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Very unfair	It seems racist	Neither biased nor unbiased.	I'm not sure	Mostly unusable	It's racist and doesn't give a fair chance	Acceptably fair	It doesn't judge by race	Very unbiased	Equally judged races	Neither useful nor unusable		No		Definitely model X	The model took into account enough factors that it was accurate	Probably model Y	Although fair, its too fair to predict accurately 	Definitely model X	Better accuracy	Definitely model X	Accuracy	Probably model Z		Both ${e://Field/pref_model} and Z are equally biased		Definitely ${e://Field/pref_model}				High	It makes you look very bad to who you're making payment to. It leaves a bad impression that can lead to lower standing in your community.			Moderate	While it can be something that panics you, you can usually go back and fix things with your bank. It's a short term issue.			High	As mentioned before, this can cause people to have bad impressions of the renters. The management can essentially ruin their reputation, causing riffs in the community. This can lead to others from outside of the community hearing things, etc.			Moderate	Overall, it's not that big of a deal. Creditors get the money back, and the victim usually recovers within a few weeks to months. While it is still a crime that can have serious impacts on a person, society overall is not impacted.			Yes			Yes			Yes			Advantaged	Primary Education	Administrative Staff	615b87637d9ee8f557b73196	615f94f07f89d7a8afda6025	6160db90ae79faf4b32de978	minority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Advantaged	6160db90ae79faf4b32de978	APPROVED	2021-10-09 00:00:32.939000	2021-10-09 00:42:10.580000	2497.641	27.0	165	1	100	2021-10-12 17:45:57.112000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
72	2021-10-08 18:29:15	2021-10-08 18:47:28	IP Address	98.201.6.90	100	1092	True	2021-10-08 18:47:29	R_3NQkRmQkdDQjRGq					29.721206665039062	-95.36260223388672	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	race driven	Mildly unbiased		Neither useful nor unusable		Neither fair nor unfair		Neither biased nor unbiased		Neither useful nor unusable		No		Probably model X		Probably model Y		Models X and Y are equally useful		Probably model X		Probably ${e://Field/pref_model}		Probably model Z		Probably ${e://Field/pref_model}			High		It could leak important information		High		the wrong person could end up with the information		High		could leak personal info		High		people worried about personal info		Yes			Yes			Yes			Disadvantaged				60fd1c83f0ef2b7b323d3f8d	615f943ec32164d0f282bd34	6160e257db851e4d2d7438cd	minority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Disadvantaged	6160e257db851e4d2d7438cd	APPROVED	2021-10-09 00:29:13.635000	2021-10-09 00:47:31.700000	1098.065	21.0	75	0	100	2021-10-13 17:04:43.101000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	No	
73	2021-10-08 18:35:09	2021-10-08 18:50:08	IP Address	96.35.69.153	100	899	True	2021-10-08 18:50:09	R_3kpkak1Kc6W1rzX					43.035202026367195	-83.5248031616211	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Whites to blacks is just off and unfavorable	Acceptably biased	What I said above, placed the blacks below the whites unjustifiably. 	Neither useful nor unusable	Not sure	Very unfair	They show that African Americans are less likely to be mistreated by ICU.	Acceptably biased	Whites have the better chance by roughly 25%	Neither useful nor unusable	The take in general is just weird to me	No		Probably model Y	Because it seems more equal.	Definitely model X	Just look at the predictions with model X, it's definitely more biased 	Probably model Y	"It's the correct answers compared to the ""predictions"""	Probably model Y	Factual information I suppose	Both ${e://Field/pref_model} and Z are equally fair	They both seem more fair and equal	Both ${e://Field/pref_model} and Z are equally biased	I'm not really sure why they both seemed biased but it is more than the other.	Probably model Z	It's more just to African Americans	High			Because if the predictions are wrong more people will be upset by the outcomes of what's going on.	Moderate			I don't think people who aren't apart of a certain community who is in the wrong will care as much	Moderate			It's worse that if the predictions are wrong the African American Community will be more angry about whether or not they were mistreated from this chart	Low				No			Yes			Yes			Advantaged			Primary Education	Business	615df59eabff221770a2f292	615f943ec32164d0f282bd34	6160e3b7036a2fa12851c1b8	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160e3b7036a2fa12851c1b8	APPROVED	2021-10-09 00:35:05.400000	2021-10-09 00:50:11.103000	905.703	22.0	61	2	96	2021-10-14 01:24:11.521000	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Male	Yes	
74	2021-10-08 18:18:08	2021-10-08 18:51:16	IP Address	23.120.249.198	100	1988	True	2021-10-08 18:51:16	R_V2vgCoA3wUtBZm1					37.97599792480469	-122.33589935302734	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to be accepted than female applicants.	Figure 1	Mildly unfair	I think model X is mildly unfair because it is better at predicting for African-American patients.	Mildly unbiased	I think model X is biased because it is better at predicting for African-American patients. 	Mostly unusable	I think model X is mostly unusable because it is biased	Mildly unfair	I think model Y is mildly unfair because there is a clear divide between the race of the patients in the data. One race or group of patients are more likely to be mistakenly admitted into the ICU.	Acceptably biased	I think model Y is acceptably biased because there was a larger percentage of African-American patients that were mistakenly granted ICU support when compared to white patients. 	Mostly ununsable	I think model Y is mostly unusable because the probability of accurately predicting if ICU care is required is an abysmal 53% for both African-American and white patients. 	Yes	I feel like the first model that I saw wasn't as biased as I thought when I saw the second model.	Probably model Y	Model Y is fairer because it has an equal probability of predicting correct information for both races.	Definitely model X	Model X is more biased because it can predict more accurately for African-American patients.	Probably model Y	Model Y can more accurately predict if patients need ICU care and it is not biased on race	Probably model Y	Model Y gives better predictions for patients of both races. 	Definitely ${e://Field/pref_model}	Equal predictions for both races	Definitely model Z	Can predict more accurately for African-Americans	Probably ${e://Field/pref_model}	It is less biased when you look at the race of patients 	Moderate			If someone is mistakenly admitted into the ICU, there will be a patient who is suffering more and should be taking their spot.	High			If someone who needs to be in the ICU is not admitted, they will not receive the proper care they need. 	Moderate			I think the impacts are moderate on society because there are people who have accidents and they could need ICU support.	Moderate			The impact would be moderate because other people may need ICU support.	Yes			No			Yes			Disadvantaged			Secondary Education	Others	6100dd0240e57d5cb3908402	615f943ec32164d0f282bd34	6160dfbc4f94282b4bf28475	majority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160dfbc4f94282b4bf28475	APPROVED	2021-10-09 00:18:06.232000	2021-10-09 00:51:20.301000	1994.069	19.0	79	1	100	2021-10-13 17:38:56.305000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	Yes	
75	2021-10-08 18:38:12	2021-10-08 19:21:08	IP Address	172.58.235.47	100	2576	True	2021-10-08 19:21:08	R_tERUHTIHFVhIgk9					40.76409912109375	-74.16539764404298	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	probability of granting access to the medical device is highly favored by mostly by racial identity and African Americans are favored most 	Mildly unbiased	the device should be designed to favor individual not based on racial identity 	Mostly unusable	because most of the white might not be recognized  by the device	Acceptably fair	the device is design to grant aces based on facial recognition, without favoring one against the other and is showing what is expected from it	Mildly unbiased	the data reveals equally granted access of each based on the identity	Mostly useful	it has been proved useful at some point since is been able to equally granted access by using face recognition 	No		Probably model Y	the main purpose of the device is to grant access by using face recognition and in comparing both data model y has proved is usefulness	Probably model X	because is favoring more African Americans as compare to the other racial identity and is main purpose is to grant access by face recognition 	Probably model Y	the main aim and objective for the device is purposely for granting access by face recognition and from my view model Y has shown and equally access granted by face recognition	Probably model Y	because is has proven is usefulness by equally filtering and granting access to every individual equally and also bring out an average data	Definitely ${e://Field/pref_model}	from the data above model Y has being able to provide   the data equally based on the facial recognition	Definitely model Z	is favoring more African Americans as compare to white Americans	Definitely ${e://Field/pref_model}	because it has been able to equally distribute data based on the facial recognition 		High		is very significant because it can lead to unauthorized personnel to the hospital data and also attending to patients that could lead to serious problems in future 		High		it will serious caused problems because it can lead to access to some sensitive data in the hospital and it might also lead to crime to some patients and the hospital staffs as a whole 		High		families might be affected because it can cause some serious crimes at the hospital		High		it will affect the society and the community a great deal because it can cause some serious crimes at the hsopital		Yes			Yes			Yes			Advantaged		Secondary Education	Business	6127344372ed5d01d83545f6	615f943ec32164d0f282bd34	6160e46b6d24db6cc21010ef	majority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160e46b6d24db6cc21010ef	APPROVED	2021-10-09 00:38:08.043000	2021-10-09 01:21:19.152000	2591.109		89	2	98	2021-10-14 16:37:32.113000	19AE28A9	DATA EXPIRED	DATA EXPIRED	DATA EXPIRED	Min	DATA EXPIRED	DATA EXPIRED	DATA EXPIRED	DATA EXPIRED	
76	2021-10-08 18:45:54	2021-10-08 19:21:58	IP Address	50.194.231.254	100	2164	True	2021-10-08 19:21:59	R_XU1qOF7ZEcr15UB					33.766693115234375	-84.3363037109375	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Neither fair nor unfair	Is just a normal model	Acceptably biased	Is shows equal right between the white, and Africa American	Mostly useful	It will be use to for day to day study	Very unfair	They are on the same level!	Very unbiased	I didn't see any biased information	Mostly useful	It shows a very good information	No		Definitely model X	Is just fair to me	Probably model Y	I just think so	Models X and Y are equally useful	They will be useful for the plan	Definitely model X	Is okay for me	Probably ${e://Field/pref_model}	The percentage of denied access to one other is not much	Probably model Z	The percentage to one another is a little bit much	Definitely ${e://Field/pref_model}	Their percentage to each other is near		High		Is bad not to recognize am hospital staff		High		It's very dangerous		Moderate		It helps to reduce the risk involve in an hospital with the society		High		It's very dangerous to the society		Yes			Yes			No			Advantaged		Secondary Education	Business	614e4d43d19a69d06535518c	615f943ec32164d0f282bd34	6160e632683dcc9492a78aa3	majority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	6160e632683dcc9492a78aa3	APPROVED	2021-10-09 00:45:50.267000	2021-10-09 01:22:13.433000	2183.166	34.0	102	1	100	2021-10-13 19:21:50.126000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Male	No	
77	2021-10-08 19:00:22	2021-10-08 19:33:11	IP Address	72.241.83.44	100	1968	True	2021-10-08 19:33:12	R_2ygEzbV2B4joEOC					41.650299072265625	-83.50650024414062	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Mildly unfair	It has a better success rate among African Americans	Neither biased nor unbiased.	It could have been programmed with out bias, but the software could have issues not related to human input.	Mostly unusable	because it will fail to grant access to some of the doctors	Acceptably fair	the probability of granting access is equal	Very biased	because it does mistakenly grant access to more African American doctors than white doctors	Mostly ununsable	high amount of mistaken access granted	No		Probably model X	mistaken access granted is equal	Probably model Y	it denies more white doctors than African American doctors	Models X and Y are equally useful	I think they are both equally useful, as neither seem to be accurate	Neither model X nor model Y	I don't feel either are accurate enough	Both ${e://Field/pref_model} and Z are equally fair	I dont think either model is considered fair	Probably ${e://Field/pref_model}	It mistakenly denied access to twice as many white doctors than African American doctors	Neither ${e://Field/pref_model} nor model Z	neither are accurate		Moderate		It would take more of their efforts to have to sign in		Low		It could breach security, but would not be the fault of the individual		High		Hospital administration could possibly face law suits for biased treatments		High		Hospital administration could be responsible for security breaches		Yes			No			Yes			Advantaged		Primary Education	Services Occupations	5c928a3abf2af20001363f48	615f94f07f89d7a8afda6025	6160e99f27a7406a168d3c62	majority	outcome-fpr	frauth	True		model X	top								Advantaged	6160e99f27a7406a168d3c62	APPROVED	2021-10-09 01:00:20.910000	2021-10-09 01:33:14.181000	1973.271	50.0	143	0	100	2021-10-13 15:06:46.316000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Female	No	
78	2021-10-08 18:49:15	2021-10-08 19:34:24	IP Address	108.6.213.182	100	2708	True	2021-10-08 19:34:24	R_2QVOZJf7ViGn0FL					40.947601318359375	-73.86239624023438	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	It accept African Americans more than White	Acceptably biased	He choose African American more compare to white	Mostly unusable	It's biased	Very fair	There's nothing like biased	Very unbiased	It choose fairly	Very useful	It work perfectly	No		Probably model Y	It doesn't choose biased when it comes to probability of granting access	Probably model X	It choose more African American than White when it comes to probability of access	Probably model Y	It choose fairly	Definitely model Y	It choose fairly enough	Probably ${e://Field/pref_model}	It choose fairly when it comes to probability of granting access	Probably model Z	It doesn't choose fairly	Definitely model Z	It worked fairly		High		The significance of the harmful of a decision varies with content, it can also endangered society		High		It can cause problem in the society		High		It can endangered the life of the patient of that hospital staff		High		It can cause alot of problem for the medical personnel		No			No			Yes			Disadvantaged		Doctoral	Engineering and Technology	6154ce71b116038a0a62b4f8	615f943ec32164d0f282bd34	6160e6fe76b9b1d990a52b09	majority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160e6fe76b9b1d990a52b09	APPROVED	2021-10-09 00:49:09.709000	2021-10-09 01:34:37.410000	2727.701	36.0	103	0	100	2021-10-14 02:43:39.300000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Male	No	
79	2021-10-08 18:54:03	2021-10-08 19:35:07	IP Address	24.242.65.19	100	2463	True	2021-10-08 19:35:07	R_6nIeRRsbsxJA9y1					30.37730407714844	-97.70999908447264	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	On the surface level, it does not look to be racially biased. 	Neither biased nor unbiased.	Typically, programs are skewed against POC however it seems this program does not display that bias. 	Neither useful nor unusable	Technology can always be improved and made to have less of a bias though that is also dependent on the biases of the coder who creates the program. 	Mildly unfair	Program Y is more biased against POC. 	Very biased	There is a disproportionate amount of approvals for white people and 'erroneous' approvals of POC	Mostly ununsable	Having a program that is biased against POC where there is already inherent racism in the ability of POC people to gain housing and this program makes a bad situation worse. 	No		Probably model X	Model X provides more benefits to those POCs who need the hand-up to obtain housing in the world of systemic racism. 	Definitely model X	Model X is more skud to deny POCs	Models X and Y are equally useful	By combining the programming and having a nonbiased part remove any unintentional racist code, both programs can make for a more unbiased program.	Probably model X	Model X seems more fair	Probably ${e://Field/pref_model}	It looks to be the fairest. 	Probably model Z	Mistakenly allowed payments in Model Z seem to be more skewed. 	Probably ${e://Field/pref_model}	Seems to be the fairest and unbiased program. 			High	If you have a limited budget or the landlord is offering something beyond the 3x a single month rent to be cleared in one month, it could be someone's only opportunity to obtain fair and affordable housing based on their circumstances. 			High	While it can cause minor inconvenience to the landlord to get it sorted out, it causes more of an issue to people who again only have limited budgets, or can only obtain fair and affordable housing under certain conditions. 			High	Denying a transaction and having it marked as a fraud on someone's credit report could have detrimental effects on their future even if it was erroneously made. Even if something is disputed and removed, it still leaves an impression on everything that person tried to do in the future. Society would include future rental groups, landlords, credit check agencies, or any governing body that can affect a person's ability to obtain something like housing, or even trickling down to credit for credit cards, vehicles, and bank loans. 			Moderate	Allowing a fraudulent payment impacts the landlord in that they do not have a guaranteed payment towards or that will cover the mortgage, however, the biggest issue is towards the applicants that were turned away who may have had limited opportunities to obtain housing. 			Yes			Yes			Yes			Advantaged	Bachelor	Administrative Staff	614f24a17833e1e4b7184fae	615f943ec32164d0f282bd34	6160e822a0def5900e677f41	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Advantaged	6160e822a0def5900e677f41	APPROVED	2021-10-09 00:53:58.638000	2021-10-09 01:35:10.567000	2471.929	26.0	120	0	100	2021-10-13 21:14:38.636000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
80	2021-10-08 18:57:46	2021-10-08 19:40:24	IP Address	71.242.92.224	100	2558	True	2021-10-08 19:40:25	R_3OiQEmG476y1TQH					40.60270690917969	-75.47779846191406	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	They are giving more opportunity to a certain race.	Very biased	It is more likely to accept payment from a certain race.	Completely unusable	Race shouldn't matter.	Acceptably fair	They are giving equal opportunity to both groups.	Mildly unbiased	It's giving an equal chance and not favoriting one over the other. 	Neither useful nor unusable	It allows for equal opportunity but it is still not foolproof as it is mistakenly accepting false payments.	No		Definitely model Y	It gives equal opportunity.	Definitely model X	It favors one race over the other by more likely allowing their payments.	Probably model Y	Gives the required data	Definitely model Y	It gives the necessary data while still being fair.	Probably model Z	Model Z mistakenly denied both groups an equal amount.	Probably ${e://Field/pref_model}	It mistakenly denied a higher amount of payments from one group over the other. 	Probably model Z	It was more fair			Low	The individual doesn't gain or lose anything.			High	They aren't getting their payment			Moderate	The society could be possible future renters in the area. If they see renters aren't being allowed to make the payment due to the owners mistake, they wouldn't want to go there. 			Moderate	The society could be possible future renters in the area. If they knew they were mistakenly allowing fraudulent payment, they probably wouldn't trust them 			No			No			Yes			Advantaged	Primary Education		614b5d9ef23406860360c959	615f943ec32164d0f282bd34	6160e90229cf0534d11cb7d9	minority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160e90229cf0534d11cb7d9	APPROVED	2021-10-09 00:57:44.044000	2021-10-09 01:40:30.438000	2566.394	20.0	118	3	96	2021-10-13 19:12:04.612000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
81	2021-10-08 19:04:31	2021-10-08 19:42:53	IP Address	172.58.3.123	100	2302	True	2021-10-08 19:42:54	R_1i28ihEkWzi22Eh					33.78379821777344	-84.44550323486328	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Acceptably fair	Blacks mostly eat unhealthy	Mildly unbiased	Blacks love foods with sodium.	Mostly useful	It seems accurate	Mildly unfair	It assumed blacks will need ICU	Neither biased nor unbiased	It seems equal 	Mostly useful	Nothing's perfect 	No		Probably model Y	Looks much closer in numbers 	Definitely model X	More distance in number predictions	Models X and Y are equally useful	They both are entitled to opinions 	Probably model Y	Closer in number conclusions	Definitely ${e://Field/pref_model}	Closer in numbers 	Probably model Z	More distance in numbers 	Definitely ${e://Field/pref_model}	Closer in numbers 	High			Because it could be a minor mistake that could cost someone their lives	High			They are fully responsible for mistakes 	Moderate			It's easy to believe a number of people 	Moderate			Also hard to believe everyone 	Yes			No			No			Advantaged			Secondary Education	Others	5fbdc52c79deff224ecaa4a3	615f943ec32164d0f282bd34	6160ea96cf137b8ff9edc262	majority	outcome-fpr	icu	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	6160ea96cf137b8ff9edc262	APPROVED	2021-10-09 01:04:28.914000	2021-10-09 01:42:58.103000	2309.189	33.0	251	1	100	2021-10-13 17:01:55.730000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Male	DATA EXPIRED	
82	2021-10-08 19:24:06	2021-10-08 19:47:00	IP Address	99.106.96.134	100	1374	True	2021-10-08 19:47:01	R_33rsnP8F0SIYzMJ					34.04859924316406	-118.17959594726562	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 2	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Mildly unfair	it will mistake african americans an equal amount of times to white americans even though there’s a higher chance of african americans being granted access in figure 1.	Acceptably biased	read answer above 	Neither useful nor unusable	needs more fine tuning but could work better down the line.	Mildly unfair	the model doesn’t seem to know how to distinguish the faces of african americans nearly as well as it does for white americans	Very biased	read answer above	Mostly ununsable	because it’s mistakenly granting access to a large amount of people 	No		Probably model X	just guessing 	Models X and Y are equally biased	read answer above	Models X and Y are equally useful	they’re both flawed 	Neither model X nor model Y	they’re both flawed 	Both ${e://Field/pref_model} and Z are equally fair	they have similar percentages 	Both ${e://Field/pref_model} and Z are equally biased	read answer above	Neither ${e://Field/pref_model} nor model Z	they both are flawed 		Moderate		i think it’s mostly just inconvenient, not the end of the world		High		people aren’t authorized access to certain parts of the hospital for a reason. it could be dangerous.		Moderate		it’s not a huge deal for society, society being anyone not apart of hospital staff		High		if someone unauthorized is allowed access and they have bad intentions it can lead to serious problems 		No			Yes			Yes			Disadvantaged		Secondary Education	Others	6132c13194808d59aae1cc62	615f943ec32164d0f282bd34	6160ef29f1b0d0137381846b	majority	outcome-fpr	frauth	True		model X	top								Disadvantaged	6160ef29f1b0d0137381846b	APPROVED	2021-10-09 01:24:04.383000	2021-10-09 01:47:04.768000	1380.385	20.0	121	1	100	2021-10-14 02:41:32.727000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
83	2021-10-08 19:25:31	2021-10-08 19:54:28	IP Address	73.220.26.171	100	1737	True	2021-10-08 19:54:29	R_bQQoSFljstW9qZb					38.025299072265625	-121.3009033203125	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	because they only take into account two racial ethnicities	Mildly unbiased	because they only take into account two racial ethnicities	Mostly unusable	for its margin of error	Mildly unfair	because not only whites or African Americans are capable of scams	Mildly unbiased	they only take ethnicities into account, and two nothing else.	Mostly ununsable	for its margin of error	No		Probably model Y	by the percentage of transactions allowed	Models X and Y are equally biased	for only taking into account two racial ethnicities	Probably model Y	by the percentage of transactions allowed	Neither model X nor model Y	by margin of error	Probably ${e://Field/pref_model}	by appropriate transactions	Both ${e://Field/pref_model} and Z are equally biased	for only taking into account two racial ethnicities	Neither ${e://Field/pref_model} nor model Z	by margin of error			High	because I would lose the opportunity to rent a house			High	it would greatly affect the person who should receive the money			Low	it wouldn't hurt much not to rent to someone, from a community point of view			High	depending on the magnitude of the fraud it can affect the community			No			No			Yes			Advantaged	Master	Healthcare Practitioner	5c9914f616743e0016280d64	615f943ec32164d0f282bd34	6160ef7f307c981d9396fe73	minority	outcome-fpr	rent	False		model Y	bottom								Advantaged	6160ef7f307c981d9396fe73	APPROVED	2021-10-09 01:25:26.292000	2021-10-09 01:55:02.048000	1775.756	25.0	612	8	97	2021-10-12 01:06:45.031000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Female	DATA EXPIRED	
84	2021-10-08 18:56:14	2021-10-08 19:58:59	IP Address	71.167.244.110	100	3764	True	2021-10-08 19:58:59	R_31Fer6WpSLb2qoq					40.83120727539063	-73.90670013427734	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 1	Very fair	Because the probabilities for both groups are the same	Very unbiased	Because the probabilities for both groups are the same	Very useful	Because it’s unbiased 	Mildly unfair	Because it passed lots of failed payments from one group way more than the other 	Mildly unbiased	Because it passed lots of failed payments from one group way more than the other 	Completely unusable	Because it’s biased	No		Probably model X	Because it’s unbiased	Definitely model Y	Because Model Y has a big gap between the two groups	Definitely model X	Because it rates both groups the same 	Definitely model X	Because it’s not biased 	Probably model Z	Because the two groups are closer together 	Both ${e://Field/pref_model} and Z are equally biased	With the new graph looks like both of them are biased 	Probably ${e://Field/pref_model}	The difference in guessing in the past graph is better than model Y			Moderate	Because that person could stay homeless for a mistake			Moderate	Because the renter will lose money 			Low	Because you couldn’t get the service 			Low	Because it affects the society and the business too, people don’t like to go places where transactions are difficult 			No			Yes			Yes			Disadvantaged	Bachelor	Others	5daffa390adfeb0016bc64dd	615f943ec32164d0f282bd34	6160e8aaae79faf4b32de99e	majority	outcome-fpr	rent	False	You have chosen model X over model Y.	model X	top								Disadvantaged	6160e8aaae79faf4b32de99e	APPROVED	2021-10-09 00:56:12.347000	2021-10-09 01:59:01.583000	3769.236	28.0	504	0	100	2021-10-13 16:57:13.885000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Male	No	
85	2021-10-08 19:18:32	2021-10-08 19:59:10	IP Address	76.30.217.33	100	2437	True	2021-10-08 19:59:10	R_3s6LxoBbU0zLcZl					29.69000244140625	-95.25640106201172	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to be accepted than female applicants.	Figure 2	Acceptably fair	Seeing that most of the African Americans that go in are most likely need to be put in the ICU results in less mistaken ICU placements for both White and African Americans. This reduces the amount of mistakenly placed patients. 	Very unbiased	If they are making decisions based on the quality of care and determine what is best for the patient I do not believe they are being biased in their decision making. 	Very useful	You can see how well the model is working and seeing the positive results of this model.	Acceptably fair	Both White and African Americans are given the same percentage of prediction about whether they need to be placed in ICU. Deciding to give one race more of a percentage would be unfair. Other factors do play into deciding such as location. However, giving both races an equal percentage about predictions I see as fair because it is giving them both an equal chance to determine whether or not they need to be placed In ICU. 	Mildly unbiased	I chose mildly unbiased because they are giving both an equal percentage to predict the intensity of their care. Whether they be right or wrong about needing the ICU, everyone still deserves to be treated. 	Mostly useful	Even with people being placed in ICU mistakenly it is showing the results of the entirety of model Y. Instead of giving race an equal opportunity, why not instead focus on the severity of the client and determine if they need to be placed in the ICU. People can see the outcome of giving each race 53% ICU requirement. 	Yes	I would change the fact that I mentioned more equality for each race would be best even though there were more mistakenly placed ICU placements. 	Models X and Y are equally fair	There are roughly the same amount of mistakenly placed ICU units. I believe both models are sort of similar despite the drastic changes in graphs between the two. 	Probably model X	It is more based on race compared to model Y that gives both races equal chances. 	Models X and Y are equally useful	Both have similar outcomes in mistakenly placed patients in ICU. 	Probably model Y	Giving people equal chances for ICU and have a slightly less mistakenly placed ICU patients. 	Probably ${e://Field/pref_model}	Giving people of each race and equal percentage despite the outcomes is more fair. 	Probably model Z	They are deciding to give one race more chance of ICU than another. 	Probably model Z	They have less mistakenly ICU support. 	High			People's lives are seen as a high priority so seeing that there are lots of mistaken placements can be a serious issue for some. 	High			I would think it would be high since people want the best care and for them to not be getting the right amount of support is a serious matter. 	Moderate			As a society I think of the general public and maybe medical officials and think there are probably other issues that are of higher attention. 	Moderate			I feel like patients are more concerned about the mistaken ICU support compared to medical staff. 	No			Yes			Yes			Disadvantaged			Bachelor	Others	61133e631ae7f0b17b809331	615f943ec32164d0f282bd34	6160ede2df10013dc973bf5d	majority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160ede2df10013dc973bf5d	APPROVED	2021-10-09 01:18:28.880000	2021-10-09 01:59:13.117000	2444.237	22.0	16	0	100	2021-10-14 02:51:52.085000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
86	2021-10-08 19:17:09	2021-10-08 20:03:50	IP Address	73.48.211.229	100	2800	True	2021-10-08 20:03:50	R_VUXZv8QHuJkYYMx					36.74049377441406	-119.75080108642578	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Now half of time it will not let white people make a valid statement	Very biased	Again you could see how one group get treated differently	Mostly unusable	It fails a third of the time and it white people will be rejected about half of the time	Mildly unfair	"While it does  accept both African and Whites as valid payments, It nearly lets half of African fraudsters make an invalid payment.
Meaning the system is not effective"	Mildly unbiased	Again yes because while trying to avoid discrimination it allows fraudsters in	Mostly ununsable	"Because all one would have to do to commit fraud would be to pretend to African American and it would work half of the time or a quarter of the time for white people.
Too ineffective "	No		Models X and Y are equally fair	They both are horrible models one model slightly is bad for white people and the other model will end up losing to the fraudsters 	Models X and Y are equally biased	A perfect model would not have such a glaring difference	Models X and Y are equally useful	I would not use them because they would get me in 	Probably model Y	because it would be perceived as fair 	Both ${e://Field/pref_model} and Z are equally fair	They are both bad	Probably ${e://Field/pref_model}	It gives the illusion of being fair	Probably ${e://Field/pref_model}	It would be harder to prove and would have less people complaining but the system still does not perform well			High	being accused of fraud is a bad feeling			Low	They would like that and they would do best to keep everyone from knowing that they got away with fraud			High	They need to conduct their business and having the money tied up is a serious problem			Low	They do not lose. They are the ones affected so it is negligible to them			Yes			Yes			Yes			Disadvantaged	Secondary Education	Others	614fa2d8176b0317b451cd0f	615f943ec32164d0f282bd34	6160ed927b478c0f281153bd	majority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160ed927b478c0f281153bd	APPROVED	2021-10-09 01:17:07.909000	2021-10-09 02:03:54.341000	2806.432	29.0	136	2	99	2021-10-13 21:17:51.552000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Male	No	
87	2021-10-08 19:17:56	2021-10-08 20:06:43	IP Address	45.36.208.18	100	2927	True	2021-10-08 20:06:43	R_1dEmX82OgiB39ce					36.12190246582031	-79.77770233154298	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	The ratio of prediction to mistake is terrible	Very biased	White people are predicted to need ICU more than black people	Completely unusable	There's too much bias in the graphs	Very fair	The predictions are even between races	Very unbiased	The predictions aren't lopsided for one race over the other	Very useful	The graph seems fair enough to use for information	No		Definitely model Y	The probability between what race needs the ICU is actually balanced.	Definitely model X	White people are more predicted to need the ICU and the mistakes are high for black people	Definitely model Y	Model Y is actually more even of a prediction between races	Definitely model Y	I trust Model Y to be more accurate and unbiased	Both ${e://Field/pref_model} and Z are equally fair	Y has a lopsided mistake graph and Z has a lopsided prediction graph	Both ${e://Field/pref_model} and Z are equally biased	Y is more biased with mistakes and Z is more biased towards predictions	Neither ${e://Field/pref_model} nor model Z	They are both pretty flawed so I don't think I'd choose either	High			Someone could be scheduled for a surgery they don't need 	High			Someone who actually needs the ICU could be overlooked.	High			I thought of people and their families who mistakenly get surgeries that they didn't need	High			I thought about people who need surgeries they won't get due to mistakes and their families who worry about those people	No			Yes			Yes			Disadvantaged			Secondary Education	Others	5e10d5554c81b27a7b112ec7	615f943ec32164d0f282bd34	6160edb456e479061754de99	minority	outcome-fpr	icu	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160edb456e479061754de99	APPROVED	2021-10-09 01:17:49.570000	2021-10-09 02:06:51.885000	2942.315	24.0	222	1	100	2021-10-13 16:58:30.003000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Male	DATA EXPIRED	
88	2021-10-08 19:25:42	2021-10-08 20:15:52	IP Address	174.134.234.156	100	3009	True	2021-10-08 20:15:52	R_x4J80amG1gSP7SV					35.297607421875	-119.04840087890624	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to be accepted than female applicants.	Figure 2	Neither fair nor unfair	It is not enough data or information for me to gather whether there is bias or not within the machine learning. The patterns machine learning use are from past data, which are done by humans who may have a bias. Therefore, I cannot tell if it is fair nor unfair with the little information given.	Neither biased nor unbiased.	Even though there was an even amount of mistaken ICU patients, I still can't determine its bias based on just numbers or rather the little information I have. It is still interesting results.	Mostly unusable	There is just too many factors to consider and there is medical racism that exists in the medical field. So those patterns that the machine learning system uses is taking human filed data, which there is a high possibility that the patterns can have bias.	Very unfair	For the same reason above, medical racism is very prevalent and this data could be used to justify not giving African Americans the proper attention.	Acceptably biased	For the same reason above, medical racism is very prevalent and this data could be used to justify not giving African Americans the proper attention.	Completely unusable	It would not be useful for the reason mentioned above.	Yes	I didn't consider the medical racism as much for the first one as being a huge factor for bias and fairness.	Probably model X	The amount mistaken is the same so there will be less justification to admit African Americans in the ICU as compared to model Y.	Models X and Y are equally biased	One prediction and results aren't better than the other in terms of how it will be used to justify who will possibly be treated for ICU. 	Probably model X	This is really difficult as both data can be used in the wrong way. Model X gives room for less discrimination...possibly.	Neither model X nor model Y	Both would allow room for discrimination.	Both ${e://Field/pref_model} and Z are equally fair	"They both leave room for discrimination and would probably lead to the same conclusions, so I'm going to say ""fair"" in terms of whatever pattern the ML is using."	Both ${e://Field/pref_model} and Z are equally biased	Again, using human data with discrimination possibly involved could lead to a lot of consequences.	Neither ${e://Field/pref_model} nor model Z	Reasons already stated above.	Moderate			If you are mistakenly put into ICU and the ICU was full, that means someone who did need it wouldn't get the proper treatment which very much could be fatal.	Moderate			It could be a life-threatening situation, but rather have individuals decide (because of their expertise) rather than a Machine Learning algorithm/device.	High			The impacts are great as some parts of society will now avoid getting hospital treatment for the sake of economical and discriminatory reasons.	High			The reason is similar above as this can lead to possible economic consequences due to not getting properly treated and can lead to their life being in danger.	No			No			Yes			Advantaged			Secondary Education	Others	610c83598db46d4d46de2c61	615f943ec32164d0f282bd34	6160ef8c31f8470d858cb606	majority	outcome-fpr	icu	True		model Y	bottom								Advantaged	6160ef8c31f8470d858cb606	APPROVED	2021-10-09 01:25:40.768000	2021-10-09 02:15:56.009000	3015.241	22.0	139	1	100	2021-10-14 16:44:18.559000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
89	2021-10-08 19:21:17	2021-10-08 20:23:31	IP Address	68.91.158.156	100	3733	True	2021-10-08 20:23:31	R_24wtOVAbt4wn9ZZ					37.92689514160156	-122.331298828125	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	African Americans are granted more access. But honestly I'm not too pressed about it. 	Mildly unbiased	I believe African Americans get more use of the medical device	Mostly useful	I think African Americans deserve some privileges. 	Mildly unfair	There's a larger granted access but it is unfair because it is a mistake. 	Acceptably biased	They are both given the same chance in the first graph but in the second graph, a lot of mistakes are made. This isn't fair. 	Mostly ununsable	It's just unfair if it's making so many mistakes. There is no point in using it. 	Yes	I don't think my logic for what is considered fair is good. I do typically side with underserved populations in everything. That's just because I'm a minority myself. 	Definitely model X	There are fewer mistakes happening.	Probably model Y	Because there are more mistakes occurring.	Probably model X	I just feel like the less mistakes, the better it is.	Definitely model X	Again, same thing with the mistakes. There are fewer mistakes.	Probably model Z	African Americans are given access more frequently.	Probably model Z	It is biased towards black people but I don't mind it.	Probably model Z	I feel like it gives minorities an upper hand.		High		It is incredibly important for hospital staff to be able to do their job efficiently. This just creates unnecessary stress for them and can cause them to not do their job efficiently. 		Moderate		Personal information can be leaked. This can make patients feel unsafe. It is very intrusive. 		Moderate		I feel like it can be seen as racist if it doesn't recognize staff. I see society as mostly minorities. 		Moderate		I also see society as minorities. I believe they would feel unsafe and violated if this information got out. It is personal after all. 		No			Yes			Yes			Disadvantaged		Bachelor	Healthcare Practitioner	611880d3d174718bfff2fd10	615f943ec32164d0f282bd34	6160ee7703516b75c9689681	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Disadvantaged	6160ee7703516b75c9689681	APPROVED	2021-10-09 01:21:15.533000	2021-10-09 02:23:35.069000	3739.536	22.0	139	0	100	2021-10-14 15:29:45.948000	19AE28A9	United States	United States	Due to start a new job within the next month	Non-Caucasian	Spanish	United States	Female	Yes	
90	2021-10-08 19:42:23	2021-10-08 20:25:59	IP Address	162.207.200.101	100	2615	True	2021-10-08 20:25:59	R_QnA4DRqRDrdM19f					37.36979675292969	-121.81600189208984	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Neither fair nor unfair	I am uncertain where I stand on this, because it is now showing that they allowed more african americans to be accepted than whites, then shows that they were wrong about more african americans than whites. This could possibly paint them in a bad light, and surely there has to be a better way of detecting fraud than using race.	Acceptably biased	I believe there should be a better way to detect fraud than race.	Neither useful nor unusable	Should Model X use a different method other than race, this could be something actually useful.	Acceptably fair	I believe that it is fair, because it does not hold somebody's race or ethnicity against them while deciding whether to deny their payment or not. This is obviously the first thing that comes to mind, because we see African American and White as the two subjects listed.	Very unbiased	As I said before, I believe that it is not holding anybody's race against them when deciding whether or not to accept their payment.	Mostly useful	I do believe that this could be very useful, but not until it has a 100% detection rate at being able to tell which payments may be fraudulent or not.	Yes	I would like to add that using other methods out there that other software or companies use to detect fraud would probably help and make these outcomes more accurate. They could also add more verifying information to make a payment, so that somebody can't just make up some information and get away with it.	Definitely model Y	I believe this is more fair, because they gave out the same amount of allowing payments to both races.	Definitely model X	I think this is more unfair, because instead of expecting a negative outcome for african american customers, they could have given the same amount then sorted the differences there.	Definitely model Y	Because they gave people the same amount and were able to see an outcome that way, instead of expecting poor outcomes for one race.	Definitely model Y	Because this model seems more fair and beneficial.	Definitely ${e://Field/pref_model}	Because they allow the same amount of payments for both races	Probably model Z	Because they allow more payments to one race rather than keeping it equal	Definitely ${e://Field/pref_model}	Because it is more fair			Moderate	i think it's moderate, because i am sure there are other places than somebody may stay should one place not work out, such as a motel, as long as this is not a permanent situation we are talking about			Moderate	i think that one payment should not affect somebody too badly, because it is only one payment over many other non-fraudulent payments.			Moderate	i say moderate, because i still think that should somebody be denied transaction, they could just go somewhere else. the worst case scenario i could think of would be by this person bad mouthing this company, saying not to go there by word of mouth			High	i would say this is higher, because if one person gets away with it, more people are going to think they could go around and make fraudulent payments and that is bad for society altogether, for everybody involved.			No			Yes			Yes			Disadvantaged	Primary Education	Others	6103d636a634895ac3451c48	615f943ec32164d0f282bd34	6160f36c70ba3b9085d52001	majority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160f36c70ba3b9085d52001	APPROVED	2021-10-09 01:42:20.954000	2021-10-09 02:26:02.705000	2621.751	27.0	107	1	99	2021-10-14 02:21:19.374000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
91	2021-10-08 19:48:58	2021-10-08 20:40:57	IP Address	24.129.86.214	100	3119	True	2021-10-08 20:40:58	R_1GKCi8cFspv4ZH0					30.28509521484375	-81.82170104980467	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Very fair	The bias seems less obvious now.	Mildly unbiased	The fraud levels are equal now. No one is above fraud.	Mostly useful	Because it shows more grace to people of color.	Very unfair	I do not like how it assumes that African Americans are more likely to commit fraud than white people.	Very biased	It does not take a balanced look at white people.	Mostly ununsable	It has too much room for bias.	Yes	I do not wish to change any aspect of my answer.	Probably model X	Because it's not so biased against a minority.	Definitely model Y	It shows favoritism towards white people.	Probably model X	It's willing to be fair to both racial groups.	Definitely model X	As a black person myself, I won't have to fear being judged harshly.	Definitely ${e://Field/pref_model}	Once again, it's selfish but it means, as a black person, I have less of a chance of getting cheated.	Probably ${e://Field/pref_model}	Because it's biased against white people.	Probably ${e://Field/pref_model}	Because I won't be denied payment.			Moderate	I said moderate, but I honestly think it can go either way. It depends on if the individual really needed that payment.			High	It means losing privileges.			Low	"One person is not going to ruin a society unless the person has some significance. I consider the ""society"" people in the corporate world."			High	"If someone was able to fool them, it'll make the society look stupid in front of their peers. I consider the ""society"" people in the corporate world."			No			Yes			Yes			Disadvantaged	Secondary Education		61452f580953ba50efed1036	615f943ec32164d0f282bd34	6160f308443383cc62d7b0eb	majority	outcome-fpr	rent	False	You have chosen model X over model Y.	model X	top								Disadvantaged	6160f308443383cc62d7b0eb	APPROVED	2021-10-09 01:48:56.291000	2021-10-09 02:41:00.779000	3124.488	20.0	93	0	100	2021-10-14 02:54:20.867000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
92	2021-10-08 19:52:08	2021-10-08 20:52:17	IP Address	50.126.235.105	100	3609	True	2021-10-08 20:52:17	R_3ndrcD1bLP9jBp3					39.53480529785156	-82.40959930419923	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The machine should be able to recognize no one race higher than the other. Their should be an equal balance/ratio between White and African American. However, the graph shows that the probability to correctly allow a medical personnel log in is 20% higher for white individuals compared to African American.	Neither biased nor unbiased.	For this situation, I believe the biased is a bit less noticeable. We do not know based on the research as to why there is a major gap between the probability of correctly determining someone.  However, when it comes to the ability to allow anyone to be recognized - the values are the same. This shows that not just anyone race will have a higher chance of getting into someone's charts if they aren't a medical personnel.	Mostly unusable	This is still one of those situations where you are letting a machine do the work rather than the individual. The correct valued graph still is low to grant access properly - as it was said - this is not perfect but the values aren't great enough to go with the model either. Yes, this value is lower for granting anyone access - but this is a hospital policy... it's not effective enough.	Acceptably fair	When it comes to the rate of the probability of granting access correctly, it is fair to say that the device will recognize the medical personnel involved equally despite their race. This is because the value is 53% for both.	Acceptably biased	However, as compared to the first question - it is seen that those individuals who are white have a higher chance of being granted access to medical charts compared to African Americans - based on the second chart. In my eyes it seems a little biased, but without further reasoning - it is hard to truly say. The machine could be recognizing white more based on the actual hospital staffing. 	Completely unusable	If this machine only as a probability of correctly identifying 53% of medical personnel who should have access to it - this value is VERY low especially seeing that when it comes to white individuals - they could be granted access close to the same rate as someone who should be allowed. It's almost a 50/50. This breaks HIPPA laws for the patients, and thus Model Y should not be used.	Yes	I believe I said the first model wasn't fair, which in a sense it was because the graph was allowing the same value of individuals who were medical personnel be recognized despite their race. I would agree that this is mildly fair, but not fully fair based on the second graph.	Probably model Y	Y is more fair because the value of medical personnel being recognized correctly is equal for both white and african american. 	Probably model X	Model X is more biased due to the fact that it correctly recognized whites more than african americans. If this was fair, these two values would be equal and not 20% different.	Models X and Y are equally useful	My gut is saying that both models are equal in terms of usefulness. When you compare the values of correctly identifying you can that they both in general are still around 50% for each.	Neither model X nor model Y	Neither. I see flaws that could provide leaked information of patients because the rate of correctly recognizing is still very low and the mistake of allowing someone involved who isn't a medical personnel - is still quite. For me, this isn't worth the trouble. 	Probably ${e://Field/pref_model}	With mistakenly denied access, I would say X is more fairy only because the mistakenly granted values are a lot less in terms compared to the model Z. 	Both ${e://Field/pref_model} and Z are equally biased	The graphs are opposites of one another for each model, they both seem to share their biased aspects where african americans are always either less likely to be granted access or are mistakenly denied compared to whites. 	Probably ${e://Field/pref_model}	The reason for this is because I know that mistakenly granted is showing that they are medical personnel and will have to sign in. However, neither model is great to use.		Low		This is because this means that the computer system didn't recognize this person as a hospital staff and just a person who is maybe visiting. This allows for medical records to still be kept safe because it does require someone to log in to see them. 		High		You are now entering the chance that someone who isn't authorized access - this breaks privacy laws and policies. You are putting not only the patients at risk but the records of the entire hospital. 		Low		This is because those that are mentioned in the charts (such as who their family is, emergency contact, any private information that needs to be kept safe, insurance companies, payment plan options, other doctors) that the model holds is being leaked, so it doesn't have much of an impact - it just requires the individuals to log in. The society isn't as disturbed. 		High		Allowing an individual to log in impacts society even more, because you are releasing information from the medical records that shouldn't be spread. You are releasing who the patient's family, who are their emergency contacts, private information, billing, insurance companies, and so forth. 		No			Yes			Yes			Advantaged		Bachelor	Others	61569ee5ec13f9d23135674e	615f943ec32164d0f282bd34	6160f5bbb915ab341dbd1bba	minority	outcome-fpr	frauth	False		model X	top								Advantaged	6160f5bbb915ab341dbd1bba	APPROVED	2021-10-09 01:52:03.616000	2021-10-09 02:52:20.372000	3616.756	32.0	147	0	100	2021-10-14 15:26:53.459000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	English	United States	Female	No	
93	2021-10-08 20:16:54	2021-10-08 20:53:24	IP Address	161.6.7.1	100	2190	True	2021-10-08 20:53:25	R_2wpknPqBmrtfIfX					37.018997192382805	-86.45369720458984	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It works more often with white people than black people.	Very biased	Same reason as before.	Neither useful nor unusable	It correctly recognizes people more often than it grants access to the wrong people, but it also seems to have a racial bias toward white people.	Neither fair nor unfair	It seems to recognize people of different races at an equal rate.	Neither biased nor unbiased	Same as before.	Mostly ununsable	It only works about half the time.	No		Probably model Y	Model Y grants access to black and white people (not by mistake) at the same rate, while Model X doesn't.	Models X and Y are equally biased	Model Y makes mistakes with white people more often, while Model X recognizes a greater percentage of white people than black people.	Definitely model Y	Model X is equally good at granting access to both races but seems to make a lot of mistakes. Model Y doesn't mistakenly let as many people in but is racially biased.	Neither model X nor model Y	Neither of them seem like they have a very good success rate for giving access to the right people.	Both ${e://Field/pref_model} and Z are equally fair	Both models have more trouble with granting access to black people than white people.	Both ${e://Field/pref_model} and Z are equally biased	Same reason as before.	Probably model Z	I would choose Model Z because while it is more skewed towards recognizing white people, it doesn't accidentally deny black people access as much as Model Y.		Moderate		It would make it less convenient for the staff member, although they would still be able to access the device.		High		Anybody who doesn't work there could get access to sensitive information.		Low		"If a hospital worker is having technical difficulties, it could cause problems for their patients or other staff members (the ""society."")"		High		Accidentally giving access to an unauthorized person could hurt the reputation of the hospital and people who work there. It could also violate the privacy of patients.		No			No			Yes			Advantaged		Secondary Education	Others	60e9f87217d55e86d5acd46a	615f943ec32164d0f282bd34	6160fb93485f64b4017c8713	minority	outcome-fpr	frauth	False		model Y	bottom								Advantaged	6160fb93485f64b4017c8713	APPROVED	2021-10-09 02:16:51.225000	2021-10-09 02:53:27.786000	2196.561	22.0	121	0	100	2021-10-13 17:03:45.602000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
94	2021-10-08 20:30:53	2021-10-08 20:56:18	IP Address	99.190.216.17	100	1524	True	2021-10-08 20:56:18	R_XEPD4ISrhnbVgid					32.78080749511719	-115.5626983642578	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Because of the low percentage of accurate White staff being allowed access.	Mildly unbiased	The percentages of it mistakenly allowed access to are equal in both.	Mostly unusable	Again, the same issue arises here. If out of the 41% White personnel allowed access, at least 35% of that is mistakenly given the access, it should not be used.	Mildly unfair	It appears to not be able to readily distinguish yet between the African American personnel. 	Neither biased nor unbiased	It is reading parameters given to it and thus only bases its choices upon an already forged algorithm. 	Mostly ununsable	If out of the 53% of staff that it does allow access to at least a good 45% is not actually personnel, then it shouldn't be used. 	No		Probably model Y	Because despite the disparity of those allowed access, those that were mistakenly granted it so is the same for both.	Probably model X	Because in spite of letting in the same percentage of staff regardless of race, it was still more unable to distinguish correctly between African American personnel and non-personnel. 	Probably model Y	Because of the fact that it gives both equal and, more importantly, lowered mistaken access granted.	Probably model Y	Because of the equal and less mistaken access given for both. 	Probably model Z	Because the times it denied access were both equal and less across both groups.	Probably ${e://Field/pref_model}	Because it denied more access to one group than the other. 	Probably model Z	Because it gave the less and equal denied access across the groups.		High		Because it may take away from the allotted time they may have to work and may take away from their hours at work.		Low		They won't benefit nor be harmed by the fact that they were given access to information.		High		Because society, in this case the patients, may be given delayed treatment. 		High		Because the unauthorized party will have information on the patient which is the society in this case.		No			Yes			Yes			Disadvantaged		Bachelor	Others	6111af7866992ef0d3b27f73	615f943ec32164d0f282bd34	6160fedba0f028645d824ad4	majority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	6160fedba0f028645d824ad4	APPROVED	2021-10-09 02:30:51.628000	2021-10-09 02:56:21.323000	1529.695	26.0	138	0	100	2021-10-14 02:28:25.564000	19AE28A9	United States	United States	Other	Non-Caucasian	Spanish	United States	Female	Yes	
95	2021-10-31 17:03:03	2021-10-31 17:32:39	IP Address	47.157.23.174	100	1776	True	2021-10-31 17:32:40	R_21mH2ZfBzwLkNXO					33.83079528808594	-118.1125030517578	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It is slightly unfair because of the difference in allowing %	Neither biased nor unbiased.	I don't think the machine learning can be inherently biased	Mostly unusable	Not really, it's such a small amount of info	Neither fair nor unfair	Machine learning is not inherently have bias, but the data used could be. With that in mind, it's hard to tell if it is fair without more info	Neither biased nor unbiased	Machine learning is not inherently have bias, but the data used could be. With that in mind, it's hard to tell if it is fair without more info. Same as last question	Mostly ununsable	Not really, without knowing where the info came from/how large of a sample size	No		Probably model Y	Because both groups have an equal opportunity 	Models X and Y are equally biased	Both have their own bias 	Models X and Y are equally useful	I don't believe either are very useful	Neither model X nor model Y	I feel like I need more information, i wouldn't choose either	Definitely ${e://Field/pref_model}	Both groups have an equal chance at the beginning, the mistakes could be variance	Both ${e://Field/pref_model} and Z are equally biased	They both show their own bias	Neither ${e://Field/pref_model} nor model Z	Like the other graphs, the information still feels incomplete and doesn't help me reach a conclusion			High	That would be a drastic situation that the individual would have to deal with, that would add a tremendous amount of stress			High	Essentially the same answer, both these things could bring lots of drama to ones life			Moderate	It's less significant because it is indirectly affecting them, society is anyone indirectly affected by the scenario			Moderate	Same answer as the previous question			Yes			Yes			Yes			Disadvantaged	Bachelor	Others	5e9859b23da07f0416fca0fb	617f102667bd25701aa8461e	617f20a499e41b8397ab5d9b	majority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	617f20a499e41b8397ab5d9b	APPROVED	2021-10-31 23:03:02.177000	2021-10-31 23:32:43.326000	1781.149	29.0	275	1	100	2021-11-03 15:18:36.615000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Male	No	
96	2021-10-31 17:21:07	2021-10-31 17:44:09	IP Address	104.174.237.100	100	1381	True	2021-10-31 17:44:09	R_3F2TdIHlhdwaq7w					33.83030700683594	-118.37950134277344	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	A large percentage in each group was found to be a mistake. 	Neither biased nor unbiased.	I dont think it is biased race wise, if that is what the question is asking, as it is just analyzing data from past patients. Cant see underlying conditions of the two groups, perhaps the caucasians had more serious needs.	Mostly unusable	Large percentage of failure, better to rely on doctors who know the patients ti make the call until the percentage of failure is below 10%.	Mildly unfair	Admitting too many caucasians who are found to be mistakes. 	Acceptably biased	Slight bias toward admitting caucasians who in reality didnt need it but also cannot see what was the issue with the patients.	Completely unusable	Not good at predicting either groups need for icu treatment.	No		Definitely model X	It had the same mistake percentage in each group meaning it messed up in the same proportion of each group.	Probably model Y	It admitted the same percentage into icu but the mistake was heavily in the caucasian group meaning too many were admitted without need.	Probably model X	Has same fail percentage.	Probably model X	It has a similar fail percentage on each side which could lead to more clear ways to fix the programs decision making.	Probably model Z	Equal percentage failure in those who were denied icu treatment.	Probably ${e://Field/pref_model}	A lot more non caucasians were denied icu treatment when needed. 	Probably model Z	Had a more consistent rate of failure in those denied icu who needed it which is more important for saving lives.	Low			Theyre still admitted even if they didnt need it but if they dont need it they can just be taken back to another room.	High			If they dont get icu treatment they can die.	Moderate			Society being others in line for icu treatment. Without the spare room, someone who actually needs it could die.	Low			Society being other patients. They get the benefit of the open icu rooms and the loss of one person will probably not affect them.	No			No			Yes			Advantaged			Bachelor	Education	6172086e1066d2a8d813943c	617f102667bd25701aa8461e	617f24def014936da126a6d2	minority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	617f24def014936da126a6d2	APPROVED	2021-10-31 23:21:04.693000	2021-10-31 23:44:12.468000	1387.775	22.0	45	0	100	2021-11-03 15:43:52.154000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	No	
97	2021-10-31 17:35:39	2021-10-31 18:01:43	IP Address	50.4.253.224	100	1563	True	2021-10-31 18:01:44	R_7TGDcheJQTqPEK5					42.46580505371094	-82.9032974243164	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It is granting more access to Caucasian people	Mildly unbiased	The mistakes are similar, but it is granting access to a larger number of Caucasian doctors vs non	Mostly unusable	While mistakes are less likely, it seems to have trouble reading non-white people	Mildly unfair	Probability of granting access is only around 50% with a large percentage being mistakes.	Mildly unbiased	There is a large percentage of mistakes made for white people, granting access to non-doctors.	Completely unusable	It seems pointless. There are a lot of mistakes on those granted access.	No		Probably model X	Model X seems to have less mistakes	Probably model Y	Model Y seems to accept more white people regardless of if it should.	Probably model X	Model X has less mistakes	Probably model X	Seems to have less mistakes	Probably ${e://Field/pref_model}	I don't really feel it is more fair, neither model seem good. However, model X has less mistakes on those being granted which seems more important.	Definitely ${e://Field/pref_model}	Model X makes more mistakes when denying access to non-whites	Probably ${e://Field/pref_model}	Model X seems to make less mistakes in those being granted access		Low		It isn't really that big of a deal to have to log in. Just an inconvenience. 		High		This is of high importance. You cannot accidentally grant access to a medical data base to someone not meant to use it.		Low		It wouldn't matter to society at all if someone has to take an extra step at logging in. Society is anyone other than the individual - other staff or patients.		Moderate		Society is anyone other than the individual - other staff or patients. Mistakenly allowing someone into a medical database could really hurt a patient if the wrong info got out to the public.		Yes			Yes			Yes			Advantaged		Secondary Education	Business	6173c0bb92e149b4127563fe	617f102667bd25701aa8461e	617f283e0209b46899b23513	minority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	617f283e0209b46899b23513	APPROVED	2021-10-31 23:35:36.291000	2021-11-01 00:01:47.998000	1571.707	43.0	25	1	97	2021-11-05 00:36:51.268000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	No	
98	2021-10-31 17:27:12	2021-10-31 18:03:03	IP Address	107.205.191.212	100	2151	True	2021-10-31 18:03:04	R_3G81e2gu1Tvh3KP					38.266998291015625	-122.04399871826172	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	it looks similar between the two groups	Mildly unbiased	it looks similar between the two groups	Mostly unusable	because there are too many mistakes 	Mildly unfair	The mistakes are more for non-caucasians 	Acceptably biased	They aren't too different.	Mostly ununsable	The access rate still isn't very good 	Yes	It made me realize that the first scenario was really inaccurate. 	Probably model Y	Because they are equally likely to get in	Models X and Y are equally biased	They are biased in different ways	Models X and Y are equally useful	They are both equally useful 	Neither model X nor model Y	Because they are not accurate enough	Both ${e://Field/pref_model} and Z are equally fair	I believe that both are equally fair	Both ${e://Field/pref_model} and Z are equally biased	I believe both are equally biased 	Neither ${e://Field/pref_model} nor model Z	I would choose either 		High		Because unauthorized access is a security risk 		High		Because unauthorized access is a security risk 		Moderate		It's important for doctors to get to where they are going quickly, this could cost lives.		Moderate		People who do not belong in a hospital should not be where they don't belong		No			Yes			Yes			Advantaged		Secondary Education	Others	61758b407988689c1554ecf3	617f102667bd25701aa8461e	617f264422a504f3e999d17b	majority	outcome-fpr	frauth	True		model X	top								Advantaged	617f264422a504f3e999d17b	APPROVED	2021-10-31 23:27:09.246000	2021-11-01 00:03:07.624000	2158.378	37.0	20	1	94	2021-11-05 00:39:13.633000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Male	Yes	
99	2021-10-31 17:43:30	2021-10-31 18:10:51	IP Address	74.130.232.142	100	1641	True	2021-10-31 18:10:52	R_2w4zG7WLf8iKXHT					36.91990661621094	-86.43920135498048	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The data it has I believe it made the best choice	Neither biased nor unbiased.	It's positivity rate was better than its mistakes	Mostly useful	It had a good positivity rate	Acceptably fair	Model y is not perfect as stated but to the best of its ability is has done its job.	Mildly unbiased	Its working to the best if it's ability.	Mostly useful	I believe it is performing to the best of its ability with the knowledge it collected	No		Probably model Y	It seemed to be equal as a whole	Probably model Y	Equality in my opinion	Probably model Y	Seems to be less favorable on either side	Probably model Y	I believe it is not as biased as x	Probably ${e://Field/pref_model}	Model y seems more equal	Probably model Z	Model z favors Caucasian 	Probably ${e://Field/pref_model}	Equality	High			If they don't need ICU support they may receive drastic care that they don't need.	High			ICU requires a different kind of care.	High			They may not need ICU. Society is the general public.	High			ICU is greatest form of care. Different levels of care. Society is the general public	Yes			Yes			Yes			Disadvantaged			Secondary Education	Others	61689ec20ea785be6b264b16	617f102667bd25701aa8461e	617f2a1d80cbd0d7f68fcb3d	majority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	617f2a1d80cbd0d7f68fcb3d	APPROVED	2021-10-31 23:43:27.521000	2021-11-01 00:10:55.110000	1647.589	70.0	84	4	93	2021-11-03 15:37:24.498000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
100	2021-10-31 17:57:47	2021-10-31 18:15:01	IP Address	155.186.208.232	100	1034	True	2021-10-31 18:15:02	R_2ZW8wN7uemb4tZk					33.94610595703125	-83.41799926757811	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	The probability of it granting access to non-caucasians is much lower 	Acceptably biased	The face recognition does not work well for everyone 	Mostly unusable	The face recognition does not work well for everyone, but the incidence of mistakenly granted access is lower so that part is a step in the right direction 	Acceptably fair	Yes as the probability of it working is equal for caucasians and non-caucasians	Neither biased nor unbiased	Not really, the probability of it working is the same for each group 	Mostly ununsable	It seems there is a large incidence of mistakenly granted access	No		Definitely model Y	Model Y has equal probability of the face recognition working for all groups 	Definitely model X	The probability of the facial recognition working for the non-Caucasian group is much lower 	Probably model Y	The probability of granting access is better overall 	Probably model Y	For the same reason as stated above 	Both ${e://Field/pref_model} and Z are equally fair	Model Y has a better probability of the facial recognition working for the non-Caucasian group but model Z has a lower probability of denied access for the non-Caucasian group so they balance out 	Both ${e://Field/pref_model} and Z are equally biased	The probability of facial recognition it equal in one but the probability of getting denied is high, vs the probability of facial recognition is low in the other but the probability of getting denied is also low 	Probably model Z	Mistakes of denied entry are equal between groups 		Moderate		It is time consuming, but won’t prevent a staff member from getting where they need to go		High		This can be potentially dangerous for staff & patients 		High		It may take longer for a medical professional to address patients and conferences family members (I.e society)		High		If society consists of patients and concerned family members, they may find the presence of non-authorized individuals to be dangerous and scary 		No			Yes			Yes			Advantaged		Bachelor	Healthcare Practitioner	61093fd0ed1b90dc46d24d69	617f102667bd25701aa8461e	617f2d77efb3d834d0794153	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f2d77efb3d834d0794153	APPROVED	2021-10-31 23:57:45.028000	2021-11-01 00:15:04.420000	1039.392	23.0	31	0	100	2021-11-03 17:04:41.400000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	Yes	
101	2021-10-31 18:00:25	2021-10-31 18:28:53	IP Address	192.181.203.153	100	1707	True	2021-10-31 18:28:53	R_3r2hoJT1bgODKTy					37.96429443359375	-84.47029876708984	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Data for mistakes is equal 	Very biased	Data skews one way for probability but is equal for mistakes 	Mostly unusable	Possibility of bias	Acceptably fair	Same probability 	Very biased	Much higher percent of mistakes for one group	Mostly ununsable	Possible bias	No		Probably model Y	Each group has an equal chance of being predicted to be ICU required 	Models X and Y are equally biased	Model X has a much higher chance of predicting ICU required for non-caucasians, which seems to be biased. While that chance is equal in Model Y, the percent of mistakes for non-caucasians being so much higher makes me think the probability model is actually biased. The probability seems equal, but that’s clearly not true 	Models X and Y are equally useful	They’re both fair but possibly biased	Neither model X nor model Y	Both seem to have faults and biases	Both ${e://Field/pref_model} and Z are equally fair	Both have large skews in one portion of data, which are nearly the same 	Both ${e://Field/pref_model} and Z are equally biased	Again, similar skews of data just in opposite areas	Neither ${e://Field/pref_model} nor model Z	Both seem to have biases in one area 	Low			You have extra support available but don’t need it. You don’t really lose anything, except perhaps having to pay more. 	High			You could miss out on a bed and supports that you need, and it could be to someone who doesn’t actually end up needing it. 	High			Other patients (society) could have needed that spot. 	Moderate			The hospital (society) becomes liable for making a mistake and not having proper supports for a patient who needs it. Other doctors (society) may have to work harder and possibly out of their specialty to try and revive/rescue a failing patient. 	Yes			No			Yes			Advantaged			Bachelor	Education	610868b46d31887395d251ed	617f102667bd25701aa8461e	617f2e1272e296d9f4fc6072	majority	outcome-fpr	icu	True		model X	top								Advantaged	617f2e1272e296d9f4fc6072	APPROVED	2021-11-01 00:00:22.231000	2021-11-01 00:28:56.816000	1714.585	24.0	41	0	100	2021-11-03 15:28:36.357000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	Yes	
102	2021-10-31 18:17:28	2021-10-31 18:42:03	IP Address	66.161.179.30	100	1475	True	2021-10-31 18:42:04	R_3GkNDhYz4pTC6ra					39.10589599609375	-84.50469970703125	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Despite the mistake rate being equal among Whites and non-whites, Non-Whites will be admitted to the ICU less frequently and may not receive appropriate care.	Very biased	Despite the mistake rate being equal among Whites and non-whites, Non-Whites will be admitted to the ICU less frequently and may not receive appropriate care.	Neither useful nor unusable	The factors that determine a patient's need for ICU should be investigated since the triaging is resulting in different probability of admission for nonwhites vs whites	Acceptably fair	both groups have the same probability of being admitted to icu 	Acceptably biased	more Whites admitted to the icu could have been admitted to a general floor	Mostly useful	seems to triage equally, but need to investigate further the disparities in mistake rates	Yes	I think perhaps the second model is actually more biased, since the mistake rates are much lower for non-Whites admitted to the ICU compared to Whites (suggesting Whites were disproportionately treated with a higher level of care than needed compared to non-Whites)	Probably model X	triage process is different but mistake rates are same, probably less risk for morbidity 	Probably model Y	mistake rates for admission to the icu are higher in one racial group	Probably model X	I think it would be easier to tailor the triage factors for Model X to make it more fair/useful	Probably model X	Reasons stated above - it may be easier to remove the bias/errors from Model X	Probably model Z	the risk is higher to make a mistake among patients who needed icu care but did not receive it	Probably ${e://Field/pref_model}	although fewer mistakes were made among the non-white patients triaged to icu, greater mistakes were made for the same racial group among patients triaged inappropriately to the other floor, increasing their risk of morbidity/mortality	Definitely model Z	I would rather a patient be inappropriately triaged to a higher level of care than a lower one	Low			If an individual is triaged to ICU care that they do not end up needing, they are at no higher risk for morbidity or mortality because they have a higher level of care	High			A very sick patient who is inappropriately triaged to another floor besides the ICU is at risk of deteriorating without the medical team being able to appropriately respond and give them the best outcome	Moderate			A patient who takes up an ICU bed that they don't truly need will deplete resources needed for other ill patients. Also, the financial and emotional impact of an ICU stay is greater on the patient and their family compared to a regular hospital bed stay.	Moderate			If a patient is very ill in a regular hospital bed, their nursing and medical staff will have to devote more time to them which may take away necessary care from other floor patients. The patient's risk of deteriorating and having a poor outcome is higher which affects the patient's family, job, etc. 	No			No			Yes			Disadvantaged			Doctoral	Healthcare Practitioner	6175dcd3852d785c86e53d72	617f10dcb9390b6c13530629	617f32130f0e91a9e1bc643b	minority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Disadvantaged	617f32130f0e91a9e1bc643b	APPROVED	2021-11-01 00:17:25.996000	2021-11-01 00:42:07.870000	1481.874	30.0	76	0	100	2021-11-03 01:30:20.490000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	No	
103	2021-10-31 18:29:58	2021-10-31 18:43:12	IP Address	67.252.54.66	100	793	True	2021-10-31 18:43:12	R_eINFWwgW2EwxE5z					42.49119567871094	-78.48320007324217	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Error rate is identical.	Neither biased nor unbiased.	Error rate is identical	Neither useful nor unusable	Error rate is still too high to be usable.	Mildly unfair	Disparity between the two mistake groups is too large. 	Mildly unbiased	Error rate is far too high.	Mostly ununsable	Error rate is too high.	No		Definitely model X	Error rate is identical.	Definitely model Y	Disparity in the error rate.	Definitely model X	Error rate is identical.	Definitely model X	Error rate is identical.	Both ${e://Field/pref_model} and Z are equally fair	Neither is fair, the error rates are far too high on both admitted and denied.	Both ${e://Field/pref_model} and Z are equally biased	Error rates are far too high on both admitted and denied.	Neither ${e://Field/pref_model} nor model Z	Error rates are too high.	High			If a mistake is made, an individual is taking up an ICU bed that they do not need, while an individual that does need it could potentially be dying.	High			The individual could potentially die without the advanced access to intensive care.	Low			Society doesn't truly care about the individual.	Low			Society doesn't truly care about the individual.	No			No			Yes			Advantaged			Secondary Education	Engineering and Technology	614e2db162f59caf94882a05	617f102667bd25701aa8461e	617f34fba2f7450cb5ab58ea	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	617f34fba2f7450cb5ab58ea	APPROVED	2021-11-01 00:29:55.907000	2021-11-01 00:43:15.279000	799.372	43.0	21	0	100	2021-11-03 17:20:05.511000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
104	2021-10-31 18:36:49	2021-10-31 18:50:47	IP Address	173.171.90.158	100	837	True	2021-10-31 18:50:48	R_1LFF61eahCw6awZ					27.91619873046875	-82.72989654541014	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 1	Mildly unfair	The probability is extremely slanted 	Mildly unbiased	It is not leaning one way or another	Mostly useful	It has consistent data time and time 	Mildly unfair	The probability is very slanted in this model 	Neither biased nor unbiased	The probability is very even 	Mostly useful	It has consistent results 	No		Probably model X	It has more consistent results and probability 	Probably model X	It has more consistent results and probability 	Probably model Y	It has more consistent results and probability 	Neither model X nor model Y	It has more consistent results and probability 	Probably ${e://Field/pref_model}	It seems this model has been tested more thoroughly 	Probably ${e://Field/pref_model}	I like the title	Probably ${e://Field/pref_model}	The data seems unobjectionable 		High		Medical records are very private  		Moderate		This seemed like the appropriate weight for measurement 		Moderate		Hospital staff have varying levels of access and bo varied level of knowledge 		Moderate		Society comes complacent 		Yes			Yes			Yes			Advantaged		Bachelor	Services Occupations	5c94745e6014f5001231d322	617f102667bd25701aa8461e	617f369c3103739cf75a3de1	minority	outcome-fpr	frauth	False		model Y	bottom								Advantaged	617f369c3103739cf75a3de1	APPROVED	2021-11-01 00:36:46.474000	2021-11-01 00:50:50.340000	843.866	30.0	584	9	96	2021-11-03 16:30:05.349000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
105	2021-10-31 18:45:40	2021-10-31 19:01:22	IP Address	107.116.13.34	100	942	True	2021-10-31 19:01:23	R_2XpWAvHrpiH06PS					42.46519470214844	-83.37129974365233	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Because when it found a mistake is had a equal opportunity to be any race	Acceptably biased	It has a lower chance to grant access to Non-Caucasian, but has an equal chance to mess up with Caucasian.	Mostly unusable	Simply because it is less likely to recognize Non-Caucasian	Very fair	Because they are both equal	Neither biased nor unbiased	Because it is equal	Very useful	Because it has equal opportunity to be even between Caucasian and Non-Caucasian.	Yes	Unsure 	Definitely model Y	Equal points of granting access.	Definitely model X	Less Non-Caucasian weren't granted access.	Definitely model Y	Because it gas equal opportunity to give access to any race 	Definitely model Y	Because it gives equal opportunity to be granted access	Probably ${e://Field/pref_model}	Unsure	Probably ${e://Field/pref_model}	More Non-Caucasian denies were mistakes	Definitely model Z	Less mistakes		High		Because anyone could watch what they type and learn from it.		High		Anyone can get access to anything and could cause people injury or other harmful things.		Low		Unsure		High		If it got out that medical records got out, the news would cover it and it would affect the whole world (society). 		No			Yes			Yes			Advantaged		Secondary Education	Others	61700ceea50b32716181325c	617f102667bd25701aa8461e	617f38ae9a9b698085047f0a	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f38ae9a9b698085047f0a	APPROVED	2021-11-01 00:45:38.887000	2021-11-01 01:01:27.079000	948.192	22.0	69	0	100	2021-11-04 15:44:24.122000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Female	No	
106	2021-10-31 18:48:09	2021-10-31 19:12:29	IP Address	73.218.19.219	100	1459	True	2021-10-31 19:12:30	R_2WYhdaf4Ex5ypf5					42.34739685058594	-71.158203125	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Because if you are non-Caucasian, you are more likely to have an allowed payment. 	Acceptably biased	Because even though the errors are the same percentages, there is a higher chance a Caucasian will be denied. 	Mostly useful	Even if it was slightly biased, the percentages of errors did provide helpful data that the errors were made without any influence of race.	Very fair	Everyone has equal probability of an allowing payment.	Very unbiased	It gave everyone an equal chance of getting approved, but it displays how non-caucasians are more likely to have been fraudulently paying.	Very useful	It had an unbiased approach.	Yes	I would change them to say that model X is a lot more biased in choosing which people to accept payments from.	Definitely model Y	The probability of an allowed payment is equal for everyone, and this is not the case in Model X. It is only in the mistakes that Model Y is revealed that more mistakes are made with non-caucasians. 	Definitely model X	It did not allow everyone's payments to be allowed equally. 	Models X and Y are equally useful	Model X shows that even with initial bias, the mistake percentages remain the same, and so the allowed payments should later reflect that. Model Y gives everyone the equal opportunity, but reveals where there are more mistakes made. 	Probably model Y	I think it would reveal more about why the software allowed the people who were later revealed as mistakes to get through the approval in the first place.	Definitely ${e://Field/pref_model}	Model Z is pretty much the exact same as Model X where one group is more likely to get approval.	Definitely model Z	It does not give everyone the same chance of being approved.	Definitely ${e://Field/pref_model}	It shows a lot better than when the payments are equally offered, they are more wrong when it comes to caucasians.			Moderate	The parties are both notified when this happens, and because the payment was real, the funds were returned and can just be paid again. 			High	The landowner will not be able to cover the current expenses without getting paid. 			High	Society could be credit card companies and banks. If they think someone might have a bad reputation, they might not be allowed to get loans or apply for new credit cards. 			Moderate	Society would be anyone using the prediction model as a way to obtain payments. It would decrease their trust in the system and it would have to be built up again.			Yes			No			Yes			Advantaged	Bachelor	Engineering and Technology	612a7b51f5c0574e3a0f4003	617f10dcb9390b6c13530629	617f3943aa698c263f4a6972	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f3943aa698c263f4a6972	APPROVED	2021-11-01 00:48:06.876000	2021-11-01 01:12:32.703000	1465.827	21.0	36	1	99	2021-11-03 00:50:09.539000	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Female	Yes	
107	2021-10-31 18:38:34	2021-10-31 19:13:30	IP Address	69.204.161.132	100	2096	True	2021-10-31 19:13:31	R_31t5TLVCmNSivs1					43.0592041015625	-73.73560333251953	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	I feel as though I can see both sides of why people would and wouldn’t like it. I think those who rent out their property would like to trust data and base the renting process on that. I think those who get denied the renting process would feel like they perhaps are the outlier and were not given a fair chance	Neither biased nor unbiased.	This is tricky, I think it as I mentioned above there’s reason to like and dislike it. So I struggle with if it’s bias or not.	Mostly useful	Because it relies on data. So there is some sort of reliability to it since it works on patterns and numbers.	Acceptably fair	Looks like racially it is even amongst who is allowed payment (yellow graph)	Mildly unbiased	Looks even among races.	Mostly useful	Based on numbers and patterns	Yes	It’s fairness	Probably model Y	Even among races	Probably model X	Uneven among races 	Models X and Y are equally useful	Unsure	Probably model Y	Racially even	Probably ${e://Field/pref_model}	Mostly even	Both ${e://Field/pref_model} and Z are equally biased	Comparing the two, they seem more even In their errors vs acceptance 	Probably model Z	Acceptance and error seem more appropriate and even			Moderate	Depends on the size of the complex.			Moderate	Fraud requires a lot of issues			High	Part of society includes those within the community directly. Denying the transaction can interfere with finances and maybe their credit			Moderate	Society is the community around the place where they are renting. Allowing the payment can interfere with their finances as well as credit.			Yes			Yes			Yes			Advantaged	Master	Education	6107d9f8e3bcf0b428aee973	617f102667bd25701aa8461e	617f370562b252b80b2d5da3	minority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f370562b252b80b2d5da3	APPROVED	2021-11-01 00:38:31.808000	2021-11-01 01:13:33.410000	2101.602	28.0	32	0	100	2021-11-03 16:45:07.204000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	Yes	
108	2021-10-31 18:49:47	2021-10-31 19:19:23	IP Address	174.241.81.177	100	1775	True	2021-10-31 19:19:23	R_3qVYc5A4EbyR93s					42.10969543457031	-87.93990325927734	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to be accepted than female applicants.	Figure 2	Mildly unfair	It favors one group over the other when granting access	Acceptably biased	It has a higher access rate for Caucasian group 	Mostly unusable	Too high of errors, and favors one group ones another	Very unfair	It provided the same amount of access for both groups 	Mildly unbiased	Not when it doesn't grant more access for one group over another 	Mostly ununsable	Too high of a degree of error	No		Probably model Y	Probability of access granted is equal	Definitely model X	Each group has the same as mistakes, but it allows more access for group 1	Models X and Y are equally useful	One will grant access easily for both, while the other (x) has same error of access 	Probably model X	Less rate of error	Probably model Z	Mistakes in denied access are the same 	Probably model Z	Mistakes in granted access are more for one group that another 	Probably ${e://Field/pref_model}	Less mistakes mass 		Low		They just have to use the original way		High		Security issues. If unauthorized personal can have access to personal information		Low		All groups, individuals, people.. students, adults, young kids.  It wouldn't have a negative impact, they would just have to use the secure way, that things are done now 		High		People in groups that make up the United States population. Personal, financial information could be accessed, individuals wouldn't feel their information was being protected		No			No			Yes			Advantaged		Bachelor	Others	6007b5de4508d81c9a2a0433	617f102667bd25701aa8461e	617f39a69776330bb01276cf	minority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	617f39a69776330bb01276cf	APPROVED	2021-11-01 00:49:45.438000	2021-11-01 01:19:26.867000	1781.429	40.0	149	0	100	2021-11-03 16:04:26.061000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
109	2021-10-31 19:03:35	2021-10-31 19:19:33	IP Address	184.83.56.199	100	957	True	2021-10-31 19:19:34	R_3GrKayR3xuUnlyl					43.522903442382805	-96.78600311279295	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The percentage of mistakes was equal among both caucasians and non-caucasians	Acceptably biased	It is predicting icu needed more often for nom-Caucasians; however, the percentage of mistakes made is the same. That makes me think the model is correctly finding patients who need icu care.	Mostly useful	Because it is predicting who needs icu care without a significant variance of mistake among the two groups used	Very unfair	There are more mistakes made among non-caucasians	Acceptably biased	It is consistently giving the wrong prediction for one group of people more often than the other group	Completely unusable	It is wrong close to 50% of the time for one of two groups	No		Probably model X	Because of the wide disparity in the level of error in model y	Probably model Y	Model y shows a higher level of error with one group	Probably model X	The level of error is equal in both groups	Definitely model X	The level of error is equal in both groups	Both ${e://Field/pref_model} and Z are equally fair	They are having about the same results, only in reverse	Both ${e://Field/pref_model} and Z are equally biased	They are having about the same results, only in reverse	Neither ${e://Field/pref_model} nor model Z	They are having about the same results, only in reverse	Moderate			An individual may be stuck with a higher healthcare bill than they could have had	High			They may not receive an appropriate level of care	Moderate			High hospital bills are a systemwide burden 	High			Unneeded death or illness is difficult on individuals and families 	Yes			No			Yes			Advantaged			Master	Others	5e7e70c583c34c53f6c643db	617f102667bd25701aa8461e	617f3cd3866f5f4994401de0	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	617f3cd3866f5f4994401de0	APPROVED	2021-11-01 01:03:33.065000	2021-11-01 01:19:37.459000	964.394	41.0	469	2	100	2021-11-03 15:16:12.520000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
110	2021-10-31 19:12:08	2021-10-31 19:26:39	IP Address	65.191.20.189	100	871	True	2021-10-31 19:26:40	R_4PzQFjqC5KAfKa5					35.001007080078125	-78.93450164794922	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Race based	Very biased	Race based	Completely unusable	Race based	Very unfair	Anything that uses race as a factor is immediately considered unfair to me.	Very biased	Again, race	Completely unusable	Race	No		Models X and Y are equally fair	Race based. It’s 2021 so this is ridiculous 	Models X and Y are equally biased	Race based	Models X and Y are equally useful	Race based	Neither model X nor model Y	Race based	Both ${e://Field/pref_model} and Z are equally fair	Race based	Both ${e://Field/pref_model} and Z are equally biased	Race based	Neither ${e://Field/pref_model} nor model Z	Race based			High	Denying payments can lead to eviction			High	Allowing fraudulent payments can lead to income loss in the long run			High	Unsure 			High	Allowing fraudulent payments can drive costs up over time. I thought of society as renters			Yes			No			Yes			Disadvantaged			61561ce2fdf4525f50aed402	617f10dcb9390b6c13530629	617f3ee0d9c23f247e6e1027	majority	outcome-fpr	rent	False		model X	top								Disadvantaged	617f3ee0d9c23f247e6e1027	APPROVED	2021-11-01 01:12:06.050000	2021-11-01 01:26:42.743000	876.693	39.0	62	0	100	2021-11-03 14:46:36.695000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	No	
111	2021-10-31 19:08:20	2021-10-31 19:29:27	IP Address	172.58.238.185	100	1267	True	2021-10-31 19:29:28	R_1QbJOIejn4ZfxlT					40.72419738769531	-74.19770050048828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	What’s the basis?	Neither biased nor unbiased.		Neither useful nor unusable		Neither fair nor unfair	Relevancy	Neither biased nor unbiased		Neither useful nor unusable		No		Probably model Y	Closer stats	Models X and Y are equally biased	Based on what information 	Models X and Y are equally useful		Neither model X nor model Y	The charts are not providing additional information 	Probably ${e://Field/pref_model}	Only denied	Both ${e://Field/pref_model} and Z are equally biased	Equally	Neither ${e://Field/pref_model} nor model Z				High	May greatly effect the individual 			High	Creates a domino effects			High	The individual needs to reside somewhere and the resources of the municipality may be affected if the individual is denied living arrangements 			High	Will affect the community and municipality			No			No			Yes			Advantaged	Bachelor	Others	5e5da6ffeacf5d0ccea9d3a8	617f102667bd25701aa8461e	617f3df7b96899ce379cf384	minority	outcome-fpr	rent	False		model Y	bottom								Advantaged	617f3df7b96899ce379cf384	APPROVED	2021-11-01 01:08:17.070000	2021-11-01 01:29:33.643000	1276.573	25.0	81	1	99	2021-11-03 16:35:46.505000	19AE28A9	United States	United States	Full-Time	Caucasian	DATA EXPIRED	United States	Male	DATA EXPIRED	
112	2021-10-31 19:25:54	2021-10-31 19:40:27	IP Address	71.89.27.251	100	873	True	2021-10-31 19:40:28	R_3hnhCc8aoGPYEd2					44.522796630859375	-89.55829620361328	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It has lower probability for non Caucasian	Very biased	Same as before	Mostly useful	It still had low percentage of mistakes	Neither fair nor unfair	It has almost a 50/50 rate	Neither biased nor unbiased	Unsure	Mostly useful	It's still granting access	No		Probably model X	The mistakes are lower	Probably model X	Probable against non Caucasians	Probably model X	Lower mistakes	Probably model X	Lower mistakes	Both ${e://Field/pref_model} and Z are equally fair	Unsure	Both ${e://Field/pref_model} and Z are equally biased	Neither are perfect	Neither ${e://Field/pref_model} nor model Z	Too high of mistakes		Low		Shouldn't be a problem if they know their credentials		High		Hippa regulations		Moderate		Society would be other staff or patients and they could learn a user name and password and hack the system		High		Patients and other staff. Information could get into the wrong hands 		Yes			No			Yes			Advantaged		Bachelor	Others	610831543e509cc882f58dc3	617f102667bd25701aa8461e	617f4219b40564a3b4534bc0	minority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	617f4219b40564a3b4534bc0	APPROVED	2021-11-01 01:25:51.658000	2021-11-01 01:40:31.298000	879.64	26.0	80	0	100	2021-11-03 15:22:08.564000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
113	2021-10-31 18:51:33	2021-10-31 19:44:53	IP Address	67.81.30.28	100	3199	True	2021-10-31 19:44:54	R_CZY43a8styw2RMd					40.86219787597656	-73.89019775390625	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Race issue as in model Y	Very biased	Race issue as in model Y	Completely unusable	Making business decisions based on race is prejudiced 	Very unfair	I think using race in the model is unfair but aggregating non-Caucasians into one group is very unfair.	Very biased	Two groups: Caucasians and everyone else. It's very presumptive.	Completely unusable	Making businesses decisions based on race is prejudiced 	No		Probably model Y	The only choice I had; I would have prefer to answer that both models are equally unfair.	Probably model X	The only choice I had; I would have prefer to answer that both models are equally biased.	Probably model Y	The only choice I had; I would have prefer to answer that both models are equally unusable 	Neither model X nor model Y	For all the reasons stated in previous responses.	Probably ${e://Field/pref_model}	It predicts equal payment probability for whites and non-whites	Definitely model Z	It allows significantly less payments non-whites despite equal payment  mistakes among both groups.	Neither ${e://Field/pref_model} nor model Z	Race bias, especially as it relates to renting a home. Same bias occurs with home lending.			High	A renter is denied the ability to find a new home and will likely feel upset about the denial			High	Because of the loss of revenue			High	Society as opposed to the individual are those who are indirectly affected. If it affects an individual and then society as a result, then the impact is high overall 			Low	Society as opposed to the individual are those who are indirectly affected. The fraud affects the individual mostly through the loss of revenue. The impact on society is minimal			Yes			No			Yes			Disadvantaged	Master	Education	5c92810addeb3d0018a19de3	617f10dcb9390b6c13530629	617f3a06787fd7c883776a87	minority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	617f3a06787fd7c883776a87	APPROVED	2021-11-01 00:51:25.055000	2021-11-01 01:44:57.042000	3211.987	41.0	299	0	100	2021-11-03 14:09:06.550000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Male	No	
114	2021-10-31 19:32:13	2021-10-31 19:51:37	IP Address	108.66.76.187	100	1163	True	2021-10-31 19:51:38	R_3rYZRhkGCwXtwqn					38.697601318359375	-90.36039733886719	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Still biased	Very biased	Looks like a more fair model but unsure of how the ethnicity is being determined. 	Neither useful nor unusable	Bias and variables 	Mildly unfair	I feel like there can be bias which makes it unfair, there are too many variables. 	Very biased	Very broad, caucasion/non caucasion, how are these being reported? Is the individual stating this themselves or is someone else 	Neither useful nor unusable	Too many variables	No		Probably model Y	The probability is equal, while the mistakes are based off of the accepted payments 	Definitely model X	Payment should be taken regardless of race, the mistakes are not based off of this factore	Probably model Y	Still too many variables and bias but it makes the most sense out of the two 	Probably model Y	Again, out of the two this seems like the better option, Allowing payment shouldn't really be compared to the mistakes. Its not a direct correlation, too many variables and too much bias. 	Probably ${e://Field/pref_model}	Mistakes cannot be predetermined, especially based off race	Definitely model Z	Disproportional percentage of payments allowed to be accepted 	Probably ${e://Field/pref_model}	Accept the payment and deal with the risk after, potential racism isn't worth declining a payment 			High	The renter will feel degraded, offended, and like trust has been broken, When they signed the lease they said they could pay that amount, it is rude to doubt that 			Low	These things can be fixed, however a relationship with the renter will be harder to repair. 			High	(Americans) Would find this extremley offensive if it was shown to have a direct correlation to race. We are in a very sensitive time and need to show respect to all. 			Low	Again, money can be sent back/covered/etc. but it will be harder to mend the relationship with the person who is paying you monthly 			No			No			Yes			Advantaged	Bachelor	Healthcare Practitioner	60fca4ea3370270b8bf97614	617f102667bd25701aa8461e	617f4397f93e52735ef5a27d	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f4397f93e52735ef5a27d	APPROVED	2021-11-01 01:32:11.499000	2021-11-01 01:51:39.938000	1168.439	27.0	65	0	100	2021-11-03 16:08:18.663000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	Yes	
115	2021-10-31 19:39:29	2021-10-31 19:52:54	IP Address	99.58.1.192	100	805	True	2021-10-31 19:52:55	R_1FtD4FCtAFV0yVm					32.82420349121094	-96.75070190429688	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Mildly unfair	There is a difference in racial groups in prediction for needing ICU that seems pretty significant. It does not seem very unfair because mistakes made did not see such a disparity.	Neither biased nor unbiased.	Not really able to tell. Evidence of racial disparity is not evidence of bias.	Mostly useful	Can give good insight into success rates	Mildly unfair	There is no disparity in predicting whether an ICU bed is needed, but the disparity for mistakes made is large. I'd be curious to the numbers in each group being sampled.	Neither biased nor unbiased	Unable to tell bias based on data with disparate outcome.	Mostly useful	Especially to see where mistakes are made and that there is a big disparity, this information could be useful to hospitals to guide decision-making.	No		Models X and Y are equally fair	They both show disparity in different areas.	Models X and Y are equally biased	Unable to know because disparate outcomes is not evidence of model bias.	Models X and Y are equally useful	They both give insights that could be useful.	Probably model X	Mistakes made seems to be the more dire outcome, and this one is even amongst racial groups.	Both ${e://Field/pref_model} and Z are equally fair	Disparate outcomes does not show fairness.	Both ${e://Field/pref_model} and Z are equally biased	Disparate outcomes does not show bias.	Probably ${e://Field/pref_model}	Equal negative outcomes among race	Low			They will have more intensive care available if they need it, but if they don't, they can be moved back.	High			They will not have the services required when they're needed.	Moderate			This could be denying someone else in need of a space in the ICU. I'm considering society to be the hospital staff, community, other patients, family of patients.	High			This could lead to unnecessary complications or death. I'm considering society to be the hospital staff, community, other patients, family of patients.	Yes			No			Yes			Advantaged			Master	Education	60fcde7858e7b5e782c587e3	617f102667bd25701aa8461e	617f454bf8510f4360907d69	minority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	617f454bf8510f4360907d69	APPROVED	2021-11-01 01:39:27.918000	2021-11-01 01:52:58.204000	810.286	30.0	125	0	100	2021-11-03 16:13:35.059000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
116	2021-10-31 19:24:32	2021-10-31 19:53:25	IP Address	184.54.147.52	100	1733	True	2021-10-31 19:53:26	R_2QlAjMqDIinSzFs					39.0697021484375	-84.29160308837889	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The amount of mistakes made were even based on race. The amount of times accepted isn’t something I could compare because I don’t know the proportion of Caucasian vs non Caucasian employees.	Mildly unbiased	It doesn’t seem to make mistakes based on race	Mostly unusable	The amount of mistakes, although evenly biased, is really high	Mildly unfair	Caucasians have a much higher rate of mistakes 	Acceptably biased	There are almost 2x more mistakes made for caucasians than for non caucasians	Mostly ununsable	The rate of error is very high and biased	No		Models X and Y are equally fair	They both have some form of bias that gives different rates of success and mistakes based on race	Models X and Y are equally biased	They both have biases and there isn’t a good way to say which is more biased without more information	Probably model Y	The totals of the probability for mistakes in model y are slightly less than that one model x. Model x has 35% for both which is a total of 70%. Model y has 45% + 24% which totals to 69%.  	Neither model X nor model Y	They’re both biased and full of errors, and frankly I think facial recognition for medical equipment use is a bad idea. 	Both ${e://Field/pref_model} and Z are equally fair	Model X is about as likely to deny a non Caucasian rightful access as model Z is to allow a Caucasian access they shouldn’t have. Neither of those are good. 	Both ${e://Field/pref_model} and Z are equally biased	They both are biased. Model X is biased to deny non caucasians. Model Z is biased to accept more caucasians. 	Neither ${e://Field/pref_model} nor model Z	They’re both terrible. The rates of mistakes are far too high and they’re both biased in favor of caucasians. 		Moderate		A worker could need to quickly access supplies or equipment in order to help a patient, but they should be able to quickly enter a password so this should not slow them down much. 		High		Someone could access sensitive records, medications, and equipment that they are not supposed to have which could cause damage to them and the people around them. 		Moderate		I consider part of the society to be the rest of the people in the hospital, both patients and staff. The risk here could be delayed and lower quality patient care, as well as potentially delaying medication or treatment and confusion. 		High		Society would be the rest of the people in the hospital, both patients and staff. The risk here is that patients personal information could be leaked to unauthorized users and used in malicious ways. Treatment could be delayed, causing unnecessary pain and discomfort. Schedules for employees could be thrown off because they don’t have access to their systems. 		No			No			Yes			Advantaged		Secondary Education	Services Occupations	60fdd86a4f7b09b8e800cbc8	617f102667bd25701aa8461e	617f40f4b0877c73aae4c200	minority	outcome-fpr	frauth	True		model X	top								Advantaged	617f40f4b0877c73aae4c200	APPROVED	2021-11-01 01:22:56.536000	2021-11-01 01:53:29.123000	1832.587	20.0	35	0	100	2021-11-03 16:39:49.824000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
117	2021-10-31 19:45:54	2021-10-31 20:05:08	IP Address	72.192.76.23	100	1153	True	2021-10-31 20:05:08	R_12mZBbLiCY4aBWe					36.09440612792969	-95.9717025756836	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	It depends on how the model was created	Neither biased nor unbiased.	It depends if there was bias when creating the model	Mostly unusable	Its not very accurate for preventing fraud	Neither fair nor unfair	It depends on the criteria used to create the model.	Neither biased nor unbiased	It depends on the criteria used to create the model contained a bias.	Mostly ununsable	It doesn't seem very good at predicting fraud	No		Models X and Y are equally fair	There is no way to tell from the data given here if either is fair.	Models X and Y are equally biased	There is no way to tell from the data given here if either is biased	Models X and Y are equally useful	Neither model is very accurate for detecting fraud.	Neither model X nor model Y	They both suck	Both ${e://Field/pref_model} and Z are equally fair	There is no way to tell from the data given here if either is fair.	Both ${e://Field/pref_model} and Z are equally biased	There is no way to tell from the data given here if either is biased.	Neither ${e://Field/pref_model} nor model Z	they both suck			Moderate	It causes a great deal of stress.  It could potentially also cause additional transaction fees.			Moderate	If they are caught afterwards then the penalty/crime would be greater than if it was stopped in advance.			Low	Family and friends of the renter could help them cope with the stress of the situation.			Moderate	Assuming this would be claimed on insurance it would cause insurance rates to go up for everyone, the corporation would lose profits from writing off the claim potentially lowering payroll, etc.			Yes			Yes			Yes			Advantaged	Bachelor	Engineering and Technology	5d5223f2ed6cf6000171f739	617f102667bd25701aa8461e	617f46c8d49d40f188cc26f9	majority	outcome-fpr	rent	False		model X	top								Advantaged	617f46c8d49d40f188cc26f9	APPROVED	2021-11-01 01:45:48.759000	2021-11-01 02:05:11.018000	1162.259	40.0	192	1	100	2021-11-03 16:34:00.790000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Male	No	
118	2021-10-31 19:45:25	2021-10-31 20:14:28	IP Address	162.207.120.130	100	1742	True	2021-10-31 20:14:28	R_3KwP2QoYNYLldgf					38.74009704589844	-90.62180328369139	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Acceptably fair	Fairly equal, many payments get through	Mildly unbiased	Fairly equal, many payments get through	Mostly useful	Fairly equal, many payments get through (not sure I'm reading the graphs right??)	Neither fair nor unfair	Only half of the payments go through	Acceptably biased	More non-white people are having fraudulent payments	Neither useful nor unusable	It's obviously biased against people of color	Yes	I think I understood the graphs better with the second model. Both are biased and not super fair, although I suppose the first one is better because less fraud is being committed. Also, it's better for payments to be stopped and the person told about it so they can submit another payment as needed.	Probably model X	It's better for payments to be stopped and the person can submit another payment, even if it's biased against who is allowed to submit payments	Definitely model Y	It assumes non-white people are frauds	Definitely model X	More payments overall will be received because it's possible for someone to resubmit payment	Definitely model X	More payments overall will be received because it's possible for someone to resubmit payment	Probably ${e://Field/pref_model}	Same as before	Definitely model Z	Same as before	Definitely ${e://Field/pref_model}	Same as before, more payments received overall			Moderate	Depends on the renter, landlord, how many people live in the place, etc. If it's a low-income renter, they're much more likely to be evicted for not paying, whereas if they're a mid- or high-income renter, the landlord is frequently more likely to be able to work with their landlord to pay later.			High	Potential for eviction			Moderate	Again, depends on how many people are affected by the transaction. If you're talking about extended family, that could be devastating to try and house a family. If you're talking about landlords, they don't really care if they can get the payment or they can get another tenant. As for broader society, people in the community, it hardly matters			Moderate	Again, depends on how many people are affected by the transaction. If you're talking about extended family, that could be devastating to try and house a family. If you're talking about landlords, they don't really care if they can get the payment or they can get another tenant. As for broader society, people in the community, it hardly matters			Yes			No			Yes			Disadvantaged	Master	Others	5c11e3cb6d1ac90001021493	617f102667bd25701aa8461e	617f46b1891629be8d151620	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	617f46b1891629be8d151620	APPROVED	2021-11-01 01:45:23.832000	2021-11-01 02:14:31.288000	1747.456	28.0	49	0	100	2021-11-03 15:57:12.913000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
119	2021-10-31 20:01:00	2021-10-31 20:23:54	IP Address	174.211.46.96	100	1373	True	2021-10-31 20:23:54	R_3jcHqXSIrtw2Khb					42.51939392089844	-83.24819946289062	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 2	Male applicants will be more likely to be accepted than female applicants.	Figure 2	Very unfair	There’s too big a discrepancy between approval granted to Caucasian vs. Non-Caucasian subjects.	Very biased	Too big a discrepancy between approval rates between Caucasian snd Non-Caucasian subjects.	Mostly unusable	Not enough Non-Caucasian people eeee recognized and granted access.	Neither fair nor unfair	Both groups of people, Caucasian snd Non-Caucasian, were equally granted access.	Mildly unbiased	A larger percentage  of mistakes was made with Caucasian subjects. 	Mostly ununsable	The rate of error is too large	No		Probably model Y	Smoker approval rates between the two groups.	Probably model X	Greater discrepancy between the two groups in terms of approval. 	Models X and Y are equally useful	Both have troublesome aspects.	Probably model Y	The problems with X seem less extreme.	Probably model Z	Less discrepancy in errors between the two groups.	Definitely ${e://Field/pref_model}	Bigger discrepancy  in errors between the two subject groups.	Probably model Z	Less difference in error percentages between the two subject groups.		High		Unauthorized people can access records which are potentially embarrassing or have access to restricted drugs.		High		Peopjj oh es’ lives could be permanently s as layered or people could be killed due to a mistake by unauthorized people.		Moderate		The general public can easily lose faith in the hospital syatem		Moderate		Society is the local public who might be consumers at the hospital system. The impact could be potentially huge-even impacting whether patients seek health care from a hospital system the have lost faith in: 		No			Yes			Yes			Advantaged		Master	Education	6144834d136ed7f27d2dbe60	617f102667bd25701aa8461e	617f4a5719be78bc751aa1d6	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f4a5719be78bc751aa1d6	APPROVED	2021-11-01 02:00:57.840000	2021-11-01 02:23:57.305000	1379.465	48.0	108	1	100	2021-11-03 17:17:39.866000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
120	2021-10-31 19:44:27	2021-10-31 20:24:24	IP Address	74.110.152.159	100	2396	True	2021-10-31 20:24:24	R_3Gemg8umAgWudtf					37.6239013671875	-77.43090057373047	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It grants access to non medical personnel 	Neither biased nor unbiased.	It is a computer program.	Mostly unusable	It would lock out medical personnel to often.	Acceptably fair	The Model is equally successful. 	Neither biased nor unbiased	Equally successful	Neither useful nor unusable	The margin of error seems to be to high	Yes	It looks like Model X has more room for error.	Probably model Y	It has less chance of not granting access to medical staff	Probably model X	I would not say it is biased but it has more trouble recognizing Caucasians	Probably model Y	It would allow more medical staff to log in	Probably model Y	It would not lock out medical staff as much	Probably model Z	Model Z has a lower chance of making a wrong choice and a higher chance of making the right decision.	Probably model Z	It has a higher chance of granting access to one group than the other.	Probably model Z	Less chance of locking medical personnel out.		Moderate		It is still important that the right person is identified correctly.		High		The unauthorized user has access to important information that might be harmful to society.		High		Important information might get out to someone who can use it to benefit themselves.  society would be the family of the patient and the patient.		High		Important information in the wrong hands.  The patient and relatives of the patient.		Yes			Yes			Yes			Advantaged		Bachelor	Others	61743e225681c2932a38ba85	617f102667bd25701aa8461e	617f45f3157f74a9f2c7d7c3	majority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f45f3157f74a9f2c7d7c3	APPROVED	2021-11-01 01:43:58.660000	2021-11-01 02:24:26.400000	2427.74	39.0	103	1	100	2021-11-03 15:51:48.044000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Caucasian	English	United States	Female	No	
121	2021-10-31 20:11:43	2021-10-31 20:29:29	IP Address	75.176.104.237	100	1066	True	2021-10-31 20:29:30	R_PZn1qk0hWtvbt05					33.973907470703125	-81.2344970703125	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The probability of granting access is higher for caucasians.	Very biased	The probability of granting access is higher for caucasians.	Completely unusable	The probability of granting access is higher for caucasians.	Neither fair nor unfair	The possibility of granting access is the same. 	Neither biased nor unbiased	The possibility of granting access is the same. 	Mostly ununsable	The percentage of mistakes is high.	No		Probably model Y		Definitely model X		Probably model Y		Probably model Y		Probably ${e://Field/pref_model}		Probably model Z		Probably ${e://Field/pref_model}			High		Then hospital staff would be delayed.		Moderate		Information may be shared, but lives are not necessarily at risk.		Moderate		With a backup in place, doctors would still be able to help society, aka the patients. 		Moderate		The exposure of private information can be negativity impactful to society, aka the patient. 		No			Yes			Yes			Advantaged		Bachelor	Others	610957fa1a38ec442cb5e5b2	617f102667bd25701aa8461e	617f4cd9791252fc974fb831	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f4cd9791252fc974fb831	APPROVED	2021-11-01 02:11:40.460000	2021-11-01 02:29:52.762000	1092.302	26.0	38	0	100	2021-11-03 16:25:01.329000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	No	
122	2021-10-31 19:20:58	2021-10-31 20:32:38	IP Address	45.24.143.202	100	4300	True	2021-10-31 20:32:38	R_2aY1gJ0szjuJCjN					34.84930419921875	-82.40409851074217	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Because the mistakes were even 	Neither biased nor unbiased.	Can’t tell by the info given	Mostly useful	To help predict who will need icu beds	Mildly unfair	Because the mistakes are higher for one group 	Neither biased nor unbiased	Can’t tell from info given	Neither useful nor unusable	Useful for Caucasian but not others	No		Probably model X	Because the mistakes are even	Models X and Y are equally biased	Can’t tell from info given	Probably model X	Seems like it is more correct 	Probably model X	Because it seems more correct	Probably model Z	They should predict equally who needs icu and deny equally	Both ${e://Field/pref_model} and Z are equally biased	Can really tell from info given	Probably model Z	It equally made the error	Moderate			I assume they can still get icu support even if it not predicted 	Moderate			I would assume they could still get a bed if needed	Low			If beds are available then it doesn’t matter. Society would be the families, hospitals and employees  	Moderate			Maybe they will not staff the icu properly if they are not expecting patients. Effects patients and hospitals and employees	No			No			No			Advantaged			Bachelor	Administrative Staff	614f5734ca61d17940ac78b3	617f102667bd25701aa8461e	617f40f2ccff4a1c6593a296	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	617f40f2ccff4a1c6593a296	APPROVED	2021-11-01 01:20:55.059000	2021-11-01 02:32:41.544000	4306.485	54.0	96	1	100	2021-11-03 15:34:51.120000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	No	
123	2021-10-31 20:08:52	2021-10-31 20:35:40	IP Address	47.40.134.139	100	1607	True	2021-10-31 20:35:40	R_2OMFLCbkqNAtx5Q					42.81230163574219	-86.14209747314453	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The likelihood of the model mistaking granted ICU supports is the same for both caucasian and non-caucasian.	Neither biased nor unbiased.	Bias would come from an outcome resulting in unnecessary or detrimental effects to one of the groups. This is not the case.	Mostly useful	While the mistake potential is high for both groups, there is a fairness of the model due to having the same accuracy for both groups.	Mildly unfair	It does not proportionately mistake non-caucasian and caucasian patients	Acceptably biased	Clearly there are data points missing about non-caucasian patients that skew it to make more mistakes when projecting ICU requirements	Mostly ununsable	Too many people being admitted into ICU when not needed, therefore too many patients not being admitted to ICU and then suffering deteriorating health unnecessarily.	No		Definitely model X	No matter what the model predicts, the accuracy being the same for both groups being equal is my determination in it being fair	Probably model Y	Model Y is more bias because it systemically creates more mistakes in non-caucasian patients	Probably model X	Model X is more useful, because it keeps ICU more efficient and effective	Definitely model X	No one group is being more or less incorrectly admitted to the ICU	Definitely model Z	It is more important for people who need ICU care to receive it, and in this case, the accuracy % for denied ICU support is the same for both groups. It would be unfair if one group was denied ICU care when needed more than the other group.	Both ${e://Field/pref_model} and Z are equally biased	Both models are biased, just in an inverse way.	Definitely model Z	It is better if the biased modeling is overly cautious (i.e. puts more people in the ICU than necessary) Otherwise one group would be unfairly denied access to the ICU based on ethnicity.	Moderate			If medical support was not necessary, then they were always safe. However, there could be financial implications from being admitted to ICU.	High			This could be highly detrimental to your health, up to and including death, if you are not admitted to ICU when in fact it was necessary.	Moderate			This may take up an ICU bed that someone who actually needs it could use.	High			The individual not being admitted to ICU could die, and the friends and family of this individual would experience emotional trauma and potentially financial hardship.	No			No			Yes			Advantaged			Bachelor	Administrative Staff	6172cf92f2ea6613f717abb6	617f102667bd25701aa8461e	617f4c2cb41382b76ad66c59	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	617f4c2cb41382b76ad66c59	APPROVED	2021-11-01 02:08:50.761000	2021-11-01 02:35:43.724000	1612.963	27.0	140	0	100	2021-11-03 15:49:13.066000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
124	2021-10-31 20:25:20	2021-10-31 20:38:44	IP Address	69.109.188.202	100	804	True	2021-10-31 20:38:45	R_1kGq4iIXFccdafQ					32.653900146484375	-116.977294921875	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 1	Neither fair nor unfair	It grants the same results but different access	Acceptably biased	It has a different access rate 	Mostly unusable	It shows bias or skewed data	Neither fair nor unfair	It represents the ratios correctly	Acceptably biased	It is biased due to the same percentages changing in the next graph	Neither useful nor unusable	The graph could be useful for future reference 	Yes	The first graph was more fair than the second	Probably model Y	It has the same probability to grant access	Probably model X	It shows significant bias	Probably model Y	Can correctly depict unskewed data	Probably model Y	It has the more fair data	Probably ${e://Field/pref_model}	Has equal forms of access	Probably model Z	Shows how the ratios are different	Probably ${e://Field/pref_model}	It is the more fair choice		Moderate		Data is often important to access in emergencies to help with the complications		High		Individuals may not always be the best humans and may use confidential information to bring others harm		Moderate		It may or may not have an impact on society depending on the individual		High		Allows for theft or access of sensitive information		Yes			Yes			Yes			Advantaged		Secondary Education	Business	61605437b994ff2aafa9588a	617f10dcb9390b6c13530629	617f500cf1d024b4dfecf514	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f500cf1d024b4dfecf514	APPROVED	2021-11-01 02:25:18.180000	2021-11-01 02:38:47.446000	809.266	19.0	66	1	99	2021-11-03 14:48:57.334000	19AE28A9	United States	United States	Due to start a new job within the next month	Non-Caucasian	Spanish	United States	Male	Yes	
125	2021-10-31 20:27:29	2021-10-31 20:53:08	IP Address	64.189.246.188	100	1539	True	2021-10-31 20:53:09	R_2tnSBNsrafO0xjU					33.208099365234375	-97.13849639892578	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The percentages for the mistakes only among allowed payments seems high.	Acceptably biased	The graphs that were created were most likely biased in some way because bias is inherently everywhere.	Mostly unusable	The information provided was not very useful because of the high percentages pertaining to the mistakes.	Mildly unfair	High percentages for mistakes.	Mildly unbiased	The initial percentages were completely even.	Mostly ununsable	The high percentage among the mistakes.	Yes	I might have analyzed the mistake percentages differently because they were both relatively high.	Probably model Y	Because the probability of allowing payments for both caucasian and non-caucasian people is even.	Probably model Y	Even ratio.	Models X and Y are equally useful	They both show different information that is valuable to the research being done currently.	Probably model Y	Despite having an even ratio for the initial graph, the graph below illustrates an interesting question of whether bias exists beyond initial payments.	Probably model Z	The ratio is even for the mistakes only among denied payments graph.	Probably model Z	There is an increased likelihood that non-caucasians will have allowed payment.	Probably model Z	Even ratio for the second graph.			Moderate	They might not have any other option concerning having a place to live.			Moderate	This allows them to have a place to live.			Low	Considering the other people who live in the renters complex, I would say the impacts are low because changes to the environment would be minimal.			High	A fraudulent payment might allow for a renter with a tricky past to live around others potentially causing danger. Others living nearby would be considered with society.			Yes			No			Yes			Disadvantaged	Bachelor	Others	60fd048315fb2282c4d5a150	617f10dcb9390b6c13530629	617f504fdc29e29c02e8ac32	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	617f504fdc29e29c02e8ac32	APPROVED	2021-11-01 02:26:27.335000	2021-11-01 02:53:19.055000	1611.72	18.0	46	0	100	2021-11-02 03:38:40.916000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	English	United States	Female	Yes	
126	2021-10-31 20:29:57	2021-10-31 20:58:50	IP Address	107.117.172.12	100	1733	True	2021-10-31 20:58:51	R_zTgLB41gGj8rE8p					38.577392578125	-90.6708984375	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Predicts less non Caucasian will need icu. That's puts non white communities in danger	Very unbiased	Based off race and in favor of Caucasians	Mostly unusable	Because it's dangerous to the non white community	Very unfair	Large margin of people that didn't need to be admitted and therefore taking up beds for people who truly need it	Very unbiased	Admits Caucasians to ICU at a higher ratio of mistake, taking up beds for people who need them	Mostly ununsable	Not accurate or fair	No		Models X and Y are equally fair	Both are too inaccurate to rely on	Probably model X	Shows higher concern for Caucasians	Models X and Y are equally useful	Neither of them are a good system	Neither model X nor model Y	They are not fair at all	Both ${e://Field/pref_model} and Z are equally fair	Neither is fair too much inaccuracies	Both ${e://Field/pref_model} and Z are equally biased	They are both unfair to a side	Neither ${e://Field/pref_model} nor model Z	They have too high of am error and bias margin	Moderate			Could cause financial hardship if not covered by insurence	High			Could cause death or permanent disability	Low			Average people may pay more in taxes	Moderate			Wrongful deaths may cause turbulence among citizens and a distrust in the system	Yes			No			Yes			Disadvantaged			Secondary Education	Business	610843cf06ad53e03e0c66fb	617f102667bd25701aa8461e	617f510d7266248532d9a9b8	minority	outcome-fpr	icu	True		model X	top								Disadvantaged	617f510d7266248532d9a9b8	APPROVED	2021-11-01 02:29:54.998000	2021-11-01 02:58:54.719000	1739.721	27.0	44	0	100	2021-11-03 17:01:06.624000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
127	2021-10-31 20:43:44	2021-10-31 21:23:09	IP Address	174.197.6.6	100	2365	True	2021-10-31 21:23:10	R_2D5hqGnZPvvDUCf					32.779693603515625	-96.8022003173828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The model X is fair because it is based off of statistics from previous data, there is no way it could be unbiased. The mistakes graphs are each 35%, and even if the model X allows more payments from non-caucasians that because the group had payments that were less fraudulent than caucasians.	Very unbiased	If the model X is created using data, then the numbers do not lie. It is simply analyzing data that was presented so it cannot be biased.	Very useful	If both parties are losing money due to fraudulent payments then model X can effectively prevent these fraudulent payments so nobody loses money. And even if it makes a mistake those can be remedied. 	Very fair	Yes the graphs are similar in terms of allowing payments for both groups. 	Very unbiased	Model Y is unbiased because it is going off data to prevent fraudulent payments. If it makes a mistake for a certain group, that doesn't necessarily mean it is biased.	Very useful	Because it has the same rate of allowing payments, model Y is useful! 	Yes	I would change the fairness rating because model Y seems to be more accurate and forgiving than model X.	Probably model Y	Model Y is more fair because it has smaller gaps in both graphs than model X	Models X and Y are equally biased	Model X and model Y are both going off of data so they cannot be biased in my opinion.	Probably model Y	The smaller gaps in errors and allowances makes model Y more useful than model X.	Probably model Y	The model Y seems to be more accurate than model X.	Probably model Z	Model Z has mistakes equally for caucasians and non caucasians.	Both ${e://Field/pref_model} and Z are equally biased	They are both going off of data, and in my opinion, cannot be biased. 	Probably model Z	Model Z is better because it is more accurate in terms of mistakes in payments being denied.			High	Some renters may have multiple bills to pay in the beginning of the month so a denied payment may risk the renter of having other money pulled out, which will make them short on rent.			Moderate	The fraudulent payments only impact the landlord so it does not actually impact the renter. It is moderate though because they can get caught and denied service.			Moderate	Society would be moderately impacted because if renters are being mistakenly denied their rent, they can face major consequences that can lead to late fees or evictions. I considered society to be the American public because many of them are renters.			Low	I considered society to be the American public because many of them are renters. Society will only be impacted in a low manner because fraudulent payments do not actively affect society.			No			Yes			Yes			Disadvantaged	Bachelor	Healthcare Practitioner	6111432635ee221fd14640db	617f10dcb9390b6c13530629	617f543e44fa94361cbf2b38	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	617f543e44fa94361cbf2b38	APPROVED	2021-11-01 02:43:36.063000	2021-11-01 03:23:13.001000	2376.938	21.0	77	0	100	2021-11-03 00:38:21.961000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	Yes	
128	2021-10-31 21:00:10	2021-10-31 21:29:05	IP Address	172.56.16.94	100	1735	True	2021-10-31 21:29:05	R_1LcEQHPt0QVAJvp					33.98890686035156	-118.15299987792967	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Very unfair	Why is model x more critical of Caucasian’s payment vs non- Caucasian’s if the rates of mistakes only among payments is identical ?	Very biased	Bias towards non- whites	Completely unusable	Model x isn’t reflective of the data extrapolated from model y.	Mildly unfair	Race shouldn’t be a indicator of fraudulent payments	Very biased	Bias towards non whites again	Neither useful nor unusable	I don’t think this can be used	No		Definitely model X	Same rates for probability of allowing payments 	Definitely model Y	Payments allowed more likely for non whites despite higher percentages of fraudulent payments 	Models X and Y are equally useful	Both useless 	Definitely model X	Still not useful 	Definitely model Z	Allowed everyone to who can pay to buy	Definitely ${e://Field/pref_model}	 More allowed payments received less fraudulent payments 	Probably model Z	Ethics			Low	Other renters can be found. You still own property			High	You lose value of property and lose the commodity			High	Homes will be hard to acquired and people will be put on the streets			Low	Money isn’t important			Yes			Yes			Yes			Disadvantaged	Secondary Education	Others	61077b735c86da4ffba9a34f	617f10dcb9390b6c13530629	617f5835a5bb2659792f36c6	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	617f5835a5bb2659792f36c6	APPROVED	2021-11-01 03:00:07.443000	2021-11-01 03:29:11.320000	1743.877	27.0	43	1	99	2021-11-03 02:48:43.244000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Male	Yes	
129	2021-10-31 20:56:26	2021-10-31 21:37:58	IP Address	98.22.105.193	100	2491	True	2021-10-31 21:37:59	R_DOHBz0Z17tKWRZD					29.837799072265625	-82.61289978027342	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	The accuracy of model X cannot be determined from this data, so I cannot determine whether model X is fair.	Very unbiased	The model accepted Caucasian people and Non-Caucasian people at a significantly different rate, but I don't know what the sample of people look like. There could be more Caucasian or Non-Caucasian people and more people who are or are not doctors between those groups. The fact that the false acceptance rate is the same suggests that the algorithm is unbiased with regards to this difference towards those that were accepted.	Completely unusable	This model also has an unacceptably large false acceptance rate. Implementing it would be a security disaster.	Neither fair nor unfair	The presence of mistakes implies that there are doctors and people that are not doctors in the sample. I do not know the proportion of either in the group or the success rate so I cannot determine whether the model is fair.	Acceptably biased	It appears that a Caucasian person is twice as likely to mistakenly be granted access compared to a Non-Caucasian person. I think that even a 24% of granting instant access to someone who isn't a doctor is unacceptable and that the model should be discarded. The bias is acceptable since the model shouldn't be put into place anyway, and it should be investigated.	Completely unusable	The model should never grant unauthorized permission to someone who isn't a doctor. The expected percentage of failure should be miniscule. This model is entirely unusable.	No		Models X and Y are equally fair	I would prefer another option here, but since I cannot determine whether either one is fair or not, they are the same in that way.	Probably model Y	The rate of false acceptance is different for only model Y.	Models X and Y are equally useful	They are both a horrible idea to use.	Neither model X nor model Y	Fairness and bias aside, the false positive rate is unacceptable for both models. They are both useless.	Both ${e://Field/pref_model} and Z are equally fair	I cannot determine whether either model is fair, because I do not know anything about the sample.	Probably ${e://Field/pref_model}	As far as false rejection is concerned, model Y appears to be biased against Non-Caucasian people. I don't know what this graph would look like for false acceptance for model Z, but in this regard model Y appears to be more biased.	Neither ${e://Field/pref_model} nor model Z	I do not know the false acceptance rates for model Z. False denial is less of an issue for security, so it's difficult to determine how useful model Z is. Since model Y is unusable and I cannot determine the usefulness of model Z, I will opt to choose neither model here.		Low		Hospital staff that are rejected by the system can just type in their username and password. It is a mild inconvenience.		Low		From the perspective of an individual, being accidentally let into a hospital system doesn't change someone's life by itself.		Low		I cannot imagine any societal impact if a doctor is inconvenienced and has to manually enter a username and password.		High		If there is a significant chance that some unauthorized can get in, then there is a chance that person has malicious intent. The person can feasibly steal all medical records they can find, which already has drastic implications for those whose privacy was violated. If such a system exists, I find it likely that they could even bring the system to a halt for ransom with a sort of malware, which will have incredible impact for anyone in need of the hospital.		No			No			Yes			Advantaged		Secondary Education	Others	6028809b7057061e0ede2cb6	617f10dcb9390b6c13530629	617f5755a19ed10fbe878095	minority	outcome-fpr	frauth	False		model Y	bottom								Advantaged	617f5755a19ed10fbe878095	APPROVED	2021-11-01 02:56:24.709000	2021-11-01 03:38:00.760000	2496.051	20.0	139	0	100	2021-11-03 02:43:27.765000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Male	Yes	
130	2021-10-31 21:30:01	2021-10-31 21:46:39	IP Address	75.7.15.83	100	998	True	2021-10-31 21:46:40	R_1cVd9mG0qKJgJ0R					36.12199401855469	-95.91829681396483	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	In the top graph it’s highly unfair and in the bottom graph it granted access to everyone. 	Neither biased nor unbiased.	A machine can’t be biased. 	Mostly unusable	Until it’s accurate it’s going to keep making mistakes 	Mildly unfair	It mistakenly grants more access to non caucasions	Neither biased nor unbiased	Machines cannot be biased	Mostly ununsable	Cannot be used until it’s accurate	No		Probably model Y	Y granted access to the same individuals and the margin of error was low on the granted acces mistakes	Models X and Y are equally biased	Machines can’t be biased 	Probably model Y	It correctly granted access to both Caucasian and non caucasions 	Neither model X nor model Y	No machine is accurate	Definitely ${e://Field/pref_model}	More accurate 	Definitely model Z	Comparable	Probably ${e://Field/pref_model}	I don’t know really this is confusing 		High		Doctors time are sparse it could save them time 		High		I don’t want a non doctor access my med records 		Low		Society is people non medical outside of medical 		High		Society is patients 		No			Yes			Yes			Advantaged		Master	Business	6161d8d8b181d894ff32d83e	617f10dcb9390b6c13530629	617f5f301c0010bd48e95edb	majority	outcome-fpr	frauth	True		model Y	bottom								Advantaged	617f5f301c0010bd48e95edb	APPROVED	2021-11-01 03:29:58.893000	2021-11-01 03:46:47.355000	1008.462	38.0	96	1	100	2021-11-03 13:50:05.129000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	Yes	
131	2021-10-31 21:31:33	2021-10-31 22:06:27	IP Address	70.225.6.146	100	2093	True	2021-10-31 22:06:28	R_2tcjowQ8CjPvf8q					33.69920349121094	-84.747802734375	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	I think its neither fair or unfair because the mistakes are both the same percentage.	Acceptably biased	its biased because the caucasians have a much higher chance of needing ICU support than non caucasians.	Mostly useful	its useful because it shows the mistakes being made.	Acceptably fair	they both have a close probability.	Acceptably biased	they both have the same probability rate but the caucasian chart has a 45% mistake rate and the non-caucasian one has a 24% rate.	Mostly useful	It shows which how many patients are usually mistaken.	Yes	I would have said it more unfair because it was more of a probability of mistakes for the caucasians than the non caucasians.	Probably model Y	model Y because both categories have the same percentage of being granted icu support.	Probably model X	the caucasian category has a higher probability of being granted icu support than the non caucasians.	Probably model Y	since both categories are given a fair chance at ICU support, the result seems more accurate.	Definitely model Y	same as the previous answer, it seems like the result would be more accurate since everyone is given a chance.	Probably ${e://Field/pref_model}	they have an equal amount of people being given icu support.	Probably model Z	the caucasian category is given a higher chance of icu support	Probably model Z	even though they give a higher chance to one category, they both have the same percentage of mistakes.	Moderate			i believe that it’s moderate because even you are wrong, they wont lose their life.	High			if someone actually need icu support and you mistakenly deny it, it could cost them their life.	Moderate			I think it would be moderate because either they wouldn’t have to risk a patient losing their life and i consider the doctors/people in charge of the patient as part of society.	High			It would be very harmful to that hospital to know you mistakenly denied someone who needed support. I believe any doctor or nurse would be apart of this aswell.	Yes			No			Yes			Disadvantaged			Primary Education	Others	61522cca7870d26be8d1ace7	617f10dcb9390b6c13530629	617f5f907851a2d7483bdb04	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	617f5f907851a2d7483bdb04	APPROVED	2021-11-01 03:31:31.343000	2021-11-01 04:06:30.623000	2099.28	21.0	102	1	100	2021-11-03 00:53:57.266000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
132	2021-10-31 21:27:22	2021-10-31 22:14:11	IP Address	104.33.174.75	100	2809	True	2021-10-31 22:14:11	R_070WiSKJ0X3HhSx					34.413299560546875	-118.09170532226562	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	They're saying that both groups have a different probability of paying because of the fraudulent detection when both groups have the same mistakes only among allowed payments.	Very biased	Very biased because they're accepting fraudulent payments in the caucasian group and not taking it into account in the probability of payment.	Mostly unusable	It's mostly unusable because of the first graph in the model.	Neither fair nor unfair	I think it's neither fair nor unfair since it's saying caucasian would mostly cause mistakes only among allowed payments when there's a 50/50 chance anyone can do it but also in this case there is a chance of it happening so I don't think it's too unfair.	Neither biased nor unbiased	It's a bit biased because it's caging a race.	Mostly useful	It's mostly useful because of the same percentages each group has in the probability of allowing payment.	Yes	The way it's unbiased/biased or acceptable/unacceptable because it wasn't that acceptable or unbiased.	Probably model Y	Mostly because of the percentages in the first graph of model Y but it's still not too fair because it doesn't take into account the mistakes only among allowed payments in the second graph towards the first graph.	Definitely model X	Model X is more biased because the probability of payment is still high even if there's fraudulent payment when the non-caucasion grouping has the same percentage but has less probability of payment.	Probably model Y	Model Y is a bit more useful but still not too useful because of the percentages 	Probably model Y	It seems to have a little more acceptable information but still not too useful.	Probably model Z	Model Z has the same percentage in mistakes only among denied payments but the probability of allowing payment is different and shows a lower percentage to the non-caucasian group and a higher one for the caucasian group.	Both ${e://Field/pref_model} and Z are equally biased	They're both biased because of the same way too different percentages within both graphs.	Probably ${e://Field/pref_model}	It shows the same probability in payment but is still hesitant because of the different percentages in the second graph.			High	High because it can be given huge biases within both groups and it can make the tenant be more hard on the renter.			High	It allows the view of payments to be different for the tenants because they can view the group that allows fraudulent payments as a group that continues paying with no issue but sees the other group as a problem.			High	For the non-caucasian group, it could be very impactful because of the stereotypes and biased opinions on those groups.			High	For the non-caucasian group when it's allowed mistakenly they can still be caught and seen in the worse light and even face consequences that the caucasian group might not face.			Yes			No			Yes			Disadvantaged	Primary Education	Others	616113bd199b4713b81ebc51	617f10dcb9390b6c13530629	617f5e95143f3356f1cad8c5	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	617f5e95143f3356f1cad8c5	APPROVED	2021-11-01 03:27:20.153000	2021-11-01 04:14:14.844000	2814.691	20.0	104	1	100	2021-11-03 02:53:46.275000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	Yes	
133	2021-10-31 21:46:22	2021-10-31 22:21:08	IP Address	76.185.120.231	100	2086	True	2021-10-31 22:21:09	R_3J4PAcDN76BCjPe					32.80079650878906	-96.78839874267578	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	It allows more payments from whites than non whites	Very biased	In that it allows many more payments rom whites than non whites.	Mostly unusable	It is very biased and would been seen as a racist algorithm	Mildly unfair	It allowed a disproportionately high percent of white payments verse non white payments. 	Acceptably biased	It allows more bad payments from whites than non whites.	Neither useful nor unusable	I am not sure, I would have to compare it to other models to make a better determination. 	No		Probably model Y	It allows an equal proportion of payments. 	Probably model X	The allowed payments seems to be a bigger metric of bias in my mind	Probably model Y	It allows an equal payment and may be easier to adjust the rejected portion	Probably model Y	Model Y seems to be a better representation and carries less bias. 	Probably ${e://Field/pref_model}	I give more weight to equality in allowing payment/ 	Probably model Z	I think allowing less non whites to make payments is worse than a disproportion in rejecting payments	Probably ${e://Field/pref_model}	I like equity in the probability of allowing payment. 			Moderate	It seems like an easy thing to rectify and it doesn not really affect yet it could be troublesome. 			Moderate	It could be a good positive net benefit for the individual 			High	I think as society as whole, meaning what we as the US view as equitable, if there is bias in it, there can be generational inequality as a result. 			Low	I think this mostly effects the renters class of society and this would have a low to marginal effect on their economic bottom line			No			Yes			Yes			Advantaged	Bachelor	Engineering and Technology	579fbb4a6f13c2000174aafc	617f102667bd25701aa8461e	617f6306d70c171f8b4ac5a4	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f6306d70c171f8b4ac5a4	APPROVED	2021-11-01 03:46:20.685000	2021-11-01 04:21:11.974000	2091.289	28.0	625	1	100	2021-11-03 15:06:01.625000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Male	DATA EXPIRED	
134	2021-10-31 22:08:29	2021-10-31 22:36:29	IP Address	70.118.62.98	100	1680	True	2021-10-31 22:36:30	R_3GrJS6iQyvH24Iq					30.022506713867188	-98.11380004882812	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 1	Mildly unfair	Anyone trying to enter should be able to have their face easily recognized. 	Mildly unbiased	From the graph Model X seems biased.	Mostly useful	Even though it is unfair it's useful to inform someone how the machinery is working and how accurate it is.	Mildly unfair	It's not exactly unfair however, the system is having trouble filtering out faces that don't work for the medical department.	Neither biased nor unbiased	I don't think it is biased in the first graph it's even and in the second graph it's having trouble distinguishing between medical and non-medical workers.	Mostly useful	The graphs can insight for people that want to know how accurate the system is.	Yes	For the first set of graphs, I honestly didn't fully understand what the questions were trying to ask me and what the graph was about. So my answers may have seemed off however, on the second graph I understood and gave meaningful answers.	Probably model X	The percentages are related to people with authorized medical access that are meant to be there.	Probably model Y	The percentages show people with no medical access getting in. Which is unfair. 	Models X and Y are equally useful	They both serve a purpose in educating about the system.	Neither model X nor model Y	Both graphs show equally important info that is useful to understand the system.	Both ${e://Field/pref_model} and Z are equally fair	Both models show something that is unequal.	Both ${e://Field/pref_model} and Z are equally biased	All the models have very similar percentage patterns.	Neither ${e://Field/pref_model} nor model Z	Again, both models serve a purpose and they don't have that many variations.		High		If an unauthorized person comes into a hospital they could do anything: harm patients, look through records, steal, etc. The consequences could be very serious.		Moderate		An unauthorized user may not reach me as an individual but still would make me scared for others.		Moderate		I see society as the rest of the hospital population. It is moderate because it would only take away someone's trust by using someone who IS authorized to show proof.		Moderate		I see society as the rest of the hospital population. Non-medical personal being authorized is a mistake because they don't have the experience or trust. You can't let just anyone into a hospital.		Yes			Yes			Yes			Advantaged		Master	Business	6108915d3132c79e6c055f04	617f10dcb9390b6c13530629	617f6830d98d53b0ff604452	majority	outcome-fpr	frauth	True		model X	top								Advantaged	617f6830d98d53b0ff604452	APPROVED	2021-11-01 04:08:26.249000	2021-11-01 04:36:35.229000	1688.98	55.0	41	1	99	2021-11-03 14:34:58.241000	19AE28A9	United States	United States	Due to start a new job within the next month	Non-Caucasian	English	United States	Female	No	
135	2021-10-31 21:53:24	2021-10-31 23:13:24	IP Address	68.52.170.114	100	4799	True	2021-10-31 23:13:25	R_24iXcirSDGnfXT9					36.17689514160156	-86.73390197753906	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Various reasons make it known that race can affect the health of a person. To try and factor it is is a fair assumption.	Acceptably biased	It’s statistically proven the health of non Caucasian people is significantly different(in decline) from that of Caucasian people. Yet the factors that play into those statistics may not apply in the same way for this case and can’t be entirely factored in.	Mostly unusable	Model Y takes into factor race when the real report shows otherwise. While you can’t predict the correct number of people, it’s still highlights that race could affect the results and researchers can keep that in mind.	Mildly unfair	Model Y reports that no matter the race of the patient they will have 53% chance of needing ICU treatment. Race however does affect the real % for various reasons that not taking into account these reasons neglects the reality of them. 	Very unbiased	Model Y shows the ideal outcome that race shouldn’t affect the need for ICU treatment. It would be dependent on the health of the patient.	Mostly ununsable	While it doesn’t take race into consideration it still shows that other factors may or may not have an affect. The patient is close to being 50/50 likely to need ICU treatment. It raises the question as to whether other factors do or don’t affect health of hospitalized patients.	No		Probably model X	It’s realistically shows what could be a factor in a patient’s health even if those factors are innately unfair.	Probably model X	Each model assumes that people are either equal or not in health based on race. Model X uses outside information to make an educated guess and Model Y neglects the use of that information.	Probably model X	Model X is closest to the real results.	Definitely model X	In my biased opinion Model X is the most relevant to the real world.	Probably model Z	Every patients has the same chance of needing ICU treatment, making them all equal.	Probably ${e://Field/pref_model}	Not only takes race into consideration but also puts a value on that factor that says Caucasian people are less likely to need ICU treatment.	Probably model Z	It’s closest to the real report, but from my background knowledge I would lean on choosing Model X if I think long enough.	Low			The real outcome would be that the patient doesn’t need ICU treatment. This would positively reflect what the health is and would be much appreciated.	High			The real result would mean that a patient needs assistance to live. This will not only affect them when they are receiving treatment but also after that.	Low			"Society: family members, coworkers, hospital staff, friends

They would welcome the news that a person doesn’t need ICU treatment, but beyond that they will not have to deal with any consequences."	High			"Society: family members, coworkers, hospital staff, friends

Society with have to be a support system to patients, whether they want to or not. "	No			Yes			Yes			Disadvantaged			Secondary Education	Others	610098ade3f5bd73650c747c	617f10dcb9390b6c13530629	617f64aeb31528de7625ac28	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Disadvantaged	617f64aeb31528de7625ac28	APPROVED	2021-11-01 03:53:20.378000	2021-11-01 05:13:27.659000	4807.281	20.0	96	0	100	2021-11-03 00:03:36.542000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	Yes	
136	2021-10-31 23:21:37	2021-10-31 23:36:32	IP Address	50.27.40.86	100	895	True	2021-10-31 23:36:33	R_1Ia7vrGlHHhoXrG					33.57060241699219	-101.86620330810545	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 2	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Mildly unfair	Mistake percentages seem pretty high. 	Very biased	It may be a little unbiased towards Caucasian’s but it seems more biased towards other raises since it accumulates them together and they don’t get a proper representation. 	Mostly unusable	It doesn’t have proper representation in races.  	Very unfair	Caucasian mistakes are still pretty high. 	Very biased	It doesn’t have proper representation and Caucasian mistake percentage is a lot higher than other races. 	Mostly ununsable	It’s accuracy doesn’t seem right. 	Yes	The fairness. It seemed that Caucasian got the most mistakes even though other races were combined. 	Probably model X	Mistakes are equal in both races. 	Models X and Y are equally biased	Races are grouped instead of spread out for better and more accurate information. 	Probably model X	Mistakes are equal for both races. 	Probably model X	Lower mistake percentages. 	Probably ${e://Field/pref_model}	Same percentage of mistakes. One doesn’t seem lesser than the other. 	Probably model Z	Higher mistake percentages. 	Probably ${e://Field/pref_model}	More accurate information. 	High			Mistakes need to be realistically almost nonexistent in ICU situation. 	High			Mistaken predictions need to be accurate and low. 	High			Required ICU predictions are very helpful to staff and patients. Accurate info is needed & crucial. 	High			Not having a mistake percentage can harm patients if you don’t have accurate information.	No			Yes			Yes			Advantaged			Secondary Education	Engineering and Technology	60fd0b299f6ddde5fda27a9e	617f10dcb9390b6c13530629	617f795dfc361d3ac1c5d97c	minority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	617f795dfc361d3ac1c5d97c	APPROVED	2021-11-01 05:21:35.360000	2021-11-01 05:36:36.964000	901.604	25.0	101	0	100	2021-11-03 14:16:18.389000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	Yes	
137	2021-10-31 23:29:46	2021-10-31 23:44:04	IP Address	40.137.48.42	100	857	True	2021-10-31 23:44:04	R_2sRLcsFpwkQgIK2					33.4678955078125	-111.71549987792967	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Has an even amount of mistakes	Mildly unbiased	Has an even amount of mistakes	Mostly useful	Shows no bias	Mildly unfair	Appears to be a bias towards non caucasian	Acceptably biased	Non Caucasian has a higher probability of being fraudulent 	Neither useful nor unusable	No way of knowing if there is a true bias	No		Probably model X	Does not a support a bias towards caucasians	Probably model Y	Has a higher probably of fraudulent payments for non caucasian	Probably model X	Less opportunity of a bias	Probably model X	Has clearer I manipulated data	Both ${e://Field/pref_model} and Z are equally fair	High and lows correlate on each graph to the percentage of people tested	Probably ${e://Field/pref_model}	Has a higher tested population of caucasian	Probably model Z	Has a fair and equal amount of race tested 			High	Questions their integrity and potentially creates a flag for them for no reason			High	Has a negative impact of the card holder			Moderate	Could put a bad taste in renters mouths if they have a bad experience 			Moderate	Affects only the parties involved			Yes			No			Yes			Disadvantaged	Bachelor	Others	6111cfbfb13669cd83689ef2	617f10dcb9390b6c13530629	617f7b451f6f31aaad022a22	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	617f7b451f6f31aaad022a22	APPROVED	2021-11-01 05:29:44.462000	2021-11-01 05:44:07.914000	863.452	24.0	131	3	99	2021-11-03 00:33:50.029000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	No	
138	2021-10-31 23:35:04	2021-10-31 23:48:01	IP Address	72.203.41.114	100	776	True	2021-10-31 23:48:01	R_2uyc01bj3i6bCUZ					36.002105712890625	-115.14700317382812	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	There's a 20% difference between caucasian and non-caucasian, it seems pretty lenient towards caucasians to get ICU 	Very biased	It is obvious there is a bias towards non-caucasians	Neither useful nor unusable	I think this information could be useful to many people but because of the mistakes it makes, it's not 100% useful.	Very fair	The predictability is the same percentage so I think this is fair.	Very unbiased	Because the predictability is the same, it doesn't seem biased at all	Mostly useful	I think it is mostly useful because mistakes are still made but the information should still be useful	No		Definitely model Y	The predictability is the same in Model Y and I think that's the basis of the entire modeling system	Definitely model X	It is definitely more biased towards caucasians	Probably model Y	I think both can be used for useful information but Y is more useful for equally predicting those for ICU	Definitely model Y	It feels more equal and fair	Definitely ${e://Field/pref_model}	The predictability is the same so it feels fair	Definitely model Z	A 20% difference is going to feel more biased, of course	Definitely ${e://Field/pref_model}	More equal	Moderate			It will take away a bed from someone that needs it more. However, if someone that doesn't need ICU is put into ICU, they would quickly get taken out of ICU once someone deems them healthy enough.	High			If someone needs ICU support but doesn't get it, their health will likely quickly deteriorate.	Moderate			Society is everyone else that could potentially end up in the hospital. An ICU bed could be taken away from somebody that needs it but it would be temporary	High			Society would be those that are very ill and need ICU support. Without it, they could die.	No			No			Yes			Disadvantaged			Secondary Education	Others	578c06326475d400015aa125	617f10dcb9390b6c13530629	617f7c8350fefb4faa647711	minority	outcome-fpr	icu	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	617f7c8350fefb4faa647711	APPROVED	2021-11-01 05:35:02.507000	2021-11-01 05:48:03.957000	781.45	28.0	94	0	100	2021-11-02 03:28:35.118000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Female	DATA EXPIRED	
139	2021-10-31 22:58:54	2021-10-31 23:52:41	IP Address	12.160.225.80	100	3227	True	2021-10-31 23:52:42	R_2pQv2OuKDiqGGNU					34.054397583007805	-118.2440948486328	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very fair	The probability of granting access to the medical device is the likelihood of someone's need.	Very unbiased	The someone's request to the medical device access is granted.	Very useful	As the medical device is used to satisfied someone's request with the access to the device.	Acceptably fair	As the probability of granting access is the same to each other.	Very unbiased	The device serves the needs of the all together.	Very useful	It is useful for all the request of the device access.	Yes	The left answer is not the same as to the right one. Because, the right one, as all the access are the same to each other.	Probably model Y	Because, all of them have the same access to the medical device as it should.	Probably model X	Their access to the medical device is not the same to each other.	Definitely model Y	They all get the same access to the device for it operations. 	Probably model Y	Model Y has the same access to the medical device for it operations.	Definitely ${e://Field/pref_model}	As compared to Z, it has the same access of denied personnel of the medical device.	Probably model Z	Their access to the medical device is not the same as to the Y.	Probably ${e://Field/pref_model}	Their access is the same and has a unique form to the medical device		Low		Without the username/ password, the authorized hospital staff will not get access to an individuals .		Low		It doesn't help the medical device to access the users.		High		To help the society to recognized in the authorized hospital staff device.		Low		It doesn't help the help the society to to be recognized in the in the device access		Yes			Yes			Yes			Advantaged		Secondary Education	Business	6175db12b41a9de303906935	617f10dcb9390b6c13530629	617f73f6fc180f7b3f918c62	minority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f73f6fc180f7b3f918c62	APPROVED	2021-11-01 04:58:47.049000	2021-11-01 05:53:26.510000	3279.461	31.0	98	2	97	2021-11-03 14:06:26.437000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Male	No	
140	2021-10-31 23:55:04	2021-11-01 00:04:59	IP Address	50.30.174.23	100	595	True	2021-11-01 00:05:00	R_30uEX6pZcjbL39S					34.42649841308594	-119.86309814453124	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	No discrimination	Mildly unbiased	No discrimination	Mostly useful	It works	Very unfair	Non white students are twice as likely to be mistakenly considered unsuccessful	Very biased	Racism	Mostly ununsable	Racism	No		Probably model X	Model X doesnt have higher likelihood of mistakes for non white students	Probably model Y	Racism	Probably model X	Because it is less biased	Probably model X	because of all the reasons from before	Definitely ${e://Field/pref_model}	It doesnt favor white people	Probably model Z	More mistakes for non white people	Definitely ${e://Field/pref_model}	Model X is not biased against non white people		Moderate		Someone could hack  the system		High		Hacking		Low		No harm is done		High		Hacking could lead to stolen money or use of private patient information 		Yes			No			Yes			Advantaged		Secondary Education	Education	610844f642f52bbd2a4cbdcf	617f10dcb9390b6c13530629	617f813046f0963307ba1b85	majority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	617f813046f0963307ba1b85	APPROVED	2021-11-01 05:54:59.622000	2021-11-01 06:05:02.674000	603.052	18.0	84	0	100	2021-11-03 00:31:16.218000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
141	2021-10-31 23:53:21	2021-11-01 00:07:07	IP Address	47.218.222.38	100	826	True	2021-11-01 00:07:08	R_1mOfkRWXi9jhc7A					30.54780578613281	-96.27149963378906	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	There is a higher chance of someone caucasian if ICU is required	Very biased	There shouldn't be a difference in race	Mostly unusable	Its a starting point but needs to be improved to use with human lives on the line	Acceptably fair	There is an equal amount regardless of race	Very biased	There are more mistaken admits for caucasians	Neither useful nor unusable	It requires just like Model X before it can be used with people's lives	No		Probably model Y	It accepts an equal amount of each race from the outset	Definitely model Y	It has more mistaken admits of one race	Models X and Y are equally useful	Both are equally useless. They can not be used in their current state	Neither model X nor model Y	If the data shows such obvious issues I would rather trust a human. They may also be biased but I would imagine to a lesser extent	Both ${e://Field/pref_model} and Z are equally fair	Both result in people dying. Each one results in some group having an advantage	Both ${e://Field/pref_model} and Z are equally biased	Both result in people dying, either from not being accepted outright or from individuals being accepted without need	Neither ${e://Field/pref_model} nor model Z	I would still prefer a human	High			Another individual could die if someone was mistakenly admitted 	High			The individual in question could die	High			One individual will be wasting resources that could be used to save another. Society in this case is everyone else who could be saved	Moderate			Society is hurt as the individual in question may die, but another's life is not being harmed	Yes			Yes			Yes			Advantaged			Secondary Education	Services Occupations	5e79393ad1dff84f7ccecb7b	617f10dcb9390b6c13530629	617f80c9b4a2630af424b023	minority	outcome-fpr	icu	True		model X	top								Advantaged	617f80c9b4a2630af424b023	APPROVED	2021-11-01 05:53:19.719000	2021-11-01 06:07:11.114000	831.395	23.0	131	0	100	2021-11-02 03:37:11.575000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	Spanish	United States	Male	Yes	
142	2021-10-31 23:16:07	2021-11-01 00:07:44	IP Address	47.6.168.35	100	3097	True	2021-11-01 00:07:45	R_2SkNlpLezkRr5jE					36.30009460449219	-119.12950134277344	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	The model allows more payments by Caucasians despite making the same amount of mistakes on both groups.	Very biased	Caucasians are favored.	Mostly useful	It allows a relatively low amount of fraudulent payments.	Acceptably fair	It doesn't consider race.	Very unbiased	No bias is shown.	Mostly useful	It seems to not allow to many fraudulent payments.	No		Definitely model Y	It does not consider race.	Definitely model X	It takes race into consideration.	Models X and Y are equally useful	They have similar failure rates.	Probably model Y	I would want to make the fair choice.	Definitely ${e://Field/pref_model}	Model z allows more caucasian than non caucasian payments.	Definitely model Z	It is biased in favor of caucasians.	Probably ${e://Field/pref_model}	I'd like to make the moral choice.			Moderate	They can likely make things right with little trouble.			Moderate	It's a loss of money but not the end of the world.			Low	The individual can likely fix this on their own. Their family may not even find out.			Moderate	Someone will likely have to pay for this. The landlord or whoever is to blame would be the society.			No			Yes			Yes			Advantaged	Secondary Education	Others	5adef850eb60400001539109	617f10dcb9390b6c13530629	617f7812f03f8bb8df23d9e6	minority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f7812f03f8bb8df23d9e6	APPROVED	2021-11-01 05:16:05.121000	2021-11-01 06:07:48.420000	3103.299	24.0	510	4	99	2021-11-02 03:29:59.924000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Male	DATA EXPIRED	
143	2021-10-31 23:18:21	2021-11-01 00:15:06	IP Address	184.53.16.238	100	3405	True	2021-11-01 00:15:06	R_2eRa0vZyRXVaOxb					39.95440673828125	-75.16570281982422	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	It’s fair enough with only a 24% difference.	Acceptably biased	It’s a 24% difference which isn’t a big deal with payments allowed. If it was mistakes allowed and their was a difference then it would matter more.	Mostly useful	The 24% doesn’t seem like it matters in the grand scheme of things.	Very unfair	21% difference in mistakes allowed in the non-Caucasian category	Very biased	It’s unacceptable to allow that many mistakes happen with the non-Caucasian category.	Completely unusable	It’s useless if it has that much of a disparity between non-Caucasian and Caucasian.	No		Probably model X	It’s not completely fair to the Caucasian category but it’s acceptable enough.	Definitely model Y	45% in mistakes allowed for non-Caucasians vs only 24% mistake with Caucasians.	Probably model X	Model X is probably useful because it shows less of a bias.	Neither model X nor model Y	The percentage difference between the both of them is too much to actually be useful.	Probably ${e://Field/pref_model}	I think the 23% of mistakes made is fair enough, allowing payments seems to matter more.	Both ${e://Field/pref_model} and Z are equally biased	They are both biased against the Caucasian group. With about a 20% difference in allowed and mistakes.	Neither ${e://Field/pref_model} nor model Z	I wouldn’t choose either of them because they are equally biased so it’s not a difficult choice.			High	If denying a payment leads to a late payment because of this mistake it can mean a lot. A lot of renters live paycheck to paycheck.			High	When you allow a fraudulent payment it can severely affect that person by loss of income and potentially effecting their credit score.			Moderate	If this mistake causes a renter to lose their apartment then society as in the community you live in would be affected by their homelessness. 			Low	Society being the community you live would be affected by fraudulent payments is low because I feel like this would the company that allows fraudulent payments happen would be jeopardized.			Yes			Yes			Yes			Disadvantaged	Secondary Education	Others	6107a4b7744b9b0ca7328cee	617f10dcb9390b6c13530629	617f7884575bb09f8ac4826d	majority	outcome-fpr	rent	True		model Y	bottom								Disadvantaged	617f7884575bb09f8ac4826d	APPROVED	2021-11-01 05:18:13.017000	2021-11-01 06:15:18.662000	3425.645	32.0	75	1	99	2021-11-03 00:19:48.877000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	English	United States	Female	No	
144	2021-10-31 23:32:22	2021-11-01 00:19:47	IP Address	75.83.213.42	100	2845	True	2021-11-01 00:19:48	R_3hoTK5IdcwN8Bgk					34.18559265136719	-119.21369934082033	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	the percentage is higher for the non-caucasians when granting access	Very unbiased	it was the same percentage in mistakes for caucasians and non-caucasians 	Mostly useful	the percentages are higher for the granting access and the percentage for mistakes is quite low, so the pros outweigh the cons	Acceptably fair	it granted access to caucasians and non-caucasians evenly, at 53%	Acceptably biased	it made more errors for non-caucasians 	Mostly ununsable	the percentages of errors is fairly high for such an important use, which is being able to access important information 	Yes	model y had a higher percentage of correctly gaining access versus the second model. I would say that perhaps the first model would be more useful for that reason	Probably model Y	the percentages for granting access are equal for caucasians and non-caucasians 	Probably model Y	the percentages for mistakes are significantly different between caucasians and non-caucasians	Probably model X	there is less bias and the percentages for granting access are decent 	Probably model X	even though the percentages for granting access should be higher there is no biases so the pros beat the cons 	Probably model Z	the percentages for mistakes among denied access are the same between caucasians non-caucasians 	Probably model Z	there is a significant difference between percentages in the mistakes among granted access 	Probably model Z	the percentages are lower for the mistakes among denied/granting access 		Low		the authorized hospital staff might just lose a few moments if the device fails to recognize them, typing in the username/password should not take that long 		Moderate		the individual's information is confidential so there might information that they do not want other individuals besides their authorized user to know		Low		the society could be the individual's family who is accompanying them. the authorized hospital staff can just input their username/password and lose a few seconds 		Low		the society can be a family member of the individual. only the individual's information would be there so there is not really a risk to the society 		Yes			Yes			Yes			Advantaged		Bachelor	Others	60fee3c356b1f3bddfe5c3ec	617f10dcb9390b6c13530629	617f7b7ac21b006bb9fae78d	majority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	617f7b7ac21b006bb9fae78d	APPROVED	2021-11-01 05:30:36.500000	2021-11-01 06:19:51.857000	2955.357	25.0	71	0	100	2021-11-03 14:20:01.834000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	Spanish	United States	Female	Yes	
145	2021-11-01 00:06:18	2021-11-01 00:28:45	IP Address	172.58.191.147	100	1347	True	2021-11-01 00:28:46	R_1rrk8EMXsL0WBmw					38.87319946289063	-77.11509704589844	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Because it shows that non-caucasians have a low probability of having their payment accepted.	Mildly unbiased	Low probability of non-caucasian payments being accepted.	Mostly unusable	It's a little biased.	Very fair	Because it is equal probability for caucasians and non-caucasians of the payment being aapproved.	Very unbiased	Because it should both ethnicities.	Very useful	Because it unbiased.	No		Definitely model Y	It's more equal in treatment.	Definitely model X	It's more biased against non-caucasians.	Definitely model Y	Not biased.	Definitely model Y	It's not biased.	Definitely model Z	It's more equal in probability.	Definitely ${e://Field/pref_model}	Higher percentage of non-caucasians being denied.	Probably model Z	Less biased in payment denial.			Moderate	Becuase they won't be able to pay their rent.			Moderate	It could mess with their reputation.			Low	Because it doesn't matter to them. The society can be a person standing on the side of the street who has no concern with this.			Low	Because it doesn't matter to the random person.			Yes			Yes			Yes			Advantaged	Primary Education	Services Occupations	5c1ac9a924e01a000144e64a	617f10dcb9390b6c13530629	617f83c9a76d5a7a71a0a783	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	617f83c9a76d5a7a71a0a783	APPROVED	2021-11-01 06:06:11.955000	2021-11-01 06:28:48.337000	1356.382	23.0	298	0	100	2021-11-03 01:42:18.929000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	No	
146	2021-11-01 00:17:48	2021-11-01 00:34:35	IP Address	63.155.12.183	100	1007	True	2021-11-01 00:34:36	R_2PBU2vNjtSkothN					44.064498901367195	-123.14610290527344	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair		Neither biased nor unbiased.		Neither useful nor unusable		Acceptably fair		Neither biased nor unbiased		Neither useful nor unusable		No		Probably model Y		Models X and Y are equally biased		Models X and Y are equally useful		Neither model X nor model Y		Probably model Z		Both ${e://Field/pref_model} and Z are equally biased		Neither ${e://Field/pref_model} nor model Z			Moderate		it’s a long process but no risk		High		don’t know a person’s intention 		Moderate		society involves anyone in this world. it is a long process but no risk		High		society involves anyone in this world. don’t know a person’s intention		No			Yes			Yes			Disadvantaged		Bachelor	Others	610af4dc507f06195b6d99fd	617f10dcb9390b6c13530629	617f867d80aa107a57722f79	minority	outcome-fpr	frauth	True		model Y	bottom								Disadvantaged	617f867d80aa107a57722f79	APPROVED	2021-11-01 06:17:42.508000	2021-11-01 06:34:40.156000	1017.648	22.0	82	0	100	2021-11-03 14:38:04.673000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
147	2021-11-01 00:19:18	2021-11-01 00:43:23	IP Address	173.16.1.244	100	1444	True	2021-11-01 00:43:23	R_OBCCStlrH5bC6lz					33.19219970703125	-111.53179931640624	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Amongst the few people it can has ease granting access to there is still a 35% chance that it will make a mistake.	Mildly unbiased	I mean it has a harder time with caucasians, but I don't think its biased.	Mostly unusable	It's going to be frustrating to use with that amount of mistakes.	Very unfair	for non-caucasians there is a 45% chance for it to make a mistake and a 24% chance for mistake for caucasians.	Acceptably biased	It makes more mistakes for non-caucasians which feels biased.	Mostly ununsable	That way too many mistakes for non-caucasians.	No		Probably model X	It feels like it has more acceptability rate than mistakes which will be more useful than model y.	Probably model Y	It has so many mistakes for non-caucasians it just feels biased to me. 	Probably model X	It will be less likely to make mistakes than model y	Probably model X	Less mistakes.	Probably ${e://Field/pref_model}	I'm getting a bit confused now but I think because its more likely to deny access mistakenly than the other?	Probably ${e://Field/pref_model}	Its mistakenly granting a lot of access to caucasians and mistakenly denying access to a lot of non-caucasians. 	Probably model Z	It feels like a safer bet than X?		Moderate		Since it still requires a username and password I feel like it would be annoying but at least they can still access the stuff and keep it protected.		High		Allowing access can be dangerous because there are people who abuse drugs and sell the drugs and that is very dangerous.		Moderate		As the society we are just the outsiders, it can delay them bringing life saving medicine which could potentially impact a lot of people or it could just make someone wait a while until they receive their medicine or what not.		High		As the society having strong drugs just wandering through being sold to children and what not after being stolen is super dangerous and harmful.		No			Yes			Yes			Advantaged		Secondary Education	Others	615bbe1b45c3c2e904778bce	617f10dcb9390b6c13530629	617f86dd62bca1a6fd800cc4	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Advantaged	617f86dd62bca1a6fd800cc4	APPROVED	2021-11-01 06:19:14.705000	2021-11-01 06:43:27.950000	1453.245	18.0	183	1	100	2021-11-03 01:26:15.627000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
148	2021-11-01 00:21:47	2021-11-01 00:53:20	IP Address	172.91.84.65	100	1892	True	2021-11-01 00:53:21	R_2ypdKQEahltr6n1					34.06480407714844	-118.44139862060545	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	I still believe race has nothing to do with whether payments should be accepted or believed fraud, the graphs should all be more equal	Mildly unbiased	I still believe race has nothing to do with whether payments should be accepted or believed fraud, the graphs should all be more equal	Mostly useful	It seems to be fair in predicting fraud 	Very unfair	It is unfair in that it thinks non-caucasians have a much higher predictability to commit fraud	Mildly unbiased	It seems biased consider  that non caucasians are believed to have more fraudulent payments	Neither useful nor unusable	Because it is biased I don’t think it would be useful to use in practice	No		Probably model X	Model x has less bias towards race in fraudulent payments	Probably model Y	Model y believes non caucasians have more likelihood to commit fraud	Probably model X	Because it is less biased it is more useful 	Probably model X	Model x has less bias towards race in fraudulent payments	Definitely ${e://Field/pref_model}	Model x has less bias towards race in fraudulent payments	Probably model Z	Model z believes that non-caucasians are more likely to commit fraud which should not be based on race	Probably ${e://Field/pref_model}	Model x has less bias towards race in fraudulent payments			Low	Because it is mistakenly done, there is not a high impact  as payment may be made again			High	Fraudulent payment is more significant and accepting it or a;lowing it to go through would be detrimental to the rentor			Moderate	Denying a payment by mistake is not as detrimental as this mistake may later be fixed, I define society as those who aren’t the individual and those who are surrounding the individual such as their fsmily			Low	Mistakenly allowing a fraudulent payment would have higher less impact as the consequences won’t be on the society, society is anyone surrounding the individual			No			No			Yes			Disadvantaged	Bachelor	Others	6104d73b7cdac0a47a905f0d	617f10dcb9390b6c13530629	617f876cfce8d86f9c1a2dcb	majority	outcome-fpr	rent	False	You have chosen model X over model Y.	model X	top								Disadvantaged	617f876cfce8d86f9c1a2dcb	APPROVED	2021-11-01 06:21:45.109000	2021-11-01 06:53:24.170000	1899.061	21.0	118	1	100	2021-11-03 00:07:28.137000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	Spanish	United States	Female	Yes	
149	2021-11-01 00:38:48	2021-11-01 00:54:35	IP Address	96.245.250.80	100	947	True	2021-11-01 00:54:36	R_1kLjjpexIPELaqI					39.93609619140625	-75.26249694824217	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	they have different ratings.	Neither biased nor unbiased.	Yes because it talks about race	Neither useful nor unusable	In a way yes to help scales	Neither fair nor unfair	Ratings	Neither biased nor unbiased	Yes the race	Neither useful nor unusable	Good to keep count	No		Models X and Y are equally fair	Rating	Models X and Y are equally biased	The race	Models X and Y are equally useful	The ratings are clear	Probably model Y	X only becase its more fair	Definitely ${e://Field/pref_model}	It seems more organized	Definitely model Z	Z seems less organized	Probably model Z	Y more organized			Moderate	Between good and bad			Moderate	Coud be accidental			High	My sister for situations like these			Low	My sister still for support			No			Yes			Yes			Disadvantaged	Primary Education	Others	60fefceabeb266acf92730e3	617f10dcb9390b6c13530629	617f8b576ab9468766b3feb7	minority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	617f8b576ab9468766b3feb7	APPROVED	2021-11-01 06:38:45.640000	2021-11-01 06:54:40.015000	954.375	21.0	115	2	99	2021-11-03 02:45:03.125000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
150	2021-11-01 00:35:02	2021-11-01 01:02:17	IP Address	24.144.45.128	100	1634	True	2021-11-01 01:02:18	R_1j9XYVrdRn8pbUg					35.081207275390625	-92.47219848632812	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Non-caucasians ere granted icu at a much higher rate	Acceptably biased	non-caucasians were unfairly predicted to need icu beds more	Mostly unusable	There was a very large percentage of mistakes	Acceptably fair	it predicted icu at an even rate between caucasians and non-caucasians	Very unbiased	No difference in predictability	Mostly ununsable	for non-caucasians, the margin of error was fairly large	No		Definitely model Y	no unfairness in predictability for icu for caucasians and non-caucasians	Definitely model X	large difference in icu for caucasians and non-caucasians with its of error	Probably model X	less error	Probably model X	more useful in terms of actual results	Probably model Z	same predictability percentage	Probably ${e://Field/pref_model}	large mistake with denied patients	Probably model Z	overall smaller margin of mistakes	Moderate			Beds will be taken up by people that don't need icu	High			greatly impacts people because getting the wrong treatment can lead to death	Moderate			society includes people that will need treatment. moderate because causes beds to be taken but no serious harm to those in beds	High			I consider the society the people in the hospital. impact is high because people not in beds not getting correct treatment	No			Yes			Yes			Disadvantaged			Secondary Education	Others	61003445c25daf9e4a306db9	617f10dcb9390b6c13530629	617f8a8f45bac4ce8a21ae59	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Disadvantaged	617f8a8f45bac4ce8a21ae59	APPROVED	2021-11-01 06:34:59.334000	2021-11-01 07:02:21.956000	1642.622	19.0	87	0	100	2021-11-02 03:40:43.667000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	Yes	
151	2021-11-01 00:29:35	2021-11-01 01:21:55	IP Address	104.32.157.18	100	3140	True	2021-11-01 01:21:56	R_1infPY7ngLav7JX					34.006805419921875	-118.25599670410156	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It doesn’t accept a fair number of people from each racial group.	Mildly unbiased	The fact that it accepts more non-Caucasian people could be a good thing but it could mean that it’s biased towards non-white people 	Mostly useful	It could be tweaked slightly, plus the fact that it makes the same number of mistakes could be a good sign.	Mildly unfair	It makes more mistakes when granting the passage of non-Caucasian people. 	Acceptably biased	While it accepts both racial categories equally it makes more mistakes with no Caucasian people which is probably biased and not as useful	Mostly ununsable	Again if it has such a high mistake level when it concern non-Caucasian personnel it would cause a lot of problems for employees trying to access services and the system	Yes	I would say that the first model wouldn’t be as biased because the error level is equal even if the acceptance rate for one group is higher than the other 	Probably model X	The same error rate makes it so an equal number of folks of each group are accepted using the model and it’s not skewed towards one group or another	Probably model Y	While the same rate of acceptance is good, the differing error levels makes it more biased towards caucasians and poses more problems for non-Caucasian personnel	Models X and Y are equally useful	I’d say both models have their faults but could be equally useful when considering the trade offs that each model represent which could be useful in analyzing in order to make a model that can take the benefits of each model	Probably model X	I’d say model x seems to have the most utility since the error rate is equal and not as skewed towards one group	Probably ${e://Field/pref_model}	The goal is to have the same percentage of mistakes among granted access so Model X has that.	Probably model Z	It allows more caucasians in on accident which seems fairly biased and skewed towards caucasians	Probably ${e://Field/pref_model}	It seems to be the better option since again the number of mistakes remain steady for both groups when it comes to granted access		Moderate		It adds more time to the staff member’s already bus schedule and takes away valuable time that they can use to advance health in whatever capacity. Howeve typing in a username and password is not a very long process and was the status quo so it would not be very difficult 		High		This could put sensitive information and resources at risk and make it harder for medical personnel to do their jobs		Low		This is more of an individual issue but at a societal level it could worsen overall health of a community or society which would have lasting effects		Moderate		Not very high however society (the family of patients, the networks/communities the patients are a part of) could be affected slightly by an inefficient medical system 		Yes			Yes			Yes			Advantaged		Bachelor	Others	6100366f23745d14d740d8ec	617f10dcb9390b6c13530629	617f89482556f28e87c17870	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Advantaged	617f89482556f28e87c17870	APPROVED	2021-11-01 06:29:31.329000	2021-11-01 07:22:02.795000	3151.466	21.0	141	1	99	2021-11-02 03:44:07.267000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	Yes	
152	2021-11-01 00:51:27	2021-11-01 01:40:37	IP Address	24.162.120.35	100	2949	True	2021-11-01 01:40:38	R_3p3geuyra6vcVHB					31.199295043945312	-97.93039703369139	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	I don't think that it is fair or unfair as it is just presenting the data that Model X predicted and the percentage of mistakes made when admitting patients that didn't need to be in the ICU. I don't see anything unfair here.	Neither biased nor unbiased.	I think that Model X uses machine learning to recognize patterns in data that allow it to make predictions. I don't see any bias here because this is based on data. 	Completely unusable	I think that the rate of patients admitted to the ICU unnecessarily is way to large at 35% for both groups.. I think that the hospital could do better and choose a better automated system that utilizes machine learning in a more accurate way. 	Neither fair nor unfair	I think this is neither fair or unfair because again, this is utilizing machine learning to be able to predict outcomes. As it is a machine learning tool, I don't think of it as being unfair or fair.	Neither biased nor unbiased	I think it is neither biased nor unbiased because it is based on data and although the data is different from the last model, it was generated through machine learning, which doesn't have a bias. 	Mostly ununsable	I think that the rate of caucasian people who were admitted to the ICU unnecessarily is really high and will take up hospital and general resources. While non-caucasians had a lower amount of unnecessary admissions to the ICU, it still wasn't to a good enough level of accuracy. 	No		Probably model Y	They are using patterns to determine the data. Although I don't think that either of these models are good quality,  they are by nature neither fair or unfair as they were made by a computer. However, my opinion is that model Y is more fair because of the equality of admissions between both groups. 	Probably model X	I think both models can't necessarily be biased because of the machine learning aspect. Because this data and predictions are generated by machine learning, both are equally unbiased in intention. However, as a human, my opinion is that model X is more biased because it admits less of of one group, which doesn't seem right.	Probably model Y	I think although both models have high rates of inaccuracy, model Y is the better choice because it gives both groups an equal opportunity of admission and seems like it would be more useful because of that. 	Neither model X nor model Y	I dislike both models because of their high levels of inaccuracy. I think that machine learning should streamline and make things more accurate. If it's not making things more accurate, then it's not at the point of where it should be implemented. While some margin of error is always expected, I think the margin is too high here and makes these models incompetent for use. 	Both ${e://Field/pref_model} and Z are equally fair	I think both models are giving reasonable numbers compared to the actual results. It seems like the prediction is fair.	Both ${e://Field/pref_model} and Z are equally biased	I don't think either of these is biased as the predictions seem to be close to the actual results. I think both models are equally unbiased.	Probably model Z	I think both models have promise and seem to demonstrate good ability to predict. I prefer model Z because it gives both groups an equal chance of having a mistaken lack of admission, which as a human, seems fair. It is also fairly accurate, so it may be useful in that way with some honing.	High			I think the impact of mistakenly predicting ICU support is needed could have significant impacts on an individual's financial health. The ICU is very expensive and a waste of resources when admission is unnecessary. 	High			The impacts of mistakenly predicting ICU support will not be required could have a major impact on  an individual, as they are missing out on needed ICU admission. In this case, the person could easily die or become even more sick without receiving the adequate care. 	High			Mistakenly predicting ICU support could significantly impact the society. When thinking about who would be affected by this mistake, I could see families becoming bankrupt from unnecessary medical bills. 	High			Mistakenly predicting ICU support will not be required could affect the society in a significant way because of the individual's family and friends. They would be devastated by their loved one being sick or even dying without adequate care. 	No			Yes			Yes			Advantaged			Secondary Education	Education	5cf7738aee94190018d73574	617f10dcb9390b6c13530629	617f8e5a18daa007293e8e5b	minority	outcome-fpr	icu	True		model X	top								Advantaged	617f8e5a18daa007293e8e5b	APPROVED	2021-11-01 06:51:10.791000	2021-11-01 07:40:39.989000	2969.198	33.0	767	1	100	2021-11-02 03:35:05.018000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Female	DATA EXPIRED	
153	2021-11-01 00:45:54	2021-11-01 01:45:16	IP Address	68.102.116.247	100	3562	True	2021-11-01 01:45:17	R_3qjQHjKxUZc2E77					37.700698852539055	-97.43830108642578	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	It seems to take a better safe than sorry aproach	Very biased	caucasians are grouped into their own chatergory, and minorities are not seperated to have results for pacific minorities. 	Mostly unusable	As a minority I cant see the percentage of patients needing ICU treatment. 	Very unfair	Again the group of minorities is grouped all together. So the results are not pacific to each race. 	Very biased	Doctors can not see the ratio for each minority race needed in icu 	Completely unusable	As a minority it can cause my pacific race to have to pay for ICU treatment that is not needed more often than others. or oposit it may make more of my race that need ICU treatment go without. 	No		Models X and Y are equally fair	Neither break down the difference of the minority races. 	Models X and Y are equally biased	for the same reason, the results for minorities is grouped. 	Models X and Y are equally useful	My minority race is not showing pacific results. 	Probably model X	concidering they are grouping every other race besides caucasians into one group their should be a higher number of ICU predictions than caucacian. 	Definitely ${e://Field/pref_model}	Their are less minority mistakes. 	Both ${e://Field/pref_model} and Z are equally biased	Their is a clear result of the mistakes made in caucasians. minorities are unclear how many mistakes were made. 	Neither ${e://Field/pref_model} nor model Z	Neither help my minority race understand how many mistakes were made. 	Low			They are getting the care regardless. 	High			The individual may not get the needed critical treatment neccessary for their life. 	Moderate			More minorities will get treatment unneccessarily which means more people will live. 	High			More of each minority will go without the treatment they need. 	No			No			Yes			Disadvantaged			Secondary Education	Services Occupations	614d465852620924f85603c8	617f10dcb9390b6c13530629	617f8d1fc890809e3b09fd0e	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Disadvantaged	617f8d1fc890809e3b09fd0e	APPROVED	2021-11-01 06:45:51.374000	2021-11-01 07:45:20.283000	3568.909	34.0	163	0	100	2021-11-03 14:45:39.770000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	DATA EXPIRED	United States	Female	DATA EXPIRED	
154	2021-11-01 00:33:41	2021-11-01 01:59:17	IP Address	172.250.135.250	100	5136	True	2021-11-01 01:59:18	R_2th7xupB7piJKGs					34.0281982421875	-117.03720092773436	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	collecting patterns so not necessarily biased / unfair but probability upon race seems like it’s prone to mistakes 	Mildly unbiased	it’s collecting data patterns so I wouldn’t say model x is biased 	Mostly unusable	as stated above, seems like it’d be full of mistakes 	Mildly unfair	the probability allowment is at even percentages 	Acceptably biased	with as little mistakes on non caucasian and more than half the percentages should be changed to be useful 	Completely unusable	Prone to mistakes 	Yes	saying it was unbiased because it was at high allotment with half mistakes 	Probably model X	less mistakes : probability ratio 	Probably model Y	more probability:mistake ratio	Probably model X	less probability:mistake ratio	Probably model X	less probability:mistake ratio so more usefulness 	Probably model Z	even probability and less mistake ratio	Both ${e://Field/pref_model} and Z are equally biased	neither is more biased than the other	Probably model Z	more usefulness in outcome			High	risk of homelessness			Moderate	if purposely done then everything worked out on their part			High	contemporary people having to work around this is a big deal to a lot of people 			High	people on the internet think this enables other frauds 			Yes			No			Yes			Disadvantaged	Secondary Education	Engineering and Technology	6019cd94485c3e19556b7428	617f10dcb9390b6c13530629	617f8a3d853179596b5ef894	minority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	617f8a3d853179596b5ef894	APPROVED	2021-11-01 06:33:36.657000	2021-11-01 07:59:34.446000	5157.789	22.0	164	0	100	2021-11-03 14:13:37.267000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	Spanish	United States	Female	Yes	
155	2021-11-01 02:57:25	2021-11-01 03:26:14	IP Address	24.183.239.197	100	1728	True	2021-11-01 03:26:15	R_1r2fQIdfhr9vKd2					35.729202270507805	-84.34359741210938	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Truth be told, i beilieve it's  a 50 50 shot as to what race will need a inc bed. There's no model that can actually predict that 	Very biased	Its showing more Caucasians will need an icu bed, but the better question would ve what is the predominant ethnicity of people in the atea? If it's  well blended then there's a 50 50 chance.	Completely unusable	A achine can't tell who will deteriorate faster 	Very fair	This is more like it as theres no guarantee what race will get sick. Its a 50 50 shot.	Very unbiased	Cuz no matter where you are theres always an equal vhance any race can get sock and need a vent, not just one kore tha the other.	Mostly ununsable	I would never tust an algorithm in a machine to faily choose who will need an ice bed. 	No		Probably model Y	As i stated before its a 50 50 chance as to which race will het most sick and need an icu bed. 	Definitely model X	It's favoring Caucasians when that may noy accurately describe the area in which the live.	Probably model Y	Personally i wouldve chose neither if that uad been an optio , bt since it wasnt i just chose the optuon that was most fair.	Neither model X nor model Y	Again i would never trust an algorithm to make a proper medical decision for me or my family!	Probably ${e://Field/pref_model}	Wjen looking at the dadta it is most equal.	Probably model Z	Again this model faors Caucasians. 	Neither ${e://Field/pref_model} nor model Z	Ill never trust a machine to have my back.	Moderate			There is always the possibility of mistake,  but when a machine  is making the decision for an individual the chaces are geater. I could be okay compated tothe huy next to me, so the machince give the icu bed to him, when really he was on the mend and soon if be in need of it.	High			There is a great risk and chance there will be more mistakes that correct outcomes.	High			Any time thete are different ethnic goups involved they make up society, thus if the algorithm for the machine is already biased toward one race more than the other the truth is there will always be more mistakes.	Low			The different ethnic groups around me make up the society around me , therefore I do believe that the chance of mistakes are greater The people of ethnicity like myself Minorities tend to always get the short end of the stick so to speak speak so now you've got a machine that is going to make decisions for you based off of past data. Will that pass data might show that there were more caucasians at 1 point in time that would have needed an icu bed but that's not the case now.. So I believe that there is a greater chance for mistakes using a machine like this in today's society.	No			Yes			Yes			Disadvantaged			Secondary Education	Education	5d497f8d3c428d0017121681	617f10dcb9390b6c13530629	617fabed750a51b2ff0cec98	minority	outcome-fpr	icu	True		model Y	bottom								Disadvantaged	617fabed750a51b2ff0cec98	APPROVED	2021-11-01 08:57:23.304000	2021-11-01 09:26:17.712000	1734.408	35.0	778	2	100	2021-11-03 01:46:18.323000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	English	United States	Female	No	
156	2021-11-10 08:27:22	2021-11-10 08:47:14	IP Address	107.77.205.54	100	1192	True	2021-11-10 08:47:15	R_1Cr2zuvLIzuVJwf					47.6033935546875	-122.34140014648436	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Because both of the mistakes are the same percentage. I do think that non-cacausians are able to make a payment more often but I do not feel this causes it to be unfair because I don't have enough data to know why more caucasians did not get allowed 	Acceptably biased	Because the model leans towards non-Caucasian	Neither useful nor unusable	I don't see how this model would help in real world application 	Mildly unfair	Because non-Caucasians have more mistakes for payment which means that the technology has denied them based on whatever information it was given by the realtor. 	Acceptably biased	Because it seems to favor caucasians	Mostly ununsable	I dont see the practicality in real world application. 	No		Probably model X	Because it allows payments from both groups but it makes the same mistakes for both equally. 	Probably model Y	Because it is more mistakes towards non-Caucasian payments. 	Probably model X	It seems the the better option between the two. 	Neither model X nor model Y	I don't see the use in either of them	Probably model Z	Because I feel that model Z is more fair	Probably ${e://Field/pref_model}	It seems to deny Caucasian cards more often but maybe the rich white men are trying to buy up properties	Probably model Z	It seems more fair			High	If a renter is able to pay but the algorithm denies it then both the rentor and the rentee miss an opportunity. 			Moderate	If an individual is allowed to rent with a fraudulent payment it doesn't impact the individual as much as it does the owner of the complex			High	For the answer I am considering society being the community that the apartment complex is within. If renters are mistakingly denied their transactions it has a major impact on society because Neither the rentor or the rentee are able to get their moneys worth and could lead to a home insecurity within a society			Moderate	Society for this question refers to the community that the apartment complex is within. If fraudulent payments are going through it moderately affects the society because apartment complex owners are missing out on their income which could impact the residence of the building by not having money to keep up with upkeep and maintenance			Yes			No			Yes			Advantaged	Bachelor	Others	6108bbc0999069998b981ffd	618b3dcb32ad5f5cc3f00d86	618be4cf8149dffc3aa6783e	majority	outcome-fpr	rent	True		model Y	bottom								Advantaged	618be4cf8149dffc3aa6783e	APPROVED	2021-11-10 15:27:19.639000	2021-11-10 15:47:21.391000	1201.752	24.0	12	0	100	2021-11-13 02:02:05.400000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
157	2021-11-10 09:10:06	2021-11-10 09:30:54	IP Address	97.71.246.195	100	1247	True	2021-11-10 09:30:55	R_2WuSW5hnkJLAkeS					28.26939392089844	-81.47419738769531	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Because the system isn't perfect and it seems to be fair for mistakes only among granted access	Neither biased nor unbiased.	There is a possibility that it could be biased due to the probability of granting access percentages. However, I do not think that the difference is substantial and could possibly be for other reasons.	Mostly useful	When used correctly, it will help physicians and nurses!	Acceptably fair	Because it grants access to most people regardless of ethnicity 	Mildly unbiased	It doesn't prohibit people access based on ethnicity 	Mostly useful	It will help doctors and nurses to do their jobs better	Yes	The probability of granting access should be equal for everyone 	Probably model Y	Because it grants people access regardless of their ethnicity 	Probably model X	Because the percentages of granting access are not even	Probably model Y	Because it provides everyone with access	Probably model Y	Because it is more inclusive 	Probably ${e://Field/pref_model}	Because the percentages are closer	Both ${e://Field/pref_model} and Z are equally biased	Both have pros and cons and provide different people with access 	Probably ${e://Field/pref_model}	It is less biased when granting access 		High		Because if they needed access to something quickly and couldn't get it, people's lives could be at stake 		High		People shouldn't be able to access others medical records 		Moderate		Society is everyone who may need help from doctors/nurses. In this situation, it may take longer for staff to be granted access, when that time could be needed.		High		Society is everyone who may need help from doctors/nurses. People's medical records could be in jeopardy.		Yes			Yes			Yes			Advantaged		Bachelor	Others	61143b4ddb431c6a3179bd66	618b3dcb32ad5f5cc3f00d86	618beed7af3b929a826e2d98	minority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	618beed7af3b929a826e2d98	APPROVED	2021-11-10 16:10:04.198000	2021-11-10 16:30:57.470000	1253.272	22.0	5	0	100	2021-11-12 17:02:52.026000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Female	Yes	
158	2021-11-10 09:29:25	2021-11-10 09:59:26	IP Address	76.93.57.25	100	1800	True	2021-11-10 09:59:27	R_sthBcVFpPAymXCx					33.871795654296875	-118.33689880371092	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Acceptably fair	There are an equal amount of mistakes	Mildly unbiased	The probability differs but the mistakes are equal 	Neither useful nor unusable		Mildly unfair	More mistakes towards non Caucasians 	Very biased	The probability is even but the mistakes are higher amongst non Caucasian 	Mostly ununsable		No		Definitely model X	Less mistakes	Definitely model Y	Uneven amount mistakes towards non caucasians 	Probably model X	Model x is more useful because there are an equal amount  of mistakes	Definitely model X	The probability of mistakes is even	Definitely ${e://Field/pref_model}	Mistakes among granted access is better than z	Definitely model Z	Mistakes amongst non caucasians is 45%	Definitely ${e://Field/pref_model}			High		Staff should be recognized without password		High		Unauthorized users should not be granted access		Low		Staff, i feel like they would want people to submit pw & username		High		Staff, im sure they can agree unauthorized users should not have access  		No			Yes			No			Advantaged		Bachelor	Administrative Staff	6108c9b49ca02152b407f6e2	618b3e1d59be0d9e09f6f943	618bf34130aefe05f2bca1ca	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Advantaged	618bf34130aefe05f2bca1ca	APPROVED	2021-11-10 16:29:05.293000	2021-11-10 16:59:29.996000	1824.703	25.0	143	0	100	2021-11-14 17:17:41.430000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	Spanish	United States	Female	No	
159	2021-11-10 09:41:23	2021-11-10 10:18:09	IP Address	75.72.189.121	100	2206	True	2021-11-10 10:18:09	R_1HoWyX13h1SlMlh					44.94020080566406	-93.21880340576173	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 1	Neither fair nor unfair	There isn't enough information to decide. I would want to understand the percent of patients not allocated to the ICU that should have been.	Neither biased nor unbiased.	Same as above, what about the percent of patients not allocated to the ICU that should have been.	Neither useful nor unusable	Same as above, what about the percent of patients not allocated to the ICU that should have been.	Mildly unfair	The model is 2x as likely to make a mistake (admit when not needed) for Caucasian patients compared to non-caucasian patients. This makes me think that scare ICU resources are being over-allocated to white people over people of color.	Very biased	It appears to be biased towards lower risk tolerance for white patients than non-white patients.	Mostly ununsable	The model is little better than a coin toss for one group of patients and may disadvantage the other group.	Yes	I ignored the percent of patients not allocated to the ICU that should have been. I still think the first model is unfair based on the disparity in risk/accuracy for patients admitted to the ICU. The outcomes for patients not admitted to the ICU (i.e., should they have been) is needed to fully assess the fairness, bias, and usability of the model.	Probably model X	With the information provided, the success rate is the same for both groups in the ICU with model X but not with model Y	Probably model Y	Model Y appears to tolerate less risk on admitting patients based on racial group.	Models X and Y are equally useful	Without knowing the false-negative rate I can't evaluate overall usefulness.	Neither model X nor model Y	I don't have information to compare the false-negative rate of the two models and the performance of doctors without use of a model. A model needs to provide some benefit over the baseline case with no model and there is no information about that provided.	Definitely model Z	Model Z l is more likely to recommend an ICU bed when it is not needed to a Caucasian patient, but it has similar performance for patients who are not assigned to the ICU. There is still some over-allocation of resources for the two different racial groups, but that does not appear to have a disparity by race for patients that aren't assigned to the ICU. Model X is clearly more likely to deny needed ICU supports to a non-Caucasian patient.	Definitely ${e://Field/pref_model}	Model X is clearly more likely to deny needed ICU supports to a non-Caucasian patient.	Probably model Z	Model Z is definitely better than Model X, but I still want to know how doctors do without the help of a model.	Moderate			The individual will receive the care they need, but the increased care will cost more. The individual may not be able to afford this higher cost if not covered by insurance.	High			The individual would face significant health risks or may die if they do not receive the needed ICU support.	High			Other patients that do need the ICU supports would not be able to access them. Doctors and nurses may be overburdened by caring for a patient that does not require ICU support. Many different parts of society may be burdened by the increased cost of unneeded care--other people on the insurance plan who may experience a rise in premiums due to higher overall cost of care, the family of the individual, broader society may also see the impacts through higher tax rates, GoFundMe campaigns, etc.	Moderate			Friends and family will face additional emotional costs if the individual suffers negative health outcomes as a result of being denied ICU supports. Doctors and nurses may feel guilt about relying on an algorithm that made poor decisions, society broadly may lose the contributions of the individual to society.	No			No			Yes			Disadvantaged			Master	Business	61720eac6fb2935967f88d5a	618b3dcb32ad5f5cc3f00d86	618bf628df43da843f581ee1	minority	outcome-fpr	icu	False		model X	top								Disadvantaged	618bf628df43da843f581ee1	APPROVED	2021-11-10 16:41:20.289000	2021-11-10 17:18:13.086000	2212.797	35.0	6	0	100	2021-11-13 02:28:05.251000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	Yes	
160	2021-11-10 10:37:25	2021-11-10 10:49:05	IP Address	67.185.222.65	100	700	True	2021-11-10 10:49:05	R_3fjyDuAZJVgolPk					47.14430236816406	-122.25450134277344	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The mistakes are the same percentage, assuming that it's a percentage of the above number and not in total (for example, only 6% of non-Caucasian being correct)	Mildly unbiased	It would depend on the factors involved in why it works better for Caucasian 	Mostly useful	It has a high percentage of understanding, even with the errors 	Mildly unfair	The mistakes are unbalanced. That makes it more irregular than if the percentage had been unbalanced	Neither biased nor unbiased	I don't think this is biased, so much as just inaccurate	Neither useful nor unusable	The mistakes are a high percentage between the two groups 	No		Definitely model X	The mistakes percentage is driving this for me	Probably model Y	The error is vast between the two groups 	Probably model X	Mistakes are equal	Probably model X	Mistakes are equal	Probably model Z	Mistakes among denied in model X are huge. That's a big room of error. Better to mistake in granted vs denied	Probably ${e://Field/pref_model}	It really unbalances non-Caucasian denied ICU	Probably model Z	This seems to have less room for error if you're denied a room	Moderate			This could take rooms from people who need it, but doesn't overly deny to others by decision	High			This could directly mean death	High			Society may include people who wanted this. It may affect them more 	Moderate			I think people will look for the greater availability	No			Yes			Yes			Advantaged			Bachelor	Transportation Occupations	616b2966d0aee4f9879a656b	618b3dcb32ad5f5cc3f00d86	618c034e957cde0d55a233af	minority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	618c034e957cde0d55a233af	APPROVED	2021-11-10 17:37:20.485000	2021-11-10 17:49:09.051000	708.566	39.0	269	0	100	2021-11-13 01:27:09.799000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
161	2021-11-10 10:16:27	2021-11-10 10:49:25	IP Address	68.59.201.87	100	1977	True	2021-11-10 10:49:26	R_22EUU6RZv97iewC					34.69200134277344	-85.26020050048828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	We shouldn't be predicting anything based on a person's race/skin color even if it is based on cultural genetics of that person's ethnicity	Neither biased nor unbiased.	It doesn't appear to be biased but I don't know if I have enough information to make that decision	Completely unusable	I don't think a computer or program should be making such important predictions about potential life/death scenarios. That should only be left up to human intellect and educated professionals.	Very unfair	I feel that it is unfair to use a model for predicting. That is not how real life sometimes happens.	Very biased	It appears to be biased toward one ethnicity instead of being fairly predicting based on something other that race/skin color.	Completely unusable	In my unprofessional, uneducated opinion it is not useful because I value person to person interaction and decision making. Sometimes, there are faults there as well, but it is more emotionally connected than statistics or models with percentages. 	No		Models X and Y are equally fair	I don't feel either are fair, because theyre both capable of making the mistake that someone will need the ICU incorrectly, which will potentially take the bed from someone who truly needs it. I don't agree with the models at all. 	Models X and Y are equally biased	They are both biased toward a certain skin color rather than being measured by illness or physiological need.	Models X and Y are equally useful	I don't have an option to choose both are equally useless	Neither model X nor model Y	To re-iterate, I don't feel the models are useful in predicting such a delicate scenario. That should be left up to medical personnel.	Both ${e://Field/pref_model} and Z are equally fair	both are equally UNfair	Both ${e://Field/pref_model} and Z are equally biased	same answers	Neither ${e://Field/pref_model} nor model Z	same answers as previous.	High			someone may lose their life as not being predicted who needed the ICU given to the mistaken individual	High			The individual will not receive care they needed in a timely manner	High			From the view of society it would cause distrust	High			Will cause distrust	Yes			No			Yes			Disadvantaged			Secondary Education	Others	5d6691bf6c7981001a7fdbd6	618b3dcb32ad5f5cc3f00d86	618bfe5e612f52b7c914f19b	majority	outcome-fpr	icu	True		model Y	bottom								Disadvantaged	618bfe5e612f52b7c914f19b	APPROVED	2021-11-10 17:16:23.218000	2021-11-10 17:49:29.007000	1985.789	37.0	150	0	100	2021-11-13 01:31:47.148000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Caucasian	English	United States	Female	No	
162	2021-11-10 10:32:33	2021-11-10 10:53:36	IP Address	75.102.105.108	100	1263	True	2021-11-10 10:53:36	R_1dnzahjHjCeWgZt					40.12809753417969	-75.1073989868164	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It allows access to more non white people but the mistakes are equal for both categories	Neither biased nor unbiased.	I don't think there is enough of a difference for there to be a significant bias	Mostly unusable	I don't know if a 35% error is considered large, but to me it should be smaller	Very fair	It allows and equal amount of whites and non whites access	Very unbiased	Again, the two categories are equal	Completely unusable	There is a very large error in the non white category	No		Probably model Y	The two categories are equal while they are not in model x	Probably model X	the two categories are not equal	Probably model X	The errors are much less 	Probably model X	The smaller errors	Probably model Z	The errors are less in both graphs	Probably ${e://Field/pref_model}	Higher errors for both graphs	Definitely model Z	lower errors		Moderate		It wastes time that hospital staff could be using to run the hospital		High		They could do serious harm to others 		Moderate		I consider everyone living in the area society, and my reason is the same as before, it wastes time for the staff		High		My reasoning is again the same as before, they could do serious harm to people in society, which is all the people living in the area		No			Yes			Yes			Advantaged		Secondary Education	Others	6109ced9183e69e8ba12c15f	618b3dcb32ad5f5cc3f00d86	618c022a07fb29a634aa0e34	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Advantaged	618c022a07fb29a634aa0e34	APPROVED	2021-11-10 17:32:30.898000	2021-11-10 17:53:41.875000	1270.977	18.0	79	0	100	2021-11-13 02:08:48.305000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	Yes	
163	2021-11-10 11:05:41	2021-11-10 11:26:37	IP Address	128.230.140.137	100	1256	True	2021-11-10 11:26:37	R_22oiksYyKjkTovg					43.039093017578125	-76.12770080566406	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Non-Caucasian individuals were allowed to make payments more than Caucasian individuals more than 20%	Neither biased nor unbiased.	They mistakenly let a fraudulent payment occur at the same rate for both caucasian and non-caucasian individuals.	Neither useful nor unusable	The amount of fraudulent payments that went through was high for both groups.	Acceptably fair	While it allowed a payment go through at the same rate for both groups, it did not catch fraudulent payments for non-caucasians verses caucasians and there was a 21% difference.	Acceptably biased	There was a 21% difference in fraudulent payments for both groups.	Mostly ununsable	The amount of fraudulent payments that went through is very high.	Yes	I think I would see Model X as more fair or unbiased	Probably model X	The amount of fraudulent payments is the same for Model X.	Probably model Y	It is more likely for non-Caucasian individuals to have fraudulent payments	Probably model X	There is less fraudulent payments that go through in Model X	Probably model X	It is less biased and more fair.	Both ${e://Field/pref_model} and Z are equally fair	While model x denies more caucasian individuals than non-caucasian for payments, model z mistaknely allows fraudulent payments to go through for non-caucasian payments.	Both ${e://Field/pref_model} and Z are equally biased	For the same reason as above.	Probably ${e://Field/pref_model}	I rather have more mistakenly denied payments than mistakenly allowed fraudulent payments.			Low	You can probably just restart the process of putting the payment through again because you did not lose any money.			High	Depending on how much money it was, it will be hard to recover that money back and that individual may have to struggle financially or be removed from their home.			Low	I consider landlords and banks to be part of society and I think it is of low significance because the payment process can just be started over.			High	As a landlord or bank that means you lose out on money.			Yes			No			Yes			Disadvantaged	Secondary Education	Others	60fec6e6098b05609f53de86	618b3e1d59be0d9e09f6f943	618c09f148834b14bd58aabf	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	618c09f148834b14bd58aabf	APPROVED	2021-11-10 18:05:39.322000	2021-11-10 18:26:40.249000	1260.927	19.0	47	0	100	2021-11-14 17:15:22.549000	19AE28A9	United States	United States	Due to start a new job within the next month	Non-Caucasian	English	United States	Female	Yes	
164	2021-11-10 11:49:07	2021-11-10 12:06:27	IP Address	174.213.160.1	100	1040	True	2021-11-10 12:06:28	R_RlXtX3izMnDmYBH					37.751007080078125	-97.82199859619139	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The mistake rate is the same for both groups.	Very unbiased	The mistakes are even across groups	Very useful	It isn't biased.	Mildly unfair	Mistakes were much higher in Caucasians.	Acceptably biased	Caucasians are being admitted more than they should.	Mostly ununsable	It is biased towards Caucasians.	No		Definitely model X	The mistakes are the same among the groups so they are admitting relatively the right number of people per group.	Definitely model Y	They are letting more Caucasians in the study than they should due to the higher mistake rate than non Caucasians.	Definitely model X	The model works well with the mistake rate being the same for both groups.	Definitely model X	The mistake rate is the same among groups for the numbers being admitted to the ICU	Definitely ${e://Field/pref_model}	Mistake rate is the same among groups	Both ${e://Field/pref_model} and Z are equally biased	It's all relative to who is being submitted	Probably ${e://Field/pref_model}	The mistake rate is the same	High			ICU support is an urgent need.	High			It could be life threatening.	Moderate			Society is those not impacted by decision. They would care in case it was someone related to them and if they have empathy.	Moderate			I considered those not impacted as part of the society, and it is because it is not impacting then directly but most people would still care.	Yes			Yes			Yes			Advantaged			Bachelor	Others	6169d9eca7d16a7447f5cd64	618b3dcb32ad5f5cc3f00d86	618c141b7f745f2ea09e9840	minority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	618c141b7f745f2ea09e9840	APPROVED	2021-11-10 18:49:04.499000	2021-11-10 19:06:31.900000	1047.401	32.0	67	0	100	2021-11-13 02:24:14.507000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	No	
165	2021-11-10 11:29:24	2021-11-10 12:10:05	IP Address	107.4.192.136	100	2440	True	2021-11-10 12:10:05	R_30pgFS8pa3mpfmc					44.94270324707031	-93.28710174560547	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very fair	If it makes the same amount of mistakes for each group, that leads me to believe that bias is not the reason for the contrast in ICU predictions.	Mildly unbiased	Any model created by humans is likely to have bias built into it in some way.	Very useful	Any data that can be examined and compared is useful data when developing this kind of technology.	Neither fair nor unfair	It is unlikely that the number of necessary ICU assignments would be exact for both groups, which means the model is unbiased but not likely accurate, and therefore possibly missing an important variable that may or may not lead to unfairness in the resulting assignments.	Very unbiased	the model assigns the same number of ICU beds to each group regardless of the number of mistakes or any other variables. It is unbiased but not well developed. 	Mostly useful	Seeing the resulting data when filtered through the control factor of the unbiased model could reveal interesting trends.	Yes	I think the model is mostly unbiased, if it makes the same amount of mistakes for each group, then that indicates there must be a reason (disparity) why the model has to predict such high numbers of ICU beds for Non-Caucasian patients for the algorithm to have the same level of accuracy (ie, to not miss patients who actually do need ICU beds vs wrongly assigning to patients who do not need them).	Definitely model X	Model X displays an example of equity, whereas Model Y displays an example of unbiased but inefficient equality.	Models X and Y are equally biased	any model created by humans is going to have human bias built into it.	Definitely model X	It leads to actual fairness in treatment by assigning no more erroneous ICU beds to one group than another.	Definitely model X	It applies the concept of equity.	Definitely model Z	it results in the same number of denied ICU beds per group, and considering the critical nature of a denied ICU bed to a person who truly needs one, no other data matters in determining the fairness of each model.	Both ${e://Field/pref_model} and Z are equally biased	they are equally biased, but from opposing angles of favor.	Definitely model Z	it results in an equal margin of error for wrongly denied ICU beds, which I believe to be the most critical data set to prioritize.	Low			the patient can simply be reassigned to a regular bed if possible, or if not, they receive a room with more life saving resources than they need.	High			A patient who needs ICU treatment and is denied an ICU bed is likely to face serious complications or loss of life as a result.	Moderate			A mistaken prediction in this direction would likely coincide with a mistaken denial of someone else, and that decision would have to be made by a critically thinking human. Also, predicting more support than is required could show data trends that result in spending more on equipment to provide more support in the future. 	High			Mistakenly denying an ICU bed is likely to result in serious complications or loss of life for the individual, which results in higher costs for the hospital to try to save the individual's life once a complication occurs, legal costs of potential malpractice lawsuits, pain and suffering of families, unexpected loss of income for family members which could lead to homelessness and further instability, a generational mistrust of medical professionals, and a public relations concern for the medical facility involved.	No			No			Yes			Disadvantaged			Secondary Education	Administrative Staff	5d231f989b3bf900189d73f7	618b3e1d59be0d9e09f6f943	618c0e9fb9f18e132240ddd9	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Disadvantaged	618c0e9fb9f18e132240ddd9	APPROVED	2021-11-10 18:29:21.150000	2021-11-10 19:10:10.312000	2449.1620000000003	32.0	331	1	100	2021-11-14 02:43:52	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Female	DATA EXPIRED	
166	2021-11-10 11:03:46	2021-11-10 12:15:34	IP Address	172.56.42.101	100	4307	True	2021-11-10 12:15:34	R_vThmKvIur6l3ADT					47.44670104980469	-122.26840209960936	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	There seems no statistical reason that 24% more of white patients would qualify as needing ICU care than non-whites. The fact that the error rate *after the unfair selection process* is the same is not relevant to that bias.	Very biased	Same answer: There seems no statistical reason that 24% more of white patients would qualify as needing ICU care than non-whites. The fact that the error rate *after the unfair selection process* is the same is not relevant to that bias.	Very useful	It reveals the unfair bias towards white patients inherent in the system. It is barely useful for admissions decisions however.	Very unfair	"1. It doesn't include valuations of mistakes of non-admissions to support the validity of the model
2. It dramatically grants admission of Caucasian patients who don't need ICU treatment over non-whites, which could be critically unfair in light of non-admissions mistake data"	Very biased	"It dramatically grants admission of Caucasian patients who don't need ICU treatment over non-whites, which could be critically biased in light of non-admissions mistake data.
Whether that represents a bias in the data gathering or the model is difficult to say without non-admissions and other data."	Very useful	It exposes flaws in the system that results in bias against non-white patients.	No		Probably model Y	At least Model Y admits white and non-white patients at the same rate. But the mistake rates still show a bias in those admissions.	Probably model X	Model X admits white patients at a higher rate than non-whites, which has no apparent basis in a non-biased model.	Models X and Y are equally useful	All data is useful, especially data that shows inherent flaws in the system or model.	Probably model Y	Question states I have to choose one. Model Y is most fair.	Both ${e://Field/pref_model} and Z are equally fair	As long as the rates of admission and of errors (in either admissions or non-admissions) favor the correct treatment of white patients twice as much as non-white patients, the model is neither fair nor un-biased.	Both ${e://Field/pref_model} and Z are equally biased	As long as the rates of admission and of errors (in either admissions or non-admissions) favor the correct treatment of white patients twice as much as non-white patients, the model is neither fair nor un-biased.	Probably model Z	I have to choose one. Model Z results in fewer un-treated ICU required patients.	Low			For that individual patient, the impact would be low as they received more care than necessary -- except they might feel long-term guilt over taking an ICU bed that a now dead patient needed to survive.	High			Because presumably an individual mistakenly excluded from the ICU will die.	High			As society includes the patients mistakenly excluded from the limited number of ICU beds that die as a result of an individual mistakenly and unnecessarily being granted an ICU bed, as well as their families, friends, doctors, nurses, etc, the impact on society is high.	High			Same answer applies: As society includes the patients mistakenly excluded from the limited number of ICU beds that die as a result, as well as their families, friends, doctors, nurses, etc, the impact on society is high.	No			Yes			Yes			Advantaged			Doctoral	Services Occupations	614f754098c9877761eafb4b	618b3dcb32ad5f5cc3f00d86	618c097bf8c349eebf391581	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c097bf8c349eebf391581	APPROVED	2021-11-10 18:03:43.545000	2021-11-10 19:15:40.697000	4317.152	49.0	123	1	99	2021-11-12 17:54:11.560000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	No	
167	2021-11-10 12:16:10	2021-11-10 12:26:56	IP Address	72.194.26.210	100	645	True	2021-11-10 12:26:57	R_2wyG1DYLtlOCt8C					34.42649841308594	-119.86309814453124	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	Same reason as model Y reasoning	Neither biased nor unbiased.	Same as model Y reasoning	Neither useful nor unusable	Same thing as model Y reasoning	Neither fair nor unfair	Because it's both Fair and not fair sometimes people's payments don't got though due to certain terms of banking	Neither biased nor unbiased	Cause it depends on also their credit but also their payments 	Neither useful nor unusable	Useful but not useful because it's just an example 	No		Models X and Y are equally fair	Because they're all based on just an example chart	Models X and Y are equally biased	You're going off of people's charts versus checking the whole credit history their whole payment process the renting's history you can't go based on a chart	Models X and Y are equally useful	They're almost identical just a little bit changed on in model X so II think that they're the same	Neither model X nor model Y	I want to choose either of them honestly I mean I feel like I couldn't put my bias or my choosing between 2 different 2 different by almost identical charts	Both ${e://Field/pref_model} and Z are equally fair	Same thing they're all both equal just a little bit different if an z	Both ${e://Field/pref_model} and Z are equally biased	Their butt butt butt they're this just the chart they're examples they're not really the same	Neither ${e://Field/pref_model} nor model Z	Same			Moderate	Because I picked moderate because it doesn't pack renter's payment because of the whole I'm trying to get into a place but it depends on if the stuff is stolen or not which is understandable so			Moderate	People will get anywhere get anything for money basically and mistakely allowing fraudulent payment should be avoided at all cost			Moderate	Our society is all turning into payments through the Internet through it's all online and I feel like you can still anybody's credit and denying anybody's transactions from merchants that's bad that's especially if it's fraudulent			Moderate	Our society has been allowing fraudulent payments because the cards are stolen or people still their information and it's not OK			Yes			Yes			Yes			Disadvantaged	Primary Education	Others	614ee2e25c7d068cb049d9e2	618b3dcb32ad5f5cc3f00d86	618c1a7559bd158cfd6630fc	minority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	618c1a7559bd158cfd6630fc	APPROVED	2021-11-10 19:16:09.134000	2021-11-10 19:27:00.781000	651.6469999999999	33.0	133	4	94	2021-11-12 17:51:35.018000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	No	
168	2021-11-10 12:12:35	2021-11-10 12:27:01	IP Address	63.142.110.105	100	866	True	2021-11-10 12:27:02	R_2WxqBrYYTznIkoz					40.093597412109375	-87.65160369873047	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Neither fair nor unfair	It says the probability. It doesn't say why.  Maybe they have bad lighting distinctions. 	Neither biased nor unbiased.	Same	Neither useful nor unusable	You never know?	Neither fair nor unfair	It shows the probability not the outcome	Neither biased nor unbiased	It's a guess?	Neither useful nor unusable	Not sure	No		Models X and Y are equally fair	Idk I like more details I guess	Models X and Y are equally biased	It happens	Models X and Y are equally useful	Depends on how you see it	Probably model X	Just picked one	Both ${e://Field/pref_model} and Z are equally fair	All guesses are fair?	Both ${e://Field/pref_model} and Z are equally biased	Sounds good	Definitely model Z	Regret choosing my x		Moderate		If it's an emergency it q few minutes could make a difference		High		Who knows what they could accomplish		High		Society hates everything		High		Never can trust people		No			Yes			Yes			Advantaged		Secondary Education	Others	5ddeb0fd370810e21afeee53	618b3dcb32ad5f5cc3f00d86	618c199d3907e7330a9df8da	majority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	618c199d3907e7330a9df8da	APPROVED	2021-11-10 19:12:33.714000	2021-11-10 19:27:05.385000	871.671	39.0	419	3	99	2021-11-13 01:34:51.961000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Female	DATA EXPIRED	
169	2021-11-10 11:29:03	2021-11-10 12:33:38	IP Address	174.212.129.236	100	3875	True	2021-11-10 12:33:39	R_24l03Y0vH9x12Ed					36.05709838867188	-86.67289733886719	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	There appears to be a large discrepancy in the payments allowed by caucasions vs those of non caucasions 	Mildly unbiased	It simply shows the error rate of processed payments	Mostly unusable	It’s a good tool to show what payments are turned down as well as to illuminate possible racism	Neither fair nor unfair	That there is no difference based on race seems fair. But only allowing 53% of payments to be processed seems low and unfair.	Acceptably biased	Yes, it seems biased because a race appears to be singled out.	Mostly useful	It shows no racial profiling, but lets the developer know that their payment acceptance rate is low.	Yes	I realized that the second chart of the first model only showed errors that were made. So, no discrimination was made. If racism was a factor, it would have been reflected in the first chart presented rather than the second.	Probably model Y	The probability of accepting payments is the same. Mistakes made cannot be controlled, only measured, after everyone has the same payment opportunity.	Probably model X	The acceptance of payments from white people is much higher.	Models X and Y are equally useful	Any data gathered can be useful as long as it’s presented in a no biased way (even if it reveals a biased system)	Probably model Y	Because it appears to not be racist 	Probably model Z	Even though the acceptance of “white” payments is higher, the technology has appeared to choose fraudulent charges equally (probably not based on race)	Probably ${e://Field/pref_model}	The disparity in results of wrongly accepted payments is so large, it appears that it was biased by trying to not be prejudiced.	Probably model Z	I think it might be a more accurate picture of the true data			Low	It seems that would be an easily remedied situation. The renter would likely still pay.			Moderate	There would probably be a loss of income that might not be recovered.			Low	It seems that it would cause problems on a more individual basis.			Low	Again, it seems that it would affect few people.			Yes			Yes			Yes			Advantaged	Secondary Education	Services Occupations	61536cbb3240efe4a10f70ec	618b3dcb32ad5f5cc3f00d86	618c0f6675a3e1592b8fe3dc	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c0f6675a3e1592b8fe3dc	APPROVED	2021-11-10 18:29:00.232000	2021-11-10 19:33:43.042000	3882.81	41.0	80	1	98	2021-11-13 02:15:40.245000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	Yes	
170	2021-11-10 12:28:41	2021-11-10 12:40:14	IP Address	68.36.87.223	100	693	True	2021-11-10 12:40:15	R_2fg1wSzrPA2piOL					42.9468994140625	-85.62059783935545	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Mildly unfair	It allows far more payments from white people than not	Very biased	It seems to trust white people way more	Mostly unusable	Seems like it has unusable bias	Mildly unfair	It looks like it makes way more mistakes for the Caucasian part of the chart	Acceptably biased	Once again it is making more mistakes for one part of the model	Neither useful nor unusable	Not sure how reliable it can be	No		Probably model X	Payments should be accepted equally, give people the chance instead of denying them, mistakes can be fixed afterwards.	Probably model X	It is doing more assumption work	Probably model Y	It finds and corrects more mistakes	Probably model Y	Like i said, give people a fair chance to make a payment	Probably ${e://Field/pref_model}	I still think you should be able to make a payment	Probably model Z	It accepts payments more from one group	Probably ${e://Field/pref_model}	Same reason as before			High	It gives people less of an opportunity then they should have			Moderate	You also dont want fraud			Moderate	Im sure on group is more affected than others.			Low	Its probably not as bad in the long run			No			No			Yes			Advantaged	Bachelor	Others	5efe1ba6649c910ffdd56c6a	618b3dcb32ad5f5cc3f00d86	618c1d65ad72daa40f489956	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c1d65ad72daa40f489956	APPROVED	2021-11-10 19:28:39.827000	2021-11-10 19:40:17.572000	697.745	30.0	184	1	100	2021-11-13 01:36:55.575000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	DATA EXPIRED	United States	Male	DATA EXPIRED	
171	2021-11-10 12:33:07	2021-11-10 12:44:36	IP Address	73.94.79.67	100	688	True	2021-11-10 12:44:37	R_1C2vCrySYhXtP1h					44.90060424804688	-92.92749786376952	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 2	Acceptably fair	I’d the model projected it, there has to be some data that suggests this as a higher probability. 	Neither biased nor unbiased.	If it’s going off of previous data to calculate projections there should not be a bias involved. 	Mostly useful	Should give a higher probability of what the outcome will be.	Acceptably fair	If it’s going off of past numbers, this should be fair data.	Mildly unbiased	There should be no bias involved with these numbers. 	Mostly useful	Should give a high probability look at the outcome.	No		Models X and Y are equally fair	Going off of projections should be fair.	Models X and Y are equally biased	Should not be any difference in bias if the numbers don’t lie.	Models X and Y are equally useful	Both give high probability looks at the outcomes.	Neither model X nor model Y	Given the information provided, I believe it’s hard to make a justification for either one.	Both ${e://Field/pref_model} and Z are equally fair	Given the numbers, they should both be fair.	Both ${e://Field/pref_model} and Z are equally biased	There should be no bias.	Neither ${e://Field/pref_model} nor model Z	Hard to justify which one is better based on the information given.			Moderate	Could cause unnecessary inconveniences.			Moderate	Would cause trouble for the one accepting payments. 			Moderate	If others in society (renters) get a hold of this, there could be some level of outrage or emotion.			Low	This would likely just affect the person accepting the payment, isn’t a large scale issue for society (others who rent).			No			No			No			Advantaged	Bachelor	Healthcare Practitioner	5def38b90336495d5f630803	618b3dcb32ad5f5cc3f00d86	618c1e6f9f5183db5d9f46e3	majority	outcome-fpr	rent	True		model X	top								Advantaged	618c1e6f9f5183db5d9f46e3	APPROVED	2021-11-10 19:33:05.536000	2021-11-10 19:44:39.564000	694.028	28.0	111	1	100	2021-11-12 17:18:26.578000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
172	2021-11-10 12:31:42	2021-11-10 12:51:49	IP Address	172.58.158.246	100	1206	True	2021-11-10 12:51:49	R_2e9d8g8wHWcxCU9					35.22959899902344	-80.84300231933594	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The model allowed more payments of one group when they both had the same percentage of bad payments	Very biased	Separating my race 	Neither useful nor unusable	I think it provides good information but should look at different characteristics 	Very fair	Allowed the same anoint of payments	Mildly unbiased	Allowed same amount of payments regardless of race	Neither useful nor unusable	Should have not allowed as many payments through since they came out fraudulent 	No		Definitely model Y	Amount of accepted payments did not change based on race	Definitely model X	Race was a determining factor	Models X and Y are equally useful	They both have flaws	Definitely model Y	More fair	Probably ${e://Field/pref_model}	Equal number approved	Probably model Z	Different approvals	Probably ${e://Field/pref_model}	Less racially biased			High	Not receiving money/worrying about eviction			High	Not receiving payment 			Moderate	Society: economic community. Less money coming in and stress about why it was denied			High	Stress of not getting money			No			No			Yes			Advantaged	Doctoral	Healthcare Practitioner	60fd99351090788eaebbda7b	618b3dcb32ad5f5cc3f00d86	618c1e1841703060e1f069a3	minority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c1e1841703060e1f069a3	APPROVED	2021-11-10 19:31:38.981000	2021-11-10 19:51:53.846000	1214.865	28.0	148	1	100	2021-11-13 01:50:58.897000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
173	2021-11-10 12:42:53	2021-11-10 12:56:42	IP Address	72.220.102.127	100	828	True	2021-11-10 12:56:43	R_1Dvu5YkazzlfzXg					32.84660339355469	-116.9770050048828	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to be accepted than female applicants.	Figure 2	Neither fair nor unfair	makes an equal amount of mistakes for both groups	Acceptably biased	the probability is skewed	Neither useful nor unusable	not sure if the amount of mistakes is acceptable	Mildly unfair	it thinks non-whites are less likely to be doctors	Acceptably biased	there seems to be more white doctors for the training photos	Mostly useful	it gives data that can be used to fix the problem	No		Models X and Y are equally fair	they both tend to favor whites in different ways	Probably model X	X seems to think more doctors are white	Probably model Y	it shows a more clear difference in access vs mistakes	Probably model Y	It is more likely to correctly allow a non-white access	Both ${e://Field/pref_model} and Z are equally fair	both are skewed	Both ${e://Field/pref_model} and Z are equally biased	both have boas in a different way	Neither ${e://Field/pref_model} nor model Z	I'm not sure which one would be easier to use in practice		Low		it allows staff to enter and does not allow non-staff		High		non-staff can get into areas and possibly cause issues and confusion		Moderate		there could be outrage that non-whites are routinely not recognized, society is twitter verse and news outlets		Low		most news outlets dont care about someone accidently wandering around in a hospital		No			Yes			Yes			Advantaged		Master	Engineering and Technology	5af20a80b300870001fd2ecb	618b3dcb32ad5f5cc3f00d86	618c20b87dbb7d9bd7016f66	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c20b87dbb7d9bd7016f66	APPROVED	2021-11-10 19:42:52.534000	2021-11-10 19:56:45.527000	832.993	29.0	1057	4	99	2021-11-12 17:15:20.898000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Male	No	
174	2021-11-10 12:52:32	2021-11-10 13:01:36	IP Address	172.58.96.166	100	544	True	2021-11-10 13:01:37	R_2S1rl8IXLdEwmp4					30.34140014648437	-97.731201171875	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Preference over Caucasian	Very biased	There is a clear preference 	Mostly unusable	Rate of error is high 	Acceptably fair	There is not preference of race 	Very unbiased	There is no preference	Very useful	Mistake percentage is fairly low 	No		Definitely model Y	Equal access to Caucasian and non Caucasian	Definitely model X	Clear preference 	Probably model X	Less percentage of error 	Probably model Y	I'm a non Caucasian 	Definitely ${e://Field/pref_model}	No preference between Caucasian and non Caucasian 	Probably model Z	More of a preference towards Caucasian 	Definitely ${e://Field/pref_model}	I'm not Caucasian		High		Of required hospital staff can't get in, people won't have access to the healthcare they provide 		High		It's a potentially dangerous situation. One wouldn't know if the person getting in wants to commit violent acts 		High		We need access to hospital staff that could save our lives 		High		Everyone (men,women,children, all races) could be put into a potentially dangerous situation if something new was granted access 		Yes			No			Yes			Disadvantaged		Secondary Education	Others	60fe73f8c8a77cdc4b4d9d4a	618b3e1d59be0d9e09f6f943	618c22f97b48237e95105a36	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c22f97b48237e95105a36	APPROVED	2021-11-10 19:52:30.774000	2021-11-10 20:01:41.974000	551.2	33.0	124	3	97	2021-11-14 02:54:08.173000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
175	2021-11-10 12:28:42	2021-11-10 13:03:32	IP Address	173.175.120.208	100	2090	True	2021-11-10 13:03:33	R_3TYha0CqHrhjkdz					29.54429626464844	-98.54989624023438	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Although the acceptance rate between measured groups differs, the error rate between them is the same, so it seems to be operating decently fairly/equally to me.	Acceptably biased	"It appears to be a little biased towards the ""non-caucasian"" group but as stated above, it also appears to be pretty fair, given that its error rate is the same across the board"	Very useful	Though a 35% error rate isn't ideal, it seems like a fair and helpful tool, though it seems to also need improvement and secondary/backup methods	Neither fair nor unfair	I'm not sure I can call this fair, given the disparity in error rate; however, I'd also feel weird about calling accepting all payments equally something unfair, so I guess I withhold judgement.	Very unbiased	It's not biased by group, but also not by learned likelihood of result.	Mostly ununsable	Doesn't seem to do a good job screening	No		Probably model X	model x if I had to choose, since its results seem to be tied to real figures	Probably model X	model x seems to have adjusted what it accepts well, given that its rate of error is the same across the board	Probably model X	its rate of error is the same across the board	Probably model X	It's more useful	Probably model Z	equal mistakes on allowed payments	Probably model Z	slightly lower error rate overall	Probably ${e://Field/pref_model}	flat rate on mistaken allowed			Moderate	There may be negative consequences to being unable to pay a bill, or at least to pay it in a timely fashion			Moderate	They think they've paid a bill and will now have to come up with the money again unexpectedly			High	People who can't rent tend to be homeless, which is good for nobody			Low	Housing is a basic human right			Yes			No			Yes			Advantaged	Secondary Education	Others	6175f1e10f3ea7e92d287b6b	618b3e1d59be0d9e09f6f943	618c1d627ab0cec725ea60ff	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Advantaged	618c1d627ab0cec725ea60ff	APPROVED	2021-11-10 19:28:40.312000	2021-11-10 20:03:36.786000	2096.474	21.0	186	1	99	2021-11-14 18:53:33.352000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	DATA EXPIRED	United States	Female	DATA EXPIRED	
176	2021-11-10 12:44:38	2021-11-10 13:03:47	IP Address	70.120.127.237	100	1148	True	2021-11-10 13:03:47	R_1NhoIIwYk2cEPjl					32.96099853515625	-96.98410034179688	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The beds seem pretty evenly split, considering population demographics.	Acceptably biased	people are biased, people are the ones who create models. If the model is based on past ICU needs, then any bias in past ICU placements would filter into the model.	Very useful	Seems like a good, non-emotional way to allocate beds	Acceptably fair	The ICU beds seem fairly even shared.	Mildly unbiased	Almost all models are biased because people are biased and people are the ones who create models.	Very useful	Seems like a good, non-emotional way to allocate ICU beds	No		Probably model Y	Seems like a more even allocation of beds	Models X and Y are equally biased	people create models, therefore all models are biased. 	Models X and Y are equally useful	They both seem to fairly allocate beds	Probably model Y	Seems a bit more equal in allocation of beds	Probably model Z	it's a bigger deal to make a mistake denying a bed, than to make a mistake allocating a bed. Model Z has a better ratio of mistakes among denials	Probably ${e://Field/pref_model}	There are way less mistakes among caucasian denials than non-caucasian denials	Definitely model Z	Seems less biased against non-caucasians	Low			To mistakenly be given an ICU bed that you don't need is probably not going to hurt you.	High			To mistakenly NOT be given an ICU bed that you do need could result in avoidable death.	High			Society would be other patients who might need beds. If too many people are mistakenly given beds that DON'T need them, then society might be short ICU beds for people who DO need them. Which could have severe consequences for society.	Moderate			Society would be other patients who might need beds. If beds go unfilled, that leaves more space for other members of society.	No			Yes			Yes			Advantaged			Secondary Education	Others	61097db4d04095b7e7d43c11	618b3dcb32ad5f5cc3f00d86	618c21214475dea3785ea2af	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c21214475dea3785ea2af	APPROVED	2021-11-10 19:44:36.632000	2021-11-10 20:03:49.528000	1152.896	35.0	158	0	100	2021-11-13 02:06:38.878000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Caucasian	English	United States	Female	No	
177	2021-11-10 12:22:32	2021-11-10 13:07:09	IP Address	65.78.3.145	100	2677	True	2021-11-10 13:07:10	R_2qgAODGvXW9AWqU					40.78199768066406	-73.99530029296875	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	"I'm a bit confused as to whether the probability of granted access = properly granted access for medical personally or granted access to anyone of that demographic, even non medical staff? 
I declared it mildly unfair because it seems to have an even rate of error between both parties so it doesn't feel entirely unfair BUT if the data above is in fact declaring it confuses caucasians faces more and therefor allows access to non-medical caucasian folks regularly then it is quite unfair/bad system. "	Very biased	If I understand correctly and its letting less non-caucasian folks in but erring at the same rate it is quite biased in that it seems to better serve a caucasian demographic. 	Mostly unusable	I suppose in caucasian heavy facilities- which I suppose are disproportionately greater than a diverse facility in USA (sad)- it could be useful but how discouraging to non-caucasian folks who do hold space in these facilities. I say, craft a space that hold room for change and diversity and this seems to halt progress to that idea by predominantly serving the same community that exists. 	Acceptably fair	Even the gap! Advance tools that progress, support, validate non-caucasian folks. I do think its flawed but in my opinion, non-caucasians experience much more strain on their mental health than caucasians in the work place. they deserve the recognition and ease. I absolutely find this more fair than the previous model. 	Acceptably biased	If both models are biased but towards different groups I believe the group with greater strain in every other sphere of life deserves even just one ounce of ease. Anywhere they can get the break. People with privilege should step down so others can step up to meet them. This is one tiny tiny step. 	Neither useful nor unusable	Again, this is hard to define. A facility with an overwhelming caucasian population would find less benefit from the use but I don't find it unusable.	No		Probably model Y	It becomes a race issue and I believe the marginalized group should be supported. 	Models X and Y are equally biased	I think they're equally biased but I believe in the bias of Y rather than X. 	Probably model Y	Considering there is a slight majority of caucasian doctors, my inclination was to say that x is probably more useful as it serves the majority but when I looked up a quick stat sheet on doctor demographics, seeing that it is a slight majority I think model Y is the better investment/use. People of color are slowly holding spaces they've always deserved to a greater degree so in theory, non-caucasians could hold a majority in the very near future. 	Definitely model Y	same as above. 	Probably model Z	46% is a large amount of error, adding more difficulty to a marginalized group with already existing pressures/expectations etc 	Probably ${e://Field/pref_model}	same as above 	Probably model Z	same as above 		Moderate		Maybe im not well versed in the depths of criminal activity that goes on from stealing peoples health information but it feels the likelihood of someone stealing someones info through this system is minimal and the consequences small		Moderate		Identity theft is a concern but I wonder how many people would actively attempt to do that? likely small. identity theft is no small crime but maybe im optimistic in thinking it would rarely happen		Low		It would create little impact, it doesn't change much more than a few minutes of time in a day 		Moderate		"same answer as the last page- 
fear of identity theft = quite bad 
likelihood = not too high?
"		Yes			Yes			Yes			Advantaged		Bachelor	Others	60fed37592fa85f71f75952a	618b3e1d59be0d9e09f6f943	618c1bf4ad556d59e7ca834c	minority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c1bf4ad556d59e7ca834c	APPROVED	2021-11-10 19:22:30.568000	2021-11-10 20:07:13.663000	2683.095	27.0	41	0	100	2021-11-14 02:57:42.556000	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Female	No	
178	2021-11-10 12:51:11	2021-11-10 13:08:46	IP Address	66.71.114.62	100	1055	True	2021-11-10 13:08:47	R_1etosB6yEWjT4p2					39.23680114746094	-77.27749633789062	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	error is even	Neither biased nor unbiased.	it does not try to base on race	Mostly useful	lower error and better acceptance percentages 	Mildly unfair	if the mistake ratio of caucasian to non- caucasian isn’t close enough to 1	Neither biased nor unbiased	i don’t think it works that way on purpose	Mostly ununsable	the margins for error are too big	No		Probably model X	error is equal	Models X and Y are equally biased	both have discriminations bases on race	Probably model X	x because the level of failure is less	Definitely model X	it seems more reasonable  	Definitely ${e://Field/pref_model}	better acceptance and even rejection 	Both ${e://Field/pref_model} and Z are equally biased	both deal with race	Definitely ${e://Field/pref_model}	more fair in both rejection and acceptance		High		the records could be needed to ensure the health of a patient 		Moderate		who knows what people could do with that type of information 		Moderate		it has little effect on a larger group of people compared to just one		High		the amount of personal data of many people can now be accessed by the public 		No			Yes			Yes			Advantaged		Secondary Education	Others	60fee9d3e4334a1f038d472a	618b3dcb32ad5f5cc3f00d86	618c22a1df92be07482ed4db	majority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	618c22a1df92be07482ed4db	APPROVED	2021-11-10 19:51:09.397000	2021-11-10 20:08:51.043000	1061.646	20.0	110	0	100	2021-11-12 17:40:21.381000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
179	2021-11-10 12:51:54	2021-11-10 13:09:35	IP Address	50.49.104.191	100	1060	True	2021-11-10 13:09:35	R_2TEJUq7TP1CyHaH					43.09080505371094	-77.43479919433594	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	The percentage of mistakes for non-Caucasian is nearly all the percentage of the non-Caucasian probability they will need it. If the mistake percentage ends up accurate that's nearly all the non-Caucasian patients that could be affected if this model is chosen. On the flip side if those chosen do not need to be put into the ICU then its a wasted bed.	Neither biased nor unbiased.	A computer shouldn't have a bias	Mostly unusable	I think its too simple of a model with not enough to determine who will need a bed	Very unfair	If all of the Caucasians were put into ICU and then didnt need it then theres a probelm.	Neither biased nor unbiased	I dont think its biased, it depends on where its getting its info	Neither useful nor unusable	I think their specific diseases/illness should be used rather than their ethnicity	Yes	I misunderstood the model at first, I would have just reworded some stuff	Probably model Y	Less mistakes 	Models X and Y are equally biased	They shouldnt use race as a way to firgure out who would be in the ICU	Probably model Y	Seems like less percentage of mistakes	Probably model Y	Seems like it may be a bit more accurate	Probably model Z	equal amount of percentage of mistakes	Probably ${e://Field/pref_model}	higher mistake of non-caucasian	Probably model Z	seems more accurate	High			if these predicted percentage that will need ICU support is inaccurate you are reserving ICU beds witha chance that they will not need it	High			If you mistaken that someone will not need ICU, give away the bed , then they need it there is an obvious issue	High			"""Society"" being ill patients it is a significant issue due to the fact that they might waste an ICU bed"	High			No ICU bed for someone who needs it from society(ill people) means death	No			No			Yes			Advantaged			Secondary Education	Others	6160d68ce187e1c1d6b50007	618b3dcb32ad5f5cc3f00d86	618c22c1b8f2eaa4b5e7788a	minority	outcome-fpr	icu	True	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c22c1b8f2eaa4b5e7788a	APPROVED	2021-11-10 19:51:40.769000	2021-11-10 20:09:40.504000	1079.735	26.0	153	0	100	2021-11-12 17:56:34.785000	19AE28A9	United States	United States	Full-Time	Caucasian	English	United States	Female	No	
180	2021-11-10 12:48:25	2021-11-10 13:09:57	IP Address	97.73.244.255	100	1291	True	2021-11-10 13:09:57	R_24iTU8178HZER8T					47.6033935546875	-122.34140014648436	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very fair	the consequences of the machine learning mistakes are balanced evenly among races	Very unbiased	no race receives preferential treatment	Very useful	it is unbiased in its inaccuracies	Very unfair	The consequences of not admitting certain patients to ICU beds disproportionately affects people of color	Very biased	Model Y is biased towards White people	Completely unusable	Model Y gives preferential treatment based on race	No		Definitely model X	because the consequences are balanced among racial participants	Definitely model Y	it shows preferential treatment for white people	Definitely model X	unbiased mistakes	Definitely model X	it is the most fair model	Definitely model Z	i would rather there be an imbalance of people granted asccess to the ICU that did not need it, than denied access to the ICU who really did need it.	Definitely ${e://Field/pref_model}	racial disparities in the outcomes of who is denied treatment in the ICU that actually needs it	Definitely model Z	mistakes of accidental ICU denial are fair across races	High			granting ICU support to someone who does not need it means that someone else who does will be denied	High			consequences of death	High			someone who needs treatment will be denied access, affecting their families and loved ones if they pass away	High			someone who is denied access will likely die, affecting their family and loved ones	Yes			No			Yes			Advantaged			Secondary Education		60fdf56ab1e177e2b4b025db	618b3dcb32ad5f5cc3f00d86	618c21fb031343a79427622c	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	618c21fb031343a79427622c	APPROVED	2021-11-10 19:48:18.820000	2021-11-10 20:10:12.586000	1313.766	32.0	38	0	100	2021-11-13 01:56:22.409000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	Yes	
181	2021-11-10 12:57:21	2021-11-10 13:14:03	IP Address	23.117.32.37	100	1002	True	2021-11-10 13:14:04	R_2sckKgf0EO9c8DV					34.482696533203125	-118.62539672851562	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It has the same mistake percentage for caucasian versus non caucasian but I do not think race should be a factor in its judgement. 	Very biased	It is using race as a factor to make its decision, I do not think that is right.	Mostly useful	It is helping weed out potential fraud, but its parameters are unfair.	Acceptably fair	Since the chart has the same percentage for caucasian versus non caucasian applicant, I do not think race went into the algorithim. 	Mildly unbiased	The percentage for race was even.	Mostly useful	It is preventing potential fraud.	No		Definitely model Y	Race is not going into the decision.	Definitely model X	Model X is factoring in race.	Definitely model Y	Their percentage of mistakes is less, and race is not a factor in the decision.	Definitely model Y	I would not want to be a factor, and had 1 percent better rate of mistakes.	Probably ${e://Field/pref_model}	Has an even distribution for applicant of different races.	Probably model Z	Has a higher percentage of accepting non-caucasian applicants.	Probably model Z	Has a lower mistake rate.			Low	Their life is not directly affected by not being able to rent. 			High	They will be out money if the payment is fraudulent.			High	The society will not be able to rent and could be homeless if they do not have another option. Society is the person trying to rent.			Low	They will be the ones making the fraudulent payment, obviously could be in legal trouble after that.			Yes			Yes			Yes			Disadvantaged	Master	Others	5d3b9f531850400001d2bac9	618b3dcb32ad5f5cc3f00d86	618c2410849296864ce4102f	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c2410849296864ce4102f	APPROVED	2021-11-10 19:57:19.123000	2021-11-10 20:14:09.674000	1010.551	27.0	394	0	100	2021-11-13 01:29:17.349000	19AE28A9	United States	United States	DATA EXPIRED	Caucasian	English	United States	Female	DATA EXPIRED	
182	2021-11-10 12:45:58	2021-11-10 13:16:12	IP Address	98.164.88.240	100	1813	True	2021-11-10 13:16:12	R_pzTInvge9YE6Ya5					29.973403930664062	-90.0885009765625	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very fair	The mistake rate is exactly the same, indicating lack of racial bias. 	Very unbiased	The mistake rate is exactly the same, indicating lack of racial bias. 	Neither useful nor unusable	The mistake rate is too high.	Very unfair	It has double the mistake rate among non-Caucasian people.	Very unbiased	It has double the mistake rate among non-Caucasian people.	Neither useful nor unusable	The mistake rate is biased.	No		Definitely model X	The mistake rate is the same.	Definitely model Y	The mistake rate favors Caucasian subjects.	Models X and Y are equally useful	Their probability of granting access and mistake rates even out.	Neither model X nor model Y	Both have mistake rates too high.	Definitely model Z	Model X denies more of one group, where model Y ACCEPTS more of one group.	Probably ${e://Field/pref_model}	Because it mistakenly denies more of one group than the other.	Neither ${e://Field/pref_model} nor model Z	Both have a high rate of falsely granted access.		Low		Typing the info is very easy and accessible.		High		Unauthorized users can cause great harm.		Low		The person can simply type the info. Society would include hospital staff, patients, patients' families, hospital board, community, etc.		Moderate		A bad actor could get data or cause harm to more than just a single patients. Society would include hospital staff, patients, patients' families, hospital board, community, etc. So it would have impacts on staff(repercussions), patient(injury or data breach), shareholders(loss of profit or lawsuits).		No			No			No			Advantaged		Master	Engineering and Technology	600b283694c55c15f17e744c	618b3dcb32ad5f5cc3f00d86	618c2172d3b92fc44c25b3df	majority	outcome-fpr	frauth	False		model X	top								Advantaged	618c2172d3b92fc44c25b3df	APPROVED	2021-11-10 19:45:56.395000	2021-11-10 20:16:14.852000	1818.457	37.0	457	0	100	2021-11-12 17:01:26.566000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Male	No	
183	2021-11-10 13:00:39	2021-11-10 13:28:20	IP Address	99.42.243.70	100	1660	True	2021-11-10 13:28:21	R_2CHyiouN9Z5bVjJ					29.897293090820312	-95.64630126953124	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Unlike Model Y, the percentages are closer to eachother when looking at the mistakes. However, there is a 26% difference in white people to non white people. That is a pretty big number. 	Acceptably biased	As stated above. 26% is a pretty big difference. White people are still getting accepted higher than non white. 	Neither useful nor unusable	I feel like it could be useful if it was not showing favor. 	Mildly unfair	The mistakes has gone down rather low among the non-caucasian. Meaning that white people are showing lesser percentage than non white. Seems pretty bias to white people. 	Very biased	As stated above, the percentage barely budged towards white people and dropped 24% towards those who are not white. 	Mostly ununsable	Seems the data put in is only favorable to white people. The data could be completely wrong and therefore unusable. Especially since it is showing favor to white. 	No		Probably model Y	Model Y is more favorable because the percentage of allowed payments are the same. 	Probably model X	There is a 26% difference in Model X when it comes to accepted payments. Almost like the non white are barely getting a chance to even be accepted. 	Probably model Y	Model Y could be more useful simply because the percentage of acceptance is the same compared to the difference in Model X.	Probably model Y	As stated previously, the payment of acceptance, to me, looks better. 	Probably model Z	They both are pretty unfair, but because they have the same denied payments, Z is better. 	Probably model Z	Z has a lower percentage of acceptance towards non white. 	Neither ${e://Field/pref_model} nor model Z	I believe that they are both unfair and I would not want to choose either. They both are biased towards white. 			High	Depending on the renters situation, if they are denied that their payment goes through, they could possibly look at being kicked out for the mistakes of the system. 			High	For this, the person who owns could end up losing money. 			High	As for the society, I look at those are renting whether is be a home or a apartment. If the payment was mistakenly denied, it could end up looking bad for those who are renting and put them in a tough spot. 			High	As the owners being the society, it could cause trouble in the economic world of renting. they could loose out on money that they should have gotten. 			Yes			No			Yes			Advantaged			614230bae101d5f2af19d6c7	618b3dcb32ad5f5cc3f00d86	618c24e1108f28b6c1233957	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c24e1108f28b6c1233957	APPROVED	2021-11-10 20:00:37.845000	2021-11-10 20:28:25.695000	1667.85	26.0	38	0	100	2021-11-12 17:05:42.854000	19AE28A9	United States	United States	Other	Caucasian	English	United States	Female	Yes	
184	2021-11-10 12:49:52	2021-11-10 13:38:41	IP Address	172.251.212.66	100	2928	True	2021-11-10 13:38:42	R_cAxXiZ89AVbUuQh					33.49479675292969	-117.09539794921876	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It has a more fair mistake rate than Model Y but considering the racial disparity between payment acceptance I think I would need more information about the denied payments and that mistake rate before I could make a proper judgement. Considering only the information provided, it doesn't look great.	Acceptably biased	"Same reasoning as why I think it is unfair, but I'll add that ""acceptable"" is not exactly how I would describe any bias. However, I can't call it ""very biased"" without more information."	Mostly unusable	It will not look good for the company if anyone notices the racial discrepancy. Some businesses may be willing to take that risk, but I wouldn't.	Very unfair	It seems to allow fraudulent payments from Caucasian people more often, hinting that it is a harsher judge of non-Caucasian people	Very biased	For the same reason: allowing fraudulent payments from Caucasian people more often implies that it is a harsher judge of non-Caucasian people	Mostly ununsable	Deploying biased machine learning models seems to be the standard business practice now, but I doubt it will be much longer.	No		Probably model Y	Compared to Model X, Model Y seems less likely to reinforce historical racial disparities.	Models X and Y are equally biased	It is hard for me to say for sure with this information.	Probably model Y	Model Y because I also think it is likely more fair.	Probably model Y	Model Y seems like the better choice with the information provided, mostly because it seems like it may be more fair.	Both ${e://Field/pref_model} and Z are equally fair	I can't decide. I feel like this could be mathematically determined if I knew how to do that kind of math	Probably ${e://Field/pref_model}	My original suspicion that it is a harsher judge of non-Caucasians appears to have been correct.	Neither ${e://Field/pref_model} nor model Z	I've seen more information about Y than Z, and I would hold out to know more about Z before deciding between the two.			High	This presents a severe hassle to the renter, who needs to work with the appropriate parties to resolve this issue, despite likely already having a busy schedule.			Moderate	This is unlikely to impact an individual as significantly as a mistaken denial, but it depends on the source of the payment and how quickly the issue is noticed			Low	For this purpose, society can be defined as the inhabitants of the country the renter lives in. One denied transaction has little impact on the whole of society.			Low	For this purpose, society can be defined as the inhabitants of the city the renter lives in. A single fraudulent payment has little impact on the rest of society.			Yes			Yes			Yes			Disadvantaged	Secondary Education	Engineering and Technology	614eacca1ae6e91a34e9658c	618b3dcb32ad5f5cc3f00d86	618c225ae05e127eec040f08	minority	outcome-fpr	rent	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c225ae05e127eec040f08	APPROVED	2021-11-10 19:49:50.156000	2021-11-10 20:38:44.217000	2934.061	30.0	122	0	100	2021-11-12 17:13:36.361000	19AE28A9	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Male	Yes	
185	2021-11-10 12:52:14	2021-11-10 13:38:54	IP Address	174.250.44.2	100	2800	True	2021-11-10 13:38:54	R_12sv6ecHRiLwZU9					37.751007080078125	-97.82199859619139	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	Because they are basing it upon race. There is bad in every race so why even use that as a category and single them out? And it's just Caucasian and then all the other races in one category, that is so racist, unfair, and wrong. 	Very biased	Again, it's still singling minorities out from the Caucasians. Why are they even doing that? Like Caucasians are different than other people? 	Completely unusable	Besides that it's morally wrong I think it would be inaccurate. I say this because there are equal amounts of each race that could possibly use fraudulent payments, I've seen it with my own eyes. 	Neither fair nor unfair	It's fair in a way that it gives everyone an equal chance. But still why is it divided into two categories? To make it completely accurate they should divide it into past history and then maybe some sub categories like male and female. And if they want to be racist they can even do a sub category beyond that like Caucasian, black, Asian, Hispanic. 	Very biased	I don't think it would be accurate per everything I said above and before. 	Neither useful nor unusable	It could be useful, yes. But to be completely useful and accurate they need to go by past history and then sub categorize that. 	No		Probably model Y	I'm going by the probability of allowing payments. Since they are both equal that would be the choice that is most fair to both categories, but I don't think that any of these models are fair. 	Models X and Y are equally biased	Because they are basing it off race. It's even more biased that they only separated by it into two categories like white people are different than everyone else. 	Probably model Y	Because both categories get an equal chance and if they detect fraud then they could go from there. 	Neither model X nor model Y	Because it's biased and racist. Since computers are so intelligent, I would think they would come up with a better plan. 	Probably ${e://Field/pref_model}	Because they are allowing equal payments 	Both ${e://Field/pref_model} and Z are equally biased	Because it's being judgemental to both categories. 	Neither ${e://Field/pref_model} nor model Z	I would come up with a better solution: past history, male and female, and then sub categories like age and then each individual race. 			High	This can cause them to lose their home, their shelter and it affects everyone else that lives in the house. 			High	Because it causes harm to the person who is being stolen from. 			High	This can affect the other people living in the house like their kids. And it's a domino affect, it will cause stress to everyone else. For example the individual could be stressed and the kids feel stressed therefore they perform poorly in school. 			High	This will affect the individual along with their family. It can cause stress on the family. For example missed car payments, or them even loosing their own shelter because they don't have the funds to have a roof over their head. 			Yes			No			Yes			Disadvantaged	Secondary Education	Administrative Staff	6120d602c1976a4114dcb110	618b3dcb32ad5f5cc3f00d86	618c22e555e1deb8b6254418	majority	outcome-fpr	rent	True		model Y	bottom								Disadvantaged	618c22e555e1deb8b6254418	APPROVED	2021-11-10 19:52:11.945000	2021-11-10 20:39:00.535000	2808.59	31.0	111	0	100	2021-11-13 02:10:57.424000	19AE28A9	United States	United States	Due to start a new job within the next month	Caucasian	English	United States	Female	No	
186	2021-11-10 12:57:33	2021-11-10 14:04:52	IP Address	152.13.121.80	100	4039	True	2021-11-10 14:04:53	R_10urgax7XqwEVYI					35.291595458984375	-80.82009887695312	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Mildly unfair	there is a large gap in the probability of granting access	Acceptably biased	it does have an issue with probability of granting access but there is no difference in the rate that it makes mistakes.	Neither useful nor unusable	leaning slightly more to useful but not mostly useful. There are still a fairly large percentage of mistakes.	Acceptably fair	it has an identical probability of granting access, but there is a discrepancy in the ratio of mistakes.	Acceptably biased	there are more mistakes when identifying Caucasian individuals than non Caucasian individuals 	Mostly ununsable	it makes a significant number of mistakes in identifying doctors.	No		Probably model Y	there is not difference in the probability of access being granted 	Probably model X	there is a large gap in the probability of access being granted	Probably model Y	it is more fair and has a lower average rate of mistake	Probably model Y	It seems more fair and less biased overall	Probably model Z	it makes an even number of mistakes between the groups	Probably ${e://Field/pref_model}	there is a very large gap in the number of mistakes made	Probably model Z	even though the probability of y is more even, z makes fewer mistakes overall 		Low		the staff should know their password to access the system, so at most it is an inconvenience and may take an extra minute		High		mistaken access could leave the individuals information vulnerable, and medical records are private information.		Moderate		putting in the password could cause some efficiency issues in the hospital, so patients aren't treated in the most effective way possible. 		High		access to a hospital system could put so many people's information at risk without them knowing, and the impacts could be far reaching 		No			Yes			Yes			Advantaged		Secondary Education	Education	60fcb64c52cb3e4c9ec0c76b	618b3dcb32ad5f5cc3f00d86	618c24293795b5173c18631b	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c24293795b5173c18631b	APPROVED	2021-11-10 19:57:31.345000	2021-11-10 21:04:55.357000	4044.012	20.0	108	0	100	2021-11-12 17:37:27.769000	19AE28A9	United States	United States	Part-Time	Caucasian	English	United States	Female	Yes	
187	2021-11-10 13:20:43	2021-11-10 14:07:34	IP Address	160.19.5.106	100	2811	True	2021-11-10 14:07:35	R_sTJhKDn0svznjDr					37.05780029296875	-76.45999908447266	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	not sure.	Mildly unbiased	not sure.	Mostly useful	looks like it works for the most part.	Mildly unfair	unsure.	Very biased	unsure how to answer.	Mostly useful	It works.	No		Definitely model Y	It looks more reliable based on granted access.	Probably model X	I just think so.	Definitely model Y	they grant access evenly.	Definitely model Y	Because it's more workable and unbiased.	Probably model Z		Probably ${e://Field/pref_model}		Probably model Z			High		It's very important to be able to recognize an authorized staff as it can be unsafe.		High		Very unsafe and scary.		High		Everyone in and around the hospital will be impacted if the system isn't accurate.		High		the community where the hospital is can have unsafe consequences.		Yes			Yes			Yes			Disadvantaged				617078fb6641a57cb762a401	618b3e1d59be0d9e09f6f943	618c2995323373344dcf0ac2	majority	outcome-fpr	frauth	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c2995323373344dcf0ac2	APPROVED	2021-11-10 20:20:40.877000	2021-11-10 21:07:37.556000	2816.679	40.0	189	1	100	2021-11-14 03:08:39.385000	19AE28A9	United States	United States	Not in paid work (e.g. homemaker', 'retired or disabled)	Non-Caucasian	English	United States	Female	No	
188	2021-11-10 13:02:56	2021-11-10 14:09:21	IP Address	173.168.222.19	100	3985	True	2021-11-10 14:09:22	R_11bY6N00XXs4u8G					27.958999633789062	-82.4937973022461	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	This model clearly is able to recognize Caucasian personnel by 24% more than their non-Caucasian personnel. While model Y granted an almost 50% mistaken access to Caucasian non medical personnel, Model Y had a closer percentage in successfully granting access to the two groups.	Acceptably biased	A 41% non-Caucasian success access to medical personnel compared to an almost 70% success access to their Caucasian counterparts is biased in my opinion as it especially when mistaken access to both groups are both at 35%.	Neither useful nor unusable	While the bias in the two groups are shown, the mistaken access for both groups are at 35%, which with more work can get that percentage lower meaning patient files won’t be corrupted.	Mildly unfair	Almost half of people logging in (who are medical personnel) will successfully be granted access to files while almost half of non medical personnel will be mistakenly granted access to files.	Mildly unbiased	The Caucasian data shows a 45% probability of granting access mistakenly to non medical personnel who are also Caucasian while non-Caucasian’s will only be mistakenly granted access by almost half of that percentage (24%).	Mostly ununsable	Model Y can become useable but right now with the data shown it is not. While half of Caucasian medical personnel will successfully be granted access, an almost 50% chance non-medical personnel can also mistakenly be granted access to those same files, making it unsafe to those patients files being compromised.	No		Models X and Y are equally fair	"Model Y shows a 53% successful access to both groups and a disadvantage of non-medical Caucasian’s getting access by 21% more than the non-medical non-Caucasian.
Model X shows a 35% mistaken access to both groups, with a disadvantage of non-Caucasian medical personnel being successfully granted access 24% less than their Caucasian counterparts."	Models X and Y are equally biased	Both are biased on different sections.	Probably model X	I decided Model X because of the mistaken access being 35% instead of Model Y where one ethnic group has a higher percentage of being mistakenly granted access than another.	Probably model X	Ultimately the model with the less percentage of having files accessed within both groups will be chosen because of the sensitivity of patient records.	Both ${e://Field/pref_model} and Z are equally fair	Both models are fair in one way or another. One model shows a denied access of 32% in both groups and another model shows a mistaken access of 35% in both groups.	Both ${e://Field/pref_model} and Z are equally biased	Both are equally biased, as Model C shows a bias of denied access to medical personnel of 23% more in non-Caucasians than their counterparts. In Model z, the mistaken access of non-medical Caucasian’s is 21% more than non-medical non-Caucasian’s.	Probably ${e://Field/pref_model}	Model X would still be my choice because of the 35% mistaken access to both groups rather than one group being mistakenly given access of more than 5% of that.		Moderate		I say moderate because whatever the situation is, being dependent on a system recognizing you (whether it be through facial recognition or having a system “remember” you password) means in the future not knowing that important information.		High		Patient files are sensitive and private and a system should not be allowed to mistakenly grant access to unauthorized personnel 		Moderate		As society, considering the medical professionals, the time to put in a username/password can take time away from potentially saving a human beings life but can mean a lesser chance of unauthorized persons being able to access files.		High		As society, especially the people who value privacy, mistakenly giving access to people who are not authorized is a high impact because they hold sensitive information.		Yes			No			Yes			Advantaged		Primary Education	Others	611137fbc2ddd7e9020a3051	618b3e1d59be0d9e09f6f943	618c256c850a75dabcadb094	minority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Advantaged	618c256c850a75dabcadb094	APPROVED	2021-11-10 20:02:54.319000	2021-11-10 21:09:25.619000	3991.3	21.0	24	0	100	2021-11-14 18:30:03.510000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	DATA EXPIRED	
189	2021-11-10 13:17:19	2021-11-10 14:11:21	IP Address	75.182.136.42	100	3241	True	2021-11-10 14:11:21	R_3lxC9eJuufCGlLY					35.81230163574219	-78.54689788818358	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	The predictions for caucasian patients were close but non-caucasian patients had a very large discrepancy.	Very biased	they prediction for non-caucasian has a much larger margin of error than for caucasian patients.	Mostly unusable	For one whole section of patients the data is not close to being as accurate as it is for the caucasian patients.	Very unfair	Again the prediction is skewed when it comes to non-caucasian patients.	Very biased	It overpredicts more towards the non-caucasian patients.	Neither useful nor unusable	For caucasian patients the data can be useful, but is very off for non-caucasian patients.	Yes	I believe model X to not be as skewed towards one specific demographic as I first thought.	Models X and Y are equally fair	There predictions each tend to be close to the actual numbers in a certain demographic but each is way off on the other demographic.	Models X and Y are equally biased	They each skew at least 30 per cent off for one demographic and around 8 per cent off on the other demographic.	Models X and Y are equally useful	They are off by similar percentage points for each demographic.	Neither model X nor model Y	Neither one is better or worse than the other and each one is skewed wildly in favor of one set of patients.	Both ${e://Field/pref_model} and Z are equally fair	Their probability was equally as inaccurate for each model.	Both ${e://Field/pref_model} and Z are equally biased	Each model was equally wrong in predicting probability for each demographic of patient.	Neither ${e://Field/pref_model} nor model Z	Both had similar percentage points in error.	High			If ICU support is given when not needed it can result in more financial cost to the individual patient's care.	High			From an individual standpoint it could mean the difference between life and death.	High			One individual's care cost will be felt as a whole on society if it puts too much of a financial strain on the individual.	High			For the family of the individual, if that person dies, the loss to the family unit and society as a whole can be immeasurable.	No			No			Yes			Disadvantaged			Bachelor	Others	611d42472542f0906bc3b54b	618b3e1d59be0d9e09f6f943	618c28c73949ac8827de7df4	majority	outcome-fpr	icu	True		model Y	bottom								Disadvantaged	618c28c73949ac8827de7df4	APPROVED	2021-11-10 20:17:17.327000	2021-11-10 21:11:28.205000	3250.878	52.0	89	0	100	2021-11-14 17:31:33.357000	19AE28A9	United States	United States	Due to start a new job within the next month	Non-Caucasian	English	United States	Female	No	
190	2021-11-10 13:51:41	2021-11-10 15:11:30	IP Address	24.3.138.99	100	4788	True	2021-11-10 15:11:31	R_0e52KSqfitxuJdn					40.4324951171875	-79.86299896240233	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The probability of a non caucasian person being granted their access is lower than it is for caucasian people.	Acceptably biased	The machine seems to be more accurate when it's caucasian people utilizing it.	Neither useful nor unusable	I'm not sure if the ratio similarities within the mistakes of access is better or worse than one being higher/lower than the other.	Mildly unfair	The probability of granting access is where it should be however the mistake only among granted access makes it slightly more difficult for caucasians to avoid those mistakes from model Y.	Acceptably biased	Model Y's percentage of mistakes with granted access seems relatively high and only applies to one race.	Mostly useful	The improvements needed seem minor regarding the mistakes with access.	Yes	The first model is better than the second one regarding its accuracy.	Probably model Y	I'm not confident in my answer but the probabilities for both caucasian and non caucasian people being granted their access is even.	Probably model X	This is a hard comparison but maybe because of the ratio of mistaken granted access.	Models X and Y are equally useful	I'm struggling trying to decipher which is better.	Probably model Y	That's what my gut is telling me.	Both ${e://Field/pref_model} and Z are equally fair	I'm indecisive and don't know which ratio is more important. Rather, which ratio differences can afford to be prioritized.	Both ${e://Field/pref_model} and Z are equally biased	See above comment.	Probably ${e://Field/pref_model}	Accuracy is vital and should be implemented.		High		High because it would be inconvenient during emergency situations, causing delays, issues with communication and documents. Hopefully with reliable hospital software, a reset or manual log would be an available alternative. 		High		"Hospitals keep a mass amount of confidential documents as well as drugs, metal instruments and other things that shouldn't be that easily accessible. 
"		Moderate		Still very inconvenient but the username/password back up is a great safety net  for when the facial recognition fails.		High		It puts confidential information, drugs and medical tools at riak.		Yes			Yes			Yes			Disadvantaged		Master	Others	6111202e9250c75296f411bf	618b3e1d59be0d9e09f6f943	618c30cb7cee7afafdd15356	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c30cb7cee7afafdd15356	APPROVED	2021-11-10 20:51:27.701000	2021-11-10 22:11:33.182000	4805.481	25.0	166	1	100	2021-11-14 18:23:24.400000	19AE28A9	United States	United States	Full-Time	Non-Caucasian	English	United States	Female	Yes	
191	2021-11-10 15:16:25	2021-11-10 15:56:34	IP Address	73.73.206.115	100	2408	True	2021-11-10 15:56:34	R_2D2mVge6RrgQTZr					41.7467041015625	-87.720703125	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	A larger percentage of one group were allowed access over the other, even though there were the same percentages of mistakes. Although I would want to know the overall number of subjects. 	Neither biased nor unbiased.	I cannot speak to the bias of the model because it is a machine and is doing what is was programmed to do. 	Neither useful nor unusable	I cannot say how useful because the mistake percentages are equal on both sides. 	Mildly unfair	Because we are having a higher  percentage of mistakes with one group than the other. 	Neither biased nor unbiased	Cannot say it is a machine doing what it was programmed to do. 	Neither useful nor unusable	I believe the purpose and idea behind it is very useful but application of the idea needs work.	No		Models X and Y are equally fair	They are both problematic in their opperations.	Models X and Y are equally biased	They are both problematic in their opperations.	Models X and Y are equally useful	I do not feel either are useful. 	Neither model X nor model Y	Because the level of mistakes on both sides are significant. 	Definitely model Z	Because the same group-wise percentage were denied access	Definitely ${e://Field/pref_model}	The group-wise percentage are severely different	Probably model Z	The overall percentages denied 		Low		They still have the ability through passwords to gain access		High		Unappropriate access to medical records		Low		The institution (Hospital and society overall) do not have to be concern with personal and private information being access		High		The institution (Hospital and society overall) do have to be concern about what is wrongfully being accessed		Yes			Yes			Yes			Disadvantaged		Master	Education	6167bd607a81dede537aa23b	618b3e1d59be0d9e09f6f943	618c44b06876195a241da92a	minority	outcome-fpr	frauth	True		model Y	bottom								Disadvantaged	618c44b06876195a241da92a	APPROVED	2021-11-10 22:16:21.936000	2021-11-10 22:56:38.352000	2416.416	47.0	14	0	100	2021-11-14 18:51:28.242000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	Yes	
192	2021-11-10 15:55:09	2021-11-10 16:19:59	IP Address	98.11.217.138	100	1490	True	2021-11-10 16:19:59	R_2Rb44k0w0cZM8XT					43.11399841308594	-77.56890106201172	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	More male applicants will be accepted for admission and fail than female applicants.	Figure 2	Mildly unfair	The probability is not equal 	Acceptably biased	It is just starting facts about the probabilities but it is also stating that Caucasian individuals will have a harder time predicting the probability.	Mostly useful	It is useful so that the company can talk about the results and better look at ways to get their workers to be equal in predicting ICU use 	Very unfair	The amount of mistakes for non-Caucasian ICU supports is way higher than Caucasians which proves that there is some sort of bias and unjust and unfairness with this model 	Acceptably biased	Due to the rates of non caucasians being higher than Caucasians 	Mostly useful	This can be useful to let the company know what is going on and how this system is flawed and implement and talk about ways to fix it 	Yes	I did not really understand what the graphs were showing until having to look at a second diagram and reading about it again so I would probably change weather or not I thought it was biased or not	Probably model Y	The percentages are closer together 	Probably model X	Because there is a difference in how well certain people can predict icu use	Models X and Y are equally useful	They are both useful to learn from and to get information from to better teach and advance people working	Probably model Y	Because it gives and equal amount of predication to both parties and still lays out the mistakes fairly 	Probably ${e://Field/pref_model}		Probably model Z		Probably ${e://Field/pref_model}		High			This could put someone’s life in danger if they are not given the care they need	Moderate			They might not know the reasonings behind it	Moderate			"Because this will impact healthcare for a lot of people 
Society being health care individuals ands well as people in society "	Moderate			People who aren’t involved in health care probably won’t care as much 	No			Yes			Yes			Disadvantaged			Master		60fda319d6d843f4541fb694	618b3e1d59be0d9e09f6f943	618c4dc75ee81215ec553f0a	majority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c4dc75ee81215ec553f0a	APPROVED	2021-11-10 22:55:06.651000	2021-11-10 23:20:03.024000	1496.3729999999998	23.0	164	2	99	2021-11-14 17:04:19.417000	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Female	Yes	
193	2021-11-10 15:54:31	2021-11-10 16:26:39	IP Address	104.175.128.116	100	1928	True	2021-11-10 16:26:40	R_pmxVHpdGa5Py1Xz					33.91900634765625	-117.23590087890624	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Because it is basing it on race, and the probability of mistakes for allowed payments is equal in both categories.	Acceptably biased	Because it is targeting the customers by race rather than a different method.	Mostly unusable	It should find a different method of targeting the fraudulent payments. 	Neither fair nor unfair	Because it targets equally the races but it does not fully solve the problem	Neither biased nor unbiased	It shows that the group that should be targeted is caucasian. 	Mostly useful	It shows that the model needs to me fixed to meet the response of the fraudulent checks. 	Yes	That it was solely based on race.	Definitely model X	Because it has less mistakes.	Probably model Y	Because it led to more mistakes 	Probably model X	Because it leads to less chances of receiving fraudulent checks and find new methods of 	Probably model X	At the end, being a little bias leads to less mistakes on the end and that would benefit the company. 	Both ${e://Field/pref_model} and Z are equally fair	It seems like the probabilities are almost the same.	Both ${e://Field/pref_model} and Z are equally biased	Both models have the same amount of mistakes, or around, simply one is mistakes for denied checks and the other is mistakes for approved checks. 	Neither ${e://Field/pref_model} nor model Z	I think they both need to be worked on to get a better result for less mistakes. 			High	It can negatively impact the individual 			Moderate	Although it is bad to allow a fraudulent check, it will be fixed but not if you falsely accuse someone of fraudulent checks. 			Moderate	The rental company moderately gets impacted because it can simply find a new renter if things go bad with the other renter. 			Low	The rental company will probably not lose much from allowing the fraudulent check and will most likely find. a way to get it fixed. 			Yes			No			Yes			Disadvantaged	Secondary Education	Administrative Staff	6127a2e8e9b8e4eae5fc48f4	618b3e1d59be0d9e09f6f943	618c4da247ddd00edd37fc71	minority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	618c4da247ddd00edd37fc71	APPROVED	2021-11-10 22:54:30.098000	2021-11-10 23:26:45.548000	1935.45	26.0	110	1	99	2021-11-14 18:48:25.847000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Female	No	
194	2021-11-10 16:08:04	2021-11-10 16:31:41	IP Address	24.189.210.36	100	1417	True	2021-11-10 16:31:41	R_3lzaFlOJBpGkngO					40.7135009765625	-73.35459899902342	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Male applicants will be more likely to mistakenly be accepted than female applicants.	Figure 1	Very unfair	Caucasians have the same amount of risk for mistake but get payments allowed at a higher rate	Very biased	It implies that non caucasian folks are less likely to get approved payment than caucasians even though the mistakes are made at the same rate. 	Completely unusable	Will result in biased practices	Very fair	It allows an even probability of approved payment regardless of race	Very unbiased	Results are consistent with fair practices regardless of race.	Very useful	Allows to preemptively filter out some applicants without bias	Yes	I think I gained a better understanding of the message behind the graphs from the second example	Probably model Y	The difference in the rates is less severe in Y	Definitely model X	There is a standard set that isn't matched for all races	Models X and Y are equally useful	Both have implied biases	Neither model X nor model Y	Both are more flawed than I would be comfortable with	Definitely model Z	It is more fair but, not fair in general	Definitely model Z	There is a heavy caucasian leaning bias	Neither ${e://Field/pref_model} nor model Z	Both are more flawed than I am comfortable with			High	Housing is a right that shouldn't be complicated by flawed technology.			Low	They can reverse the payment			Low	Society at large would likely be indifferent 			Low	The company will not be bankrupted by fraudsters so there is little risk when large corporations like that take losses			No			Yes			Yes			Disadvantaged	Secondary Education	Others	6108957f175a2eb650221b8e	618b3e1d59be0d9e09f6f943	618c50cd5e85622efa19caa3	minority	outcome-fpr	rent	False		model Y	bottom								Disadvantaged	618c50cd5e85622efa19caa3	APPROVED	2021-11-10 23:07:59.879000	2021-11-10 23:31:48.028000	1428.149	26.0	149	0	100	2021-11-14 18:07:01.740000	19AE28A9	United States	United States	Due to start a new job within the next month	Non-Caucasian	English	United States	Female	Yes	
195	2021-11-10 15:50:15	2021-11-10 16:33:03	IP Address	70.184.164.176	100	2568	True	2021-11-10 16:33:04	R_0HvN1tTUjX7W2id					36.796905517578125	-76.17970275878906	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The bottom one is an equal amount of percentage while the top one has different scales	Mildly unbiased	The amount of non-Caucasian acceptance is very low compared to the amount of Caucasians. 	Neither useful nor unusable	I honestly think it only speaks to the one-sided percentage. Although the amount is equal when it comes to mistakes made when access is granted.	Mildly unfair	I think that everything cannot be determined by machines. Everyone makes mistakes but the machine is showing that one race can make the most.	Neither biased nor unbiased	In the first one, I honestly thought that it was biased because the chances of mistakes made by the other race increased. However, it seems that the computer has done the opposite.	Neither useful nor unusable	A person's worth cannot be determined by a machine. Even machines can be wrong	Yes	I would like the people who granted access percentage and percentage of mistakes to be even. The reason is that a machine cannot determine whether a person makes more mistakes than the other.	Models X and Y are equally fair	Because they both hold an equal percentage of pass and failure.	Models X and Y are equally biased	The ones with different percentages for both mistakes and access granted.	Models X and Y are equally useful	If both machines showed an equal percentage then they are useful	Neither model X nor model Y	I would choose neither because a machine can also make mistakes.	Both ${e://Field/pref_model} and Z are equally fair	They both show the equal percentage	Both ${e://Field/pref_model} and Z are equally biased	The percentages that show the difference between each race	Neither ${e://Field/pref_model} nor model Z	The machines can be wrong and mistakes do not need to be tested by them		High		There are many sick people who just want to kill others and they will do different things to achieve that goal.		High		The unauthorized individuals are just as dangers as strangers walking into your house.		High		I consider all people part of society because every person does something that affects how others live.		High		I consider all people part of society because every person does something that affects how others live. It is a dangerous thing to allow non-medical personnel into security-secured places.		Yes			Yes			Yes			Disadvantaged		Bachelor	Others	610e628bb9ef91fb413c9865	618b3e1d59be0d9e09f6f943	618c4c8daafae2de522e48b6	minority	outcome-fpr	frauth	True		model Y	bottom								Disadvantaged	618c4c8daafae2de522e48b6	APPROVED	2021-11-10 22:50:13.433000	2021-11-10 23:33:06.184000	2572.751	21.0	46	1	99	2021-11-14 17:29:24.742000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	Yes	
196	2021-11-10 15:13:15	2021-11-10 16:39:38	IP Address	47.39.199.253	100	5183	True	2021-11-10 16:39:39	R_1KvbEIDThz7yMkO					33.94610595703125	-83.41799926757811	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 1	Acceptably fair	I think they are fair because there is the same margin of error. 	Neither biased nor unbiased.	I am not sure what the criteria are for being accepted but maybe it is possible that there are more physicians and nurses who are not Caucasian than those who are.	Mostly unusable	It seems to have done its job with fewer mistakes than it could have had but there is still much room for improvement before it is ready to be used and relied on.	Very fair	Everyone has an equal chance.	Very unbiased	There is no difference between who is granted access and who is not granted access.	Neither useful nor unusable	There was a large margin of error, too high to be effective.	Yes	I would say that Model X is actually not fair because it did not give an equal chance to all of the nurses and physicians.	Definitely model Y	Having equal opportunities allows for everyone to have a level playing field, thus being fair.	Definitely model X	Although the results were more accurate, the way in which they were obtained was unfair and not equal.	Models X and Y are equally useful	Model X's data is more useful but it is not produced in the right way and should be voided. Model Y is not as useful but it is fair.	Neither model X nor model Y	They both have flaws that make them ineffective and/or immoral.	Probably model Z	It seems that denying access at an unequal rate is less morally acceptable than accepting access at a different rate. Thus, Model Z is more fair.	Probably ${e://Field/pref_model}	Model X denied Caucasian people access at an alarmingly high and different rate than Non-Caucasian, which shows a clear criteria that was made to leave them out.	Neither ${e://Field/pref_model} nor model Z	Again, those margins of error on either side are too high for me to consider it effective and I would seek an alternative.		High		The wrong personnel or lack of that help could be catastrophic in a hospital setting where time and expertise is valued.		Moderate		Unauthorized persons could wreak havoc, distract from the task, or gain improper access to confidential information.		Moderate		Society, the people in the hospital realm, could have their work spaces infiltrated and messed with, disrupting their routines.		Moderate		The wrong information could get into the wrong person's hands.		Yes			No			Yes			Advantaged		Secondary Education	Others	6131415f7e41bb553ed1add4	618b3e1d59be0d9e09f6f943	618c43f59fca1e6aad28c7f0	majority	outcome-fpr	frauth	True		model X	top								Advantaged	618c43f59fca1e6aad28c7f0	APPROVED	2021-11-10 22:13:11.964000	2021-11-10 23:39:42.447000	5190.483	20.0	191	1	100	2021-11-22 20:16:25.365000	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Female	Yes	
197	2021-11-10 16:27:36	2021-11-10 16:40:06	IP Address	174.76.116.43	100	750	True	2021-11-10 16:40:06	R_BPOtGNtfUL5LUBz					32.89360046386719	-83.7429962158203	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	The same amount of mistakes.	Acceptably biased	Caucasian has higher acceptance rate.	Mostly useful	If unbiased, it has a lot of potential 	Mildly unfair	The mistake percentage is too high.	Acceptably biased	Makes more mistakes with Caucasians.	Mostly useful	Higher success rate than previous model	Yes	I don’t think model x is as useful as model y. Model y was more unbiased and equal success rates.	Models X and Y are equally fair	I’m actually not sure now that I’m looking at both… I think both models still need work.	Models X and Y are equally biased	Both biased in different areas.	Models X and Y are equally useful	I think more work needs to be done before putting them in use.	Neither model X nor model Y	Neither are completely reliable 	Both ${e://Field/pref_model} and Z are equally fair	Neither are completely fair.	Both ${e://Field/pref_model} and Z are equally biased	They both are biased in some areas.	Neither ${e://Field/pref_model} nor model Z	Neither I wouldn’t use a biased program		Low		It’s an inconvenience but it isn’t really a big problem 		High		This is very dangerous and could cause a lot of damage		Low		It’s just another safety precaution 		High		It’s very dangerous, medical records can be tampered with		No			Yes			Yes			Disadvantaged		Secondary Education	Others	5e730aa954a7b42617d99c5a	618b3e1d59be0d9e09f6f943	618c55614c4a461489f8077e	minority	outcome-fpr	frauth	True		model Y	bottom								Disadvantaged	618c55614c4a461489f8077e	APPROVED	2021-11-10 23:27:32.824000	2021-11-10 23:40:09.542000	756.7180000000001	24.0	453	3	99	2021-11-14 02:45:05.937000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Female	DATA EXPIRED	
198	2021-11-10 16:26:25	2021-11-10 16:43:11	IP Address	172.58.45.148	100	1006	True	2021-11-10 16:43:11	R_Z1YAmwIXlcnvuKt					47.894805908203125	-122.20309448242188	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The model is biased towards non-caucasians.	Acceptably biased	The model is biased towards non-caucasians.	Mostly unusable	The model is biased towards non-caucasians.	Mildly unfair	It is biased towards caucasians.	Acceptably biased	It is biased towards caucasians.	Mostly ununsable	It is biased towards caucasians.	No		Probably model Y	The difference in bias between caucasians and non-caucasians is less in model y.	Probably model X	The difference in bias between caucasians and non-caucasians is less in model y.	Probably model Y	The difference in bias between caucasians and non-caucasians is less in model y, so it is slightly more reliable.	Probably model Y	The difference in bias between caucasians and non-caucasians is less in model y, so it is slightly more reliable.	Probably ${e://Field/pref_model}	The difference in bias between caucasians and non-caucasians is less in model y.	Probably model Z	The difference in bias between caucasians and non-caucasians is less in model y.	Probably ${e://Field/pref_model}	The difference in bias between caucasians and non-caucasians is less in model y.			Moderate	It is significant enough to have to contact the person in order to make sure the payment is made.			High	It is losing money for no reason, which is a big deal.			Low	It is low since denying a payment does not affect society, just an individual. The society would be people in a community.			Moderate	It is moderate considering that it could happen to other members of society. The society would be people in a community.			No			Yes			Yes			Disadvantaged	Secondary Education	Others	6163ae32f4a006501a1d6448	618b3e1d59be0d9e09f6f943	618c551bf27d92ea1cacee65	majority	outcome-fpr	rent	True	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c551bf27d92ea1cacee65	APPROVED	2021-11-10 23:26:23.381000	2021-11-10 23:43:14.442000	1011.061	18.0	204	0	100	2021-11-14 18:49:30.457000	19AE28A9	United States	United States	Other	Non-Caucasian	English	United States	Female	Yes	
199	2021-11-10 12:38:28	2021-11-10 16:45:50	IP Address	50.215.5.25	100	14842	True	2021-11-10 16:45:51	R_2ZTtzYFhuizJH23					37.779693603515625	-122.41590118408205	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Because it has a higher probability of finding an error in marginalized groups of peoisihh	Acceptably biased	It seems to be biased towards white people, but then again, it’s possible that these groups have a lower tendency to commit fraud due to real, socioeconomic conditkons	Neither useful nor unusable	I think I’d need a bit more information in order to judge 	Acceptably fair	The probability for making a mistake is the same across minority and Caucasian users. To be really sure of how fair it is; I would need to see more data, but I’d assume there are more Caucasian users than non-Caucasian, hence the higher chance for a mistake to be allowed in that group. 	Neither biased nor unbiased	I think I would need to see more information 	Neither useful nor unusable	Again, I believe I would need more information 	Yes	I think that a better goal is equal percentage of mistakes made across the board, rather than equal percentage of payments flagged. So if flagging a higher percentage of white applicants means those mistakes are equivalent percentages, I think it’s the right choice 	Probably model X	I’m not sure I understand the data correctly because it seems to tell two different stories 	Models X and Y are equally biased	I’m not sure I understand the data correctly because it seems to tell two different stories 	Models X and Y are equally useful	I’m not sure I understand the data correctly because it seems to tell two different stories 	Neither model X nor model Y	I’m not sure I understand the data correctly because it seems to tell two different stories 	Probably model Z	" while the first model has a larger percentage of non-Caucasian payments that were legitimate and got denied unfairly, the second model shows a larger percentage of CAUCASIAN payments that were ILLEGITIMATE and made it through the system. So both systems are showing some bias, one in terms of unfairly punishing POC users, and the other in terms of allowing non-POC users to get past the system without being flagged, when they should have been. 
However, I think in terms of fairness, one of those scenarios is preferable, because it is more fair to let fraudulent users through the system without catching them, then it is to not allow non-fraudulent users to pay their rent. So basically, in model X, the non-criminal POC are at a disadvantage, while in model Y, the criminal POC are not able to commit crime as easily as their white counterparts. So it’s basically a matter of which group you care about more. "	Both ${e://Field/pref_model} and Z are equally biased	Technically, they have equivalent amounts of bias. 	Probably model Z	If I HAD to choose one or the other, I would personally pick model Z. HOWEVER, for the company, if they, as most companies do, put profit above everything else, they might prefer model x. This is because, allowing fraudulent users to go unfettered through their safeguards, probably has an ultimately worse impact on their bottom line than denying legitimate customers to pass through. Although, since we are talking about merely a middleman, their loss in both instances is the same: the profit they would have gotten from a successful, legitimate payment. If they are expected to pay the bank back for illegitimate payments that were allowed to go through their system, then it might be different. 			High	Because they didn’t do anything, but now are unable to pay their rent 			Low	It might have a larger negative impact on an individual user to be unfairly denied, but allowing criminals to pass through without catching them is worse in terms of the larger scale impact 			Low	To allow users committing fraud to pass through without punishing them, is to encourage them to continue to commit fraud. “Society” refers to the community where the fraud-committers live and commit fraud.			Moderate	Because it will encourage more, potentially more damaging types of fraud			Yes			No			Yes			Advantaged	Secondary Education	Services Occupations	6024cc27a4f7f31f698a87d6	618b3dcb32ad5f5cc3f00d86	618c1faf0a2bff90ea2280ff	minority	outcome-fpr	rent	False		model X	top								Advantaged	618c1faf0a2bff90ea2280ff	APPROVED	2021-11-10 19:38:25.514000	2021-11-15 00:16:37.222000	362291.708	27.0	117	0	100	2021-11-15 00:16:37.310000	Manual Completion	United States	United States	Unemployed (and job seeking)	Caucasian	English	United States	Female	No	
200	2021-11-10 16:15:00	2021-11-10 16:50:01	IP Address	107.218.33.137	100	2101	True	2021-11-10 16:50:02	R_3JJ0PhAiETBvmNd					34.21830749511719	-118.37179565429688	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	not sure why one group will more likely require ICU	Neither biased nor unbiased.	This does not give me much information on why one group was more likely to require ICU	Mostly unusable	This does not tell me why one group id more likely to require ICU	Mildly unfair	Some people who didn't need the support were given the ICU.	Mildly unbiased	I'm not sure how many mistakes were made for those who weren't given ICUs.	Mostly ununsable	What matters is how many people who did need it didn't get the ICUs because of the errors presented	No		Probably model Y	I think its fair to say that both  groups are equally as likely to require the ICU for support	Probably model X	Not sure why group will most likely require the ICU	Models X and Y are equally useful	I'm not sure what the negative affects are	Probably model Y	I think its fair to say that both  groups are equally as likely to require the ICU for support	Probably model Z	I think it's unfair one group is impacted the most for denial of ICU use	Probably ${e://Field/pref_model}	why is one group being affected more by denials than the other one.	Probably model Z	they are equally affected by denying them an ICU	High			ICU is important to get a person to live longer	High			This prediction denies the person from receiving help	High			Society means every individual who is a part of a country or city. People who are mistakenly given ICU support could take the place of someone who needs it	Moderate			Every person who visits the ER can be mistakenly denied ICU support	No			No			Yes			Disadvantaged			Bachelor	Others	60ce20b540d291328bf5b356	618b3e1d59be0d9e09f6f943	618c526beed4b90f6483ce4a	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c526beed4b90f6483ce4a	APPROVED	2021-11-10 23:14:58.211000	2021-11-10 23:50:05.244000	2107.033	26.0	77	1	99	2021-11-14 18:01:12.254000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	Spanish	United States	Female	DATA EXPIRED	
201	2021-11-10 16:24:27	2021-11-10 16:51:42	IP Address	172.58.228.155	100	1635	True	2021-11-10 16:51:43	R_cIwPa8CWKeJFwUp					40.83430480957031	-73.92350006103516	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Very unfair	It should be equal	Very unbiased	Access gained 60% Caucasian. It should be fair or close to similarity in performance.	Neither useful nor unusable	It’s flawed	Very unfair	Pr yo babii ty is too high. Not trust worthy.	Neither biased nor unbiased	It’s just data 	Completely unusable	Only 50%	No		Probably model Y	Granted access is equal	Definitely model X	It’s software seems to be better equipped for a certain group	Definitely model X	Less granted access mistakes	Definitely model Y	Better ergonomic Enviroment for the workers. 	Definitely ${e://Field/pref_model}	Equal acess	Definitely model Z	"Unfair poll e
Resultz"	Definitely ${e://Field/pref_model}	It works better 		High		People’s privacy is at stake		High		Stolen data		High		Hoops violation 		High		Having access to anyone’s personal health care records is a solid no. There should be a margin of 1-3% error results.		Yes			No			Yes			Disadvantaged		Bachelor	Healthcare Practitioner	615ecd9df93b5467b032282b	618b3e1d59be0d9e09f6f943	618c54a38abf1abbaf68a342	minority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c54a38abf1abbaf68a342	APPROVED	2021-11-10 23:24:25.342000	2021-11-10 23:51:45.949000	1640.6070000000002	34.0	20	0	100	2021-11-14 03:05:06.255000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Male	No	
202	2021-11-10 16:08:05	2021-11-10 16:56:10	IP Address	70.115.231.14	100	2884	True	2021-11-10 16:56:10	R_V2LqP8S3LmvEtwd					32.783599853515625	-96.86740112304688	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	It has a significantly higher chance of granting access to people of color over white people.	Acceptably biased	People of color seem to be more easily detected by the device. It needs some work done for it to behave more equally towards both whites and people of color.	Mostly useful	The device is not totally useless given how it grants an equal amount of access to unauthorized personnel. It may just need a few tweaks done. 	Acceptably fair	There's a 50/50 chance of the device granting access to either a Caucasian or Non-Caucasian.	Very unbiased	I see no signs of bias.	Mostly useful	The device doesn't seem to unfairly distinguish people of color from white people.	No		Definitely model Y	For Model Y, there's an equal amount of chance that either a Caucasian or Non-Caucasian will be granted access, while Model X is significantly biased towards granting access to Non-Caucasians. Also, Model Y has an overall 69% chance of granting access to unauthorized personnel, while Model X has a 70% chance. Making model Y the most fair and superior model by being the least biased and also by being 1% less likely to grant unauthorized access.	Definitely model X	Model X is the most biased due to it having a higher probability of granting access to Non-Caucasians over Caucasians.	Definitely model Y	As previously stated, it is the least biased and safest.	Definitely model Y	Model Y is the least biased/superior model.	Definitely ${e://Field/pref_model}	It's got an equal chance of granting access to either a Caucasian or Non-Caucasian.	Definitely model Z	It's got a significantly higher chance of granting access to a non-Caucasian.	Definitely ${e://Field/pref_model}	It's less biased despite it rejecting more Caucasian authorized personnel.		Low		Requiring a usename/password is only an inconvenience to staff.		High		Mistakenly granting access to unauthorized users in any setting can and will spell trouble in any significant way.		Low		I consider everyone in the world to be part of society. Including plants, animals, the universe, etc. . . It is still a low impact to fail to recognize hospital staff because it is only a minor inconvenience to need to type a username and password.		High		Society includes people, animals, plants, and the universe. Mistakenly granting access to unauthorized personnel is high impact because it puts people at risk.		No			Yes			Yes			Advantaged		Secondary Education	Others	6176377745c7352033f281df	618b3e1d59be0d9e09f6f943	618c50cc9937ab9f0c0b57b9	majority	outcome-fpr	frauth	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c50cc9937ab9f0c0b57b9	APPROVED	2021-11-10 23:08:02.838000	2021-11-10 23:56:21.457000	2898.619	25.0	212	1	100	2021-11-14 17:56:18.850000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	English	United States	Male	Yes	
203	2021-11-10 16:32:45	2021-11-10 16:57:18	IP Address	174.250.34.6	100	1472	True	2021-11-10 16:57:18	R_2ZWh1g9ispE9aUc					37.751007080078125	-97.82199859619139	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	For similar reasons as the previous example. The model sees something in non-Caucasian patients as a flag for ICU.	Acceptably biased	Same as before, just in a different direction.	Mostly useful	Models X and Y both fall into what I would consider acceptable error levels.	Mildly unfair	There’s a visible disparity between “false positive” cases (did not need ICU) between Caucasian and non-Caucasian patients.	Neither biased nor unbiased	Human bias in training material likely led to a biased algorithm, in that it sees something in non-Caucasian patients that would require them to be in ICU. However given that non-whites in America tend to have poorer health for various reasons, it makes intuitive sense to me.	Mostly useful	While there are likely better models, I think Model Y’s predictions make sense, and are likely similar to choices a human would have made. To me, it intuitively feels “safer” to err on the side of “send them to the ICU”, even bearing in mind limited bedspace.	No		Probably model Y	Its prediction levels are flat, symmetrical, the same among groups, even if that’s not how the actual world works.	Probably model X	It flagged more non-Caucasian patients as requiring ICU.	Probably model X	Model X, despite its disparity in ICU choices, has a flat mistake level for both categories.	Probably model X	The point difference between the two models is only 1%. Model X would result in a more even distribution.	Probably model Z	Model Z’s denial ratio bars are flat, even though it’s admission ratio bars are not. Directly denying people medical care feels worse to me.	Probably ${e://Field/pref_model}	Model X denies more people ICU care who would have actually needed it.	Probably model Z	I would prefer to admit too many people than turn away too many. They are both bad but one is easier to sleep with at night.	Moderate			If you don’t need ICU care and you have really great insurance/financial situation, it just results in wasted time for you. But if you need to pay for it, that can be devastating.	High			If the machine says you don’t need ICU care but you do, you could die or develop complications from untreated illnesses.	High			Anyone who needs a bed that is not available is immediately negatively affected. Their family will have to grieve them if they die, as well as pay for the funeral. Employers may lose good employees. Staff at the hospital may have to watch them die in the hallways.	High			Similar to above, but more direct. Instead of a possibility of someone needing care and not being able to access it, now it is a direct event that happened. It affects their family, employers, friends, as well as the staff who treated them.	Yes			No			Yes			Disadvantaged			Primary Education	Transportation Occupations	60c826709a3e2208cf06fe71	618b3e1d59be0d9e09f6f943	618c56993baa2088c674e577	majority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Disadvantaged	618c56993baa2088c674e577	APPROVED	2021-11-10 23:32:43.463000	2021-11-10 23:57:23.727000	1480.264	24.0	187	1	100	2021-11-14 16:46:53.081000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Female	DATA EXPIRED	
204	2021-11-10 16:44:47	2021-11-10 17:12:02	IP Address	23.242.126.72	100	1635	True	2021-11-10 17:12:03	R_3iVw6euqU8iFvpU					33.885498046875	-117.22259521484376	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	I do think it's fair because it picked people from both categories to get put into the ICU.	Acceptably biased	I do think it is biased towards Caucasians because it let more of them into the ICU, but they had the same amount of people who were mistakingly put there.	Mostly unusable	In this case it wasn't very useful. A great chunk of the non-caucasion people who were sent to the ICU weren't even supposed to be there so it was just messing with everything else. 	Neither fair nor unfair	I think it is neither fair or unfair because although it did make some mistakes, it also had some good predictions.	Acceptably biased	I think the model might have been a little biased towards those who are not Caucasian seeming as they had less mistakes among those granted access into the ICU.	Mostly useful	I think it is mostly useful because it can help predict help for those that are needed. It could be very useful if they worked out the kinks and made it more accurate.	No		Probably model Y	I think model Y is more fair because it let in the same amount of people from both groups.	Probably model X	I think model X might be more biased solely based off the fact that more people of one race were let in, while simultaneously having more mistakes for those that needed to be in there. 	Models X and Y are equally useful	I think they both have pros and cons and can be useful. 	Probably model Y	This one seemed as the more fair choice overall, so I would have to pick model Y. 	Both ${e://Field/pref_model} and Z are equally fair	I think they both have pros and cons for how fair they are.	Probably ${e://Field/pref_model}	I think model Y is more biased because they let in more Caucasian people when a big chunk of them simply did not need to be there.	Probably model Z	I think I would pick model Z because it had less mistakes compared to their initial group that was chosen.	Low			I don't think it would affect the individual very much. If anything they will be relieved that they will not have to go to the ICU.	High			I would be very impactful because the ICU is where people who are in serious condition are sent and if someone that needs it doesn't get sent there it could seriously harm them.	High			Society is everyone around the individual who is being directly affected. Society will be impacted greatly because there will be other individuals who need the help who aren't getting it because you were mistakingly put there.	Low			Society is everyone around the individual who is directly being affected. This opens up room for someone else, so society won't be too worried.	No			Yes			Yes			Disadvantaged					6109e6edf61c274f2d101c23	618b3e1d59be0d9e09f6f943	618c5964d126e81421562515	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Disadvantaged	618c5964d126e81421562515	APPROVED	2021-11-10 23:44:45.110000	2021-11-11 00:12:06.806000	1641.696	21.0	277	1	100	2021-11-14 18:13:16.523000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	Yes	
205	2021-11-10 16:26:43	2021-11-10 17:25:45	IP Address	68.202.227.217	100	3541	True	2021-11-10 17:25:45	R_WCLNm0nPkOpGIWR					28.188507080078125	-81.26950073242188	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for male applicants than female applicants.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	Well for one the model realizes that not everyone is white and can actually differentiate between poc people that it can between white people but it seems like a somewhat balanced percentage between both groups overall especially when it comes to the amount of mistakes as its even for both whites and poc.	Acceptably biased	Well the model for some reason knows how to better detect poc faces than white faces which is surprising given how most things are made with white people in mind and poc as an afterthought. So in this case, surprisingly it has a bias towards poc which although surprising is quite refreshing.	Mostly useful	It has a somewhat even percentage for both poc and whites especially in the case of mistakes.	Acceptably fair	Well it has even percentage of granting access to whites and poc. On the other hand it seems to make more mistakes when it comes to poc which just makes it seem like it has a bias towards whites.	Very biased	It has a huge percentage difference in mistakes towards poc than whites which kinda makes it seem mildly racist since its basically saying that all poc look the same.	Mostly ununsable	Its a racist model.	No		Definitely model X	Overall it has a more even probability of access than the other model and the rate of mistakes is equal for both whites and poc.	Definitely model Y	It's a somewhat racist model that implies all poc look the same since its probability to make mistakes identifying them is much higher than its probabilities with identifying white people.	Definitely model X	Less biased all around.	Definitely model X	Offers fairness to both whites and poc.	Both ${e://Field/pref_model} and Z are equally fair	They offer a similar amount of mistakes.	Both ${e://Field/pref_model} and Z are equally biased	They're both equally biased but to opposite groups.	Neither ${e://Field/pref_model} nor model Z	They're too similar in their bias therefore its not really possible to choose between one or the other.		High		Well depending on the kind of doctor said person is it could potentially authorize a person who could cause harm to the patients or worse.		High		There's no telling what said users will do with said access. They could end up committing a crime.		High		I considered the patients who would be impacted by an unlawful authorization to be a part of 'society'.		High		This one is even worse because it would be someone who has no medical background posing as hospital staff. In this one I'd consider the other staff and the patients as part of society since they would both be affected by it.		No			Yes			Yes			Disadvantaged		Secondary Education	Others	61140514150f4cdab3ba1f21	618b3e1d59be0d9e09f6f943	618c552e86862f3c1be9a092	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Disadvantaged	618c552e86862f3c1be9a092	APPROVED	2021-11-10 23:26:41.556000	2021-11-11 00:25:48.047000	3546.491	28.0	191	0	100	2021-11-14 18:36:05.308000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Female	No	
206	2021-11-10 17:04:22	2021-11-10 17:27:27	IP Address	69.109.184.161	100	1384	True	2021-11-10 17:27:28	R_1rxiY2wvBr6cReH					32.744903564453125	-117.16500091552734	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	The model has an equal percentage of mistakes between Caucasians and Non-Caucasians. 	Mildly unbiased	It doesn't seem as though model X is biased because the mistakes are equal. 	Neither useful nor unusable	35% of mistakes is pretty high.	Mildly unfair	Mistakes are significantly higher for Non-Caucasians. 	Acceptably biased	I'd say it's somewhat biased because of that increase in percentage for mistakes. 	Very useful		No		Probably model X	Although the percentage of mistakes is greater, they are equal and unbiased. 	Probably model Y	Mistakes are significantly skewed towards Non-Caucasians. 	Probably model X		Probably model X		Both ${e://Field/pref_model} and Z are equally fair	The difference between the mistakes is relatively equal for both models.	Probably model Z	Unequal mistakes percentage.	Probably ${e://Field/pref_model}		Low			Receiving ICU support when you don't need it is far better than needing it and not receiving it.	High			A person can suffer extreme health decline if they are not provided with ICU support when needed. 	Moderate			The society (hospital staff) will be impacted because another patient would be added to their load despite their need to. 	High			A person can go into significant health decline if not put in the ICU, and therefore will cause more work for the society (hospital staff).	No			Yes			Yes			Advantaged			Bachelor	Administrative Staff	60ff4cca713180d934efc4c7	618b3e1d59be0d9e09f6f943	618c5e00523c37c9e1794a41	majority	outcome-fpr	icu	True	You have chosen model X over model Y.	model X	top								Advantaged	618c5e00523c37c9e1794a41	APPROVED	2021-11-11 00:04:21.148000	2021-11-11 00:27:31.245000	1390.0970000000002	25.0	208	2	100	2021-11-14 02:59:51.160000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	No	
207	2021-11-10 17:22:11	2021-11-10 17:45:40	IP Address	76.126.12.138	100	1408	True	2021-11-10 17:45:41	R_2eQo9XTrbbfjj9O					37.48280334472656	-122.21440124511717	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	Non-Caucasians have a better probability to access it	Neither biased nor unbiased.	The mistakes are both equal whether they're Caucasian or not.	Very useful	Quickly accessing devices is useful.	Very fair	Both Caucasians and non-Caucasians have an equal chance to access model Y.	Acceptably biased	It has mistakes for non-caucasians more.	Very useful	Still able to access devices simply.	No		Definitely model Y	In model Y's yellow graph, they are both equal in the probability of access. 	Definitely model Y	Model X has equal mistakes while Model Y mistakes Caucasians less than non-Caucasians.	Models X and Y are equally useful	They both are equal in one graph while the other is mildly unequal.	Definitely model X	I'm a non-caucasian so the chances of access are easier for me, and the mistakes are equal.	Definitely model Z	Access is for model Z is equal with caucasians and non-caucasians.	Definitely model Z	It has mistakes more for non-Caucasians.	Neither ${e://Field/pref_model} nor model Z	As a non-caucasian, they both have bad statistics for a non-caucasian.		Moderate		Sometimes the staff might need access quickly and may have forgotten their password.		High		Unauthorized users will have access to data they shouldn't be looking at.		High		Society likes to get things done quickly, so a staff failing to access data will anger patients.		High		Society will begin to worry and panic if someone has access to their personal information.		Yes			Yes			Yes			Advantaged		Secondary Education	Others	616ddca50e70b5920ddcdebd	618b3e1d59be0d9e09f6f943	618c62308222f74df1befd77	majority	outcome-fpr	frauth	True	You have chosen model X over model Y.	model X	top								Advantaged	618c62308222f74df1befd77	APPROVED	2021-11-11 00:22:09.890000	2021-11-11 00:45:43.879000	1413.989	19.0	184	2	99	2021-11-14 03:07:37.628000	19AE28A9	United States	United States	Unemployed (and job seeking)	Non-Caucasian	Spanish	United States	Male	Yes	
208	2021-11-10 16:52:20	2021-11-10 18:07:13	IP Address	173.173.248.110	100	4492	True	2021-11-10 18:07:13	R_3kInpjOaZlC44wQ					26.181304931640625	-97.74829864501952	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Acceptably fair	It is very equal.	Neither biased nor unbiased.	It doesnt discriminate on ethnicity.	Mostly useful	It seems to be more persistent. 	Mildly unfair	It is denying more caucasians than non-caucasions.	Neither biased nor unbiased	It it doesnt know what biased is.	Neither useful nor unusable	It needs to be more consistent.	Yes	The first one was very biased and inconsistent.	Probably model X	It is equal.	Definitely model Y	It mistaked more caucasions.	Probably model Y	It is not one sided because of race.	Definitely model X	Just seems more consistent and less biased on ethnicity. 	Definitely model Z	It is equal.	Probably ${e://Field/pref_model}	Has higher rate of noncaucasions.	Definitely model Z	It seems fair.		Low		It is not that serious.		High		Someone can take advantage of the device.		Low		It is not that serious because society wont think any less of you for that. I considered by standards.		High		They will know the medical place does not have things working correctly. I considered myself.		Yes			Yes			No			Disadvantaged		Secondary Education	Transportation Occupations	5e952145d04298128b30bfcd	618b3e1d59be0d9e09f6f943	618c5b2fd51645b12d1d4579	minority	outcome-fpr	frauth	False	You have chosen model X over model Y.	model X	top								Disadvantaged	618c5b2fd51645b12d1d4579	APPROVED	2021-11-10 23:52:18.188000	2021-11-11 01:07:17.527000	4499.339	26.0	653	3	100	2021-11-14 02:52:17.091000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	DATA EXPIRED	United States	Male	DATA EXPIRED	
209	2021-11-10 18:00:36	2021-11-10 18:19:58	IP Address	66.69.2.9	100	1161	True	2021-11-10 18:19:58	R_2uF4j5mAShGsM6t					29.33099365234375	-98.49659729003906	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	It seems pretty reasonable to me.	Very unbiased	I'm sure white people are more likely not to pay rent.	Very useful	To help prevent fraudulent payments. 	Mildly unfair	Because why are the even, when there are many more non-white people?	Very biased	Why are white people given equal opportunities in that is given to several races?	Neither useful nor unusable	It will wrongly flag fraudulent payments.	No		Definitely model X	"Because there are more races involved than just 1 in the ""non-Caucasian"" categories. "	Probably model Y	Because whites should not be more likely to be approved than all other races simply based on the basis that they are white.	Definitely model X	Because it is more inclusive.	Definitely model X	Because it fits the real demographics better and is not biased towards POC.	Probably ${e://Field/pref_model}	Because a larger portion of people are not being targeted unjustly. 	Definitely model Z	Because whites are not equal to several different races so should not be represented that way.	Definitely ${e://Field/pref_model}	Because I believe it to be the least biased.			High	Because people can become homeless in this situation.			Low	Because the people who are getting paid off of people's rents are leeches and have enough money to survive whether or not they get their rent payment.			High	Society = human beings. It highly affects us as a society because people should not be homeless regardless of if they can or can't pay rent.			Low	Landlords are leeches on society (society = human beings). I don't consider them human because they don't see us as more than a dollar sign. They don't deserve rent. They'll be okay without it. 			Yes			Yes			Yes			Disadvantaged	Secondary Education	Others	5daf9e6815dc35001498cd77	618b3e1d59be0d9e09f6f943	618c6b2fa6498ba4e4fc616e	majority	outcome-fpr	rent	True	You have chosen model X over model Y.	model X	top								Disadvantaged	618c6b2fa6498ba4e4fc616e	APPROVED	2021-11-11 01:00:34.212000	2021-11-11 01:20:02.710000	1168.498	25.0	318	0	100	2021-11-14 16:32:52.794000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	No	
210	2021-11-10 18:16:00	2021-11-10 18:44:14	IP Address	73.70.97.46	100	1693	True	2021-11-10 18:44:15	R_1imla7k1a9wvHd9					37.341705322265625	-121.9752960205078	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	None of the above can be inferred from the figures.	Figure 2	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Neither fair nor unfair	It is a program based on the needs of entering the ICU or not, it is guided by parameters, according to this will be your decision	Very unbiased	because it is the duty to be, that there are no biases of any kind.	Mostly useful	can help to have a guiding idea	Neither fair nor unfair	it is a program that is not exact and throws errors like any other	Very unbiased	it is the duty to be, to be impartial	Mostly useful	can help to have a guiding idea	No		Definitely model X	because it has the same error margins	Definitely model Y	because it is the one with the most margin of error	Definitely model X	because it has the same margin of error for both ethnicities	Definitely model X	because it has the same margin of error for both ethnicities	Definitely ${e://Field/pref_model}	because it has the same margin of error for both ethnicities	Definitely model Z	because it is the one with the most margin of error for Caucasian people	Definitely ${e://Field/pref_model}	because it has the same margin of error for both ethnicities	High			because it would affect the individual in a negative way in case of not ruling in his favor, he could even die	High			because your life could depend on that decision	High			because it would affect an individual in a negative way in case of not ruling in his favor, he could even die	High			because the life of an individual could depend on that decision	No			Yes			Yes			Advantaged			Bachelor	Healthcare Practitioner	5d003450f641130018828b8e	618b3e1d59be0d9e09f6f943	618c6ebc1c1ba76cced53b30	minority	outcome-fpr	icu	False	You have chosen model X over model Y.	model X	top								Advantaged	618c6ebc1c1ba76cced53b30	APPROVED	2021-11-11 01:15:47.660000	2021-11-11 01:44:41.392000	1733.7320000000002	26.0	533	5	98	2021-11-14 17:59:16.310000	19AE28A9	United States	United States	DATA EXPIRED	Non-Caucasian	English	United States	Female	DATA EXPIRED	
211	2021-11-10 18:16:54	2021-11-10 18:58:15	IP Address	75.189.5.30	100	2480	True	2021-11-10 18:58:15	R_2AT2htaqNkvllAa					33.88200378417969	-80.40399932861328	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is more likely to predict correctly for female applicants than male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	The probability of predicting required ICU is much higher in Caucasian compared to non-Caucasian.	Neither biased nor unbiased.	No way to tell because if the model X is biased due to not knowing the data used or who program it.	Mostly useful	It can help cut down the amount of time used to evaluate patients.	Mildly unfair	Mistakes among only granted ICU support for Caucasians are almost double that of non-Caucasians.	Neither biased nor unbiased	It's hard to tell.	Mostly useful	In some cases, it could be useful as far as time and the number of patients that have to be evaluated.	No		Probably model Y	The probability of predicting ICU support require equally with both caucasian and non-caucasian.	Probably model X	The probability of predicting ICU support requires is much higher in caucasian compared to non-caucasian.	Models X and Y are equally useful	I think both models can be useful.	Probably model Y	The probability of predicting ICU support required is equal with both caucasian and non-caucasian in model Y.	Both ${e://Field/pref_model} and Z are equally fair	Depending on what part of the Model  X or Y it will display some form of fairness.	Probably model Z	The probability of predicting ICU support require is higher in caucasian compared to non-caucasian with model Z.	Probably ${e://Field/pref_model}	Model Y is equal on both sides when it comes to predicting ICU support.	Moderate			Based on both Models shown I believe this to be so.	Low			Based on both models shown I believe this to be so.	Moderate			Taking both models into consideration with Caucasians and non-Caucasians in mind.	Low			Taking both models into consideration with Caucasians and non-Caucasians in mind.	Yes			Yes			Yes			Advantaged			Secondary Education	Others	61096a016b964e7af149fce3	618b3e1d59be0d9e09f6f943	618c6eff0052a32c69a2d568	minority	outcome-fpr	icu	False	You have chosen model Y over model X.	model Y	bottom								Advantaged	618c6eff0052a32c69a2d568	APPROVED	2021-11-11 01:16:52.250000	2021-11-11 01:58:21.934000	2489.684	42.0	341	0	100	2021-11-14 03:03:10.330000	19AE28A9	United States	United States	Part-Time	Non-Caucasian	English	United States	Female	No	
212	2021-11-10 18:45:22	2021-11-10 19:32:02	IP Address	73.58.118.169	100	2799	True	2021-11-10 19:32:02	R_agXGns1G4bBcJ0J					41.76969909667969	-87.69850158691406	anonymous	EN	I have had the opportunity to read this consent form and have the research study explained.,I have had the opportunity to ask questions about the research study, and my questions have been answered.  I am prepared to participate in the research study described above.,I accept the use of my Prolific ID ${e://Field/PROLIFIC_PID}	Model X is equally likely to predict correctly for female applicants and male applicants.	Figure 1	Female applicants will be more likely to mistakenly be rejected than male applicants.	Figure 2	Mildly unfair	I think it's unfair because there are too many fraudulent payments being allowed. It makes one think if those who weren't allowed payments were the real payments.	Mildly unbiased	I don't think it's biased. It seems like a good amount of people are able to pay.	Completely unusable	The bottom graph shows that more than half of the payments were mistakes. It allows for too many errors.	Acceptably fair	The probability of allowing payments is equal. Then, the mistakes made are really close as well.	Very unbiased	I don't think it's biased. The probability of allowing payments is equal. It isn't until after when mistakes are realized.	Mostly ununsable	I think it's not useful.  The bottom graph shows that 24% of allowed payments had mistakes, meaning almost half of that data. I think that's a big difference.	Yes	I think I would want to change how usable the models were. I think the second model was far more unusable than the first.	Probably model Y	I think it's more fair because it has fewer mistakes in the bottom graph. Model X has more than half the data that is incorrect.	Models X and Y are equally biased	I think they are equally biased because they have similar mistakes. While model Y has equal groups, they still have a lot of mistakes in the non-caucasian category. Model X lets more non-caucasian payments through but still has many mistakes.	Models X and Y are equally useful	I think they're equally not useful. They give a huge difference in data when you compare top and bottom charts. I don't think the models make any positive differences in detecting mistakes.	Neither model X nor model Y	I wouldn't choose either. I think they cannot easily detect mistakes since their mistakes are really high and cover more than half of the allowed payments.	Both ${e://Field/pref_model} and Z are equally fair	Both have issues in opposite categories. And both are unfair to opposite races. Model X is unfair to Caucasians and Model Y is unfair to non-Caucasians.	Both ${e://Field/pref_model} and Z are equally biased	Model X was biased against Caucasians and Model Y was biased against non-Caucasians. Of course, this depends on many factors and not just race. 	Neither ${e://Field/pref_model} nor model Z	I don't think either is helpful. We are looking at race and I think that might be the problem. We should be looking at other factors when creating the model.			Moderate	If it was a check and it bounces back, the renter can face consequences at their bank. They may be charged a fee and might not have enough anymore. They can also not have enough time to get another check or go to the bank.			Low	You can always work with the bank and flag something as fraudulent. 			Low	"I consider ""society"" as those living their daily lives as well as friends and family. If a renter has to try to make the payment again and the check bounces back and they have a fee, they might have to resort to friends/family for help for that month."			Low	"I consider ""society"" as those who work at the banks and those that are accepting the payment. Mistakenly allowing the payment causes issues for paying other things not included in the rent. And it creates paperwork for banks and those who have to now cancel the payment."			Yes			No			Yes			Disadvantaged	Bachelor	Others	61247491151682b679e31db0	618b3e1d59be0d9e09f6f943	618c75af8970228790501b1a	majority	outcome-fpr	rent	False		model X	top								Disadvantaged	618c75af8970228790501b1a	APPROVED	2021-11-11 01:45:19.945000	2021-11-11 02:32:05.330000	2805.385	22.0	196	0	100	2021-11-14 17:35:09.360000	19AE28A9	United States	United States	Due to start a new job within the next month	Non-Caucasian	English	United States	Female	No	
