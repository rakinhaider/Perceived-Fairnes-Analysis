PROLIFIC_PID	scenario	IFPI	Why?	IFNI	Why?	SFPI	Why?	SFNI	Why?	AB	DB	DT	PGM	Q5.7	Q5.8	Q5.9	Q5.10	Q5.11	Q5.12	Q6.7	Q6.8	Q6.9	Q6.10	Q6.11	Q6.12	Q10.14	Q10.15	Q10.16	Q10.17	Q10.18	Q10.19	Q10.20	Q10.21	Q199	Q203	Q200	Q204	Q201	Q205	pref_model	group	Ethnicity	BFPI	BFNI
5e9859b23da07f0416fca0fb	rent	2	Essentially the same answer, both these things could bring lots of drama to ones life	2	That would be a drastic situation that the individual would have to deal with, that would add a tremendous amount of stress	1	Same answer as the previous question	1	It's less significant because it is indirectly affecting them, society is anyone indirectly affected by the scenario	Yes	Yes	Yes	Disadvantaged	Mildly unfair	It is slightly unfair because of the difference in allowing %	Neither biased nor unbiased.	I don't think the machine learning can be inherently biased	Mostly unusable	Not really, it's such a small amount of info	Neither fair nor unfair	Machine learning is not inherently have bias, but the data used could be. With that in mind, it's hard to tell if it is fair without more info	Neither biased nor unbiased	Machine learning is not inherently have bias, but the data used could be. With that in mind, it's hard to tell if it is fair without more info. Same as last question	Mostly ununsable	Not really, without knowing where the info came from/how large of a sample size	Probably model Y	Because both groups have an equal opportunity 	Models X and Y are equally biased	Both have their own bias 	Models X and Y are equally useful	I don't believe either are very useful	0	I feel like I need more information, i wouldn't choose either	Definitely ${e://Field/pref_model}	Both groups have an equal chance at the beginning, the mistakes could be variance	Both ${e://Field/pref_model} and Z are equally biased	They both show their own bias	0	Like the other graphs, the information still feels incomplete and doesn't help me reach a conclusion	model Y	majority	Caucasian	1.5	1.5
61689ec20ea785be6b264b16	icu	2	If they don't need ICU support they may receive drastic care that they don't need.	2	ICU requires a different kind of care.	2	They may not need ICU. Society is the general public.	2	ICU is greatest form of care. Different levels of care. Society is the general public	Yes	Yes	Yes	Disadvantaged	Acceptably fair	The data it has I believe it made the best choice	Neither biased nor unbiased.	It's positivity rate was better than its mistakes	Mostly useful	It had a good positivity rate	Acceptably fair	Model y is not perfect as stated but to the best of its ability is has done its job.	Mildly unbiased	Its working to the best if it's ability.	Mostly useful	I believe it is performing to the best of its ability with the knowledge it collected	Probably model Y	It seemed to be equal as a whole	Probably model Y	Equality in my opinion	Probably model Y	Seems to be less favorable on either side	1	I believe it is not as biased as x	Probably ${e://Field/pref_model}	Model y seems more equal	Probably model Z	Model z favors Caucasian 	-1	Equality	model Y	majority	Caucasian	2.0	2.0
61093fd0ed1b90dc46d24d69	frauth	2	This can be potentially dangerous for staff & patients 	1	It is time consuming, but won’t prevent a staff member from getting where they need to go	2	If society consists of patients and concerned family members, they may find the presence of non-authorized individuals to be dangerous and scary 	2	It may take longer for a medical professional to address patients and conferences family members (I.e society)	No	Yes	Yes	Advantaged	Very unfair	The probability of it granting access to non-caucasians is much lower 	Acceptably biased	The face recognition does not work well for everyone 	Mostly unusable	The face recognition does not work well for everyone, but the incidence of mistakenly granted access is lower so that part is a step in the right direction 	Acceptably fair	Yes as the probability of it working is equal for caucasians and non-caucasians	Neither biased nor unbiased	Not really, the probability of it working is the same for each group 	Mostly ununsable	It seems there is a large incidence of mistakenly granted access	Definitely model Y	Model Y has equal probability of the face recognition working for all groups 	Definitely model X	The probability of the facial recognition working for the non-Caucasian group is much lower 	Probably model Y	The probability of granting access is better overall 	1	For the same reason as stated above 	Both ${e://Field/pref_model} and Z are equally fair	Model Y has a better probability of the facial recognition working for the non-Caucasian group but model Z has a lower probability of denied access for the non-Caucasian group so they balance out 	Both ${e://Field/pref_model} and Z are equally biased	The probability of facial recognition it equal in one but the probability of getting denied is high, vs the probability of facial recognition is low in the other but the probability of getting denied is also low 	1	Mistakes of denied entry are equal between groups 	model Y	minority	Caucasian	2.0	1.5
5c94745e6014f5001231d322	frauth	1	This seemed like the appropriate weight for measurement 	2	Medical records are very private  	1	Society comes complacent 	1	Hospital staff have varying levels of access and bo varied level of knowledge 	Yes	Yes	Yes	Advantaged	Mildly unfair	The probability is extremely slanted 	Mildly unbiased	It is not leaning one way or another	Mostly useful	It has consistent data time and time 	Mildly unfair	The probability is very slanted in this model 	Neither biased nor unbiased	The probability is very even 	Mostly useful	It has consistent results 	Probably model X	It has more consistent results and probability 	Probably model X	It has more consistent results and probability 	Probably model Y	It has more consistent results and probability 	0	It has more consistent results and probability 	Probably ${e://Field/pref_model}	It seems this model has been tested more thoroughly 	Probably ${e://Field/pref_model}	I like the title	-1	The data seems unobjectionable 	model Y	minority	Caucasian	1.0	1.5
61700ceea50b32716181325c	frauth	2	Anyone can get access to anything and could cause people injury or other harmful things.	2	Because anyone could watch what they type and learn from it.	2	If it got out that medical records got out, the news would cover it and it would affect the whole world (society). 	0	Unsure	No	Yes	Yes	Advantaged	Acceptably fair	Because when it found a mistake is had a equal opportunity to be any race	Acceptably biased	It has a lower chance to grant access to Non-Caucasian, but has an equal chance to mess up with Caucasian.	Mostly unusable	Simply because it is less likely to recognize Non-Caucasian	Very fair	Because they are both equal	Neither biased nor unbiased	Because it is equal	Very useful	Because it has equal opportunity to be even between Caucasian and Non-Caucasian.	Definitely model Y	Equal points of granting access.	Definitely model X	Less Non-Caucasian weren't granted access.	Definitely model Y	Because it gas equal opportunity to give access to any race 	2	Because it gives equal opportunity to be granted access	Probably ${e://Field/pref_model}	Unsure	Probably ${e://Field/pref_model}	More Non-Caucasian denies were mistakes	2	Less mistakes	model Y	minority	Caucasian	2.0	1.0
612a7b51f5c0574e3a0f4003	rent	2	The landowner will not be able to cover the current expenses without getting paid. 	1	The parties are both notified when this happens, and because the payment was real, the funds were returned and can just be paid again. 	1	Society would be anyone using the prediction model as a way to obtain payments. It would decrease their trust in the system and it would have to be built up again.	2	Society could be credit card companies and banks. If they think someone might have a bad reputation, they might not be allowed to get loans or apply for new credit cards. 	Yes	No	Yes	Advantaged	Mildly unfair	Because if you are non-Caucasian, you are more likely to have an allowed payment. 	Acceptably biased	Because even though the errors are the same percentages, there is a higher chance a Caucasian will be denied. 	Mostly useful	Even if it was slightly biased, the percentages of errors did provide helpful data that the errors were made without any influence of race.	Very fair	Everyone has equal probability of an allowing payment.	Very unbiased	It gave everyone an equal chance of getting approved, but it displays how non-caucasians are more likely to have been fraudulently paying.	Very useful	It had an unbiased approach.	Definitely model Y	The probability of an allowed payment is equal for everyone, and this is not the case in Model X. It is only in the mistakes that Model Y is revealed that more mistakes are made with non-caucasians. 	Definitely model X	It did not allow everyone's payments to be allowed equally. 	Models X and Y are equally useful	Model X shows that even with initial bias, the mistake percentages remain the same, and so the allowed payments should later reflect that. Model Y gives everyone the equal opportunity, but reveals where there are more mistakes made. 	1	I think it would reveal more about why the software allowed the people who were later revealed as mistakes to get through the approval in the first place.	Definitely ${e://Field/pref_model}	Model Z is pretty much the exact same as Model X where one group is more likely to get approval.	Definitely model Z	It does not give everyone the same chance of being approved.	-2	It shows a lot better than when the payments are equally offered, they are more wrong when it comes to caucasians.	model Y	majority	Non-Caucasian	1.5	1.5
6107d9f8e3bcf0b428aee973	rent	1	Fraud requires a lot of issues	1	Depends on the size of the complex.	1	Society is the community around the place where they are renting. Allowing the payment can interfere with their finances as well as credit.	2	Part of society includes those within the community directly. Denying the transaction can interfere with finances and maybe their credit	Yes	Yes	Yes	Advantaged	Neither fair nor unfair	I feel as though I can see both sides of why people would and wouldn’t like it. I think those who rent out their property would like to trust data and base the renting process on that. I think those who get denied the renting process would feel like they perhaps are the outlier and were not given a fair chance	Neither biased nor unbiased.	This is tricky, I think it as I mentioned above there’s reason to like and dislike it. So I struggle with if it’s bias or not.	Mostly useful	Because it relies on data. So there is some sort of reliability to it since it works on patterns and numbers.	Acceptably fair	Looks like racially it is even amongst who is allowed payment (yellow graph)	Mildly unbiased	Looks even among races.	Mostly useful	Based on numbers and patterns	Probably model Y	Even among races	Probably model X	Uneven among races 	Models X and Y are equally useful	Unsure	1	Racially even	Probably ${e://Field/pref_model}	Mostly even	Both ${e://Field/pref_model} and Z are equally biased	Comparing the two, they seem more even In their errors vs acceptance 	1	Acceptance and error seem more appropriate and even	model Y	minority	Caucasian	1.0	1.5
5e5da6ffeacf5d0ccea9d3a8	rent	2	Creates a domino effects	2	May greatly effect the individual 	2	Will affect the community and municipality	2	The individual needs to reside somewhere and the resources of the municipality may be affected if the individual is denied living arrangements 	No	No	Yes	Advantaged	Neither fair nor unfair	What’s the basis?	Neither biased nor unbiased.		Neither useful nor unusable		Neither fair nor unfair	Relevancy	Neither biased nor unbiased		Neither useful nor unusable		Probably model Y	Closer stats	Models X and Y are equally biased	Based on what information 	Models X and Y are equally useful		0	The charts are not providing additional information 	Probably ${e://Field/pref_model}	Only denied	Both ${e://Field/pref_model} and Z are equally biased	Equally	0		model Y	minority	Caucasian	2.0	2.0
5c92810addeb3d0018a19de3	rent	2	Because of the loss of revenue	2	A renter is denied the ability to find a new home and will likely feel upset about the denial	0	Society as opposed to the individual are those who are indirectly affected. The fraud affects the individual mostly through the loss of revenue. The impact on society is minimal	2	Society as opposed to the individual are those who are indirectly affected. If it affects an individual and then society as a result, then the impact is high overall 	Yes	No	Yes	Disadvantaged	Very unfair	Race issue as in model Y	Very biased	Race issue as in model Y	Completely unusable	Making business decisions based on race is prejudiced 	Very unfair	I think using race in the model is unfair but aggregating non-Caucasians into one group is very unfair.	Very biased	Two groups: Caucasians and everyone else. It's very presumptive.	Completely unusable	Making businesses decisions based on race is prejudiced 	Probably model Y	The only choice I had; I would have prefer to answer that both models are equally unfair.	Probably model X	The only choice I had; I would have prefer to answer that both models are equally biased.	Probably model Y	The only choice I had; I would have prefer to answer that both models are equally unusable 	0	For all the reasons stated in previous responses.	Probably ${e://Field/pref_model}	It predicts equal payment probability for whites and non-whites	Definitely model Z	It allows significantly less payments non-whites despite equal payment  mistakes among both groups.	0	Race bias, especially as it relates to renting a home. Same bias occurs with home lending.	model Y	minority	Non-Caucasian	1.0	2.0
60fca4ea3370270b8bf97614	rent	0	These things can be fixed, however a relationship with the renter will be harder to repair. 	2	The renter will feel degraded, offended, and like trust has been broken, When they signed the lease they said they could pay that amount, it is rude to doubt that 	0	Again, money can be sent back/covered/etc. but it will be harder to mend the relationship with the person who is paying you monthly 	2	(Americans) Would find this extremley offensive if it was shown to have a direct correlation to race. We are in a very sensitive time and need to show respect to all. 	No	No	Yes	Advantaged	Mildly unfair	Still biased	Very biased	Looks like a more fair model but unsure of how the ethnicity is being determined. 	Neither useful nor unusable	Bias and variables 	Mildly unfair	I feel like there can be bias which makes it unfair, there are too many variables. 	Very biased	Very broad, caucasion/non caucasion, how are these being reported? Is the individual stating this themselves or is someone else 	Neither useful nor unusable	Too many variables	Probably model Y	The probability is equal, while the mistakes are based off of the accepted payments 	Definitely model X	Payment should be taken regardless of race, the mistakes are not based off of this factore	Probably model Y	Still too many variables and bias but it makes the most sense out of the two 	1	Again, out of the two this seems like the better option, Allowing payment shouldn't really be compared to the mistakes. Its not a direct correlation, too many variables and too much bias. 	Probably ${e://Field/pref_model}	Mistakes cannot be predetermined, especially based off race	Definitely model Z	Disproportional percentage of payments allowed to be accepted 	-1	Accept the payment and deal with the risk after, potential racism isn't worth declining a payment 	model Y	minority	Caucasian	0.0	2.0
6144834d136ed7f27d2dbe60	frauth	2	Peopjj oh es’ lives could be permanently s as layered or people could be killed due to a mistake by unauthorized people.	2	Unauthorized people can access records which are potentially embarrassing or have access to restricted drugs.	1	Society is the local public who might be consumers at the hospital system. The impact could be potentially huge-even impacting whether patients seek health care from a hospital system the have lost faith in: 	1	The general public can easily lose faith in the hospital syatem	No	Yes	Yes	Advantaged	Very unfair	There’s too big a discrepancy between approval granted to Caucasian vs. Non-Caucasian subjects.	Very biased	Too big a discrepancy between approval rates between Caucasian snd Non-Caucasian subjects.	Mostly unusable	Not enough Non-Caucasian people eeee recognized and granted access.	Neither fair nor unfair	Both groups of people, Caucasian snd Non-Caucasian, were equally granted access.	Mildly unbiased	A larger percentage  of mistakes was made with Caucasian subjects. 	Mostly ununsable	The rate of error is too large	Probably model Y	Smoker approval rates between the two groups.	Probably model X	Greater discrepancy between the two groups in terms of approval. 	Models X and Y are equally useful	Both have troublesome aspects.	1	The problems with X seem less extreme.	Probably model Z	Less discrepancy in errors between the two groups.	Definitely ${e://Field/pref_model}	Bigger discrepancy  in errors between the two subject groups.	1	Less difference in error percentages between the two subject groups.	model Y	minority	Caucasian	1.5	1.5
61743e225681c2932a38ba85	frauth	2	The unauthorized user has access to important information that might be harmful to society.	1	It is still important that the right person is identified correctly.	2	Important information in the wrong hands.  The patient and relatives of the patient.	2	Important information might get out to someone who can use it to benefit themselves.  society would be the family of the patient and the patient.	Yes	Yes	Yes	Advantaged	Mildly unfair	It grants access to non medical personnel 	Neither biased nor unbiased.	It is a computer program.	Mostly unusable	It would lock out medical personnel to often.	Acceptably fair	The Model is equally successful. 	Neither biased nor unbiased	Equally successful	Neither useful nor unusable	The margin of error seems to be to high	Probably model Y	It has less chance of not granting access to medical staff	Probably model X	I would not say it is biased but it has more trouble recognizing Caucasians	Probably model Y	It would allow more medical staff to log in	1	It would not lock out medical staff as much	Probably model Z	Model Z has a lower chance of making a wrong choice and a higher chance of making the right decision.	Probably model Z	It has a higher chance of granting access to one group than the other.	1	Less chance of locking medical personnel out.	model Y	majority	Caucasian	2.0	1.5
610957fa1a38ec442cb5e5b2	frauth	1	Information may be shared, but lives are not necessarily at risk.	2	Then hospital staff would be delayed.	1	The exposure of private information can be negativity impactful to society, aka the patient. 	1	With a backup in place, doctors would still be able to help society, aka the patients. 	No	Yes	Yes	Advantaged	Mildly unfair	The probability of granting access is higher for caucasians.	Very biased	The probability of granting access is higher for caucasians.	Completely unusable	The probability of granting access is higher for caucasians.	Neither fair nor unfair	The possibility of granting access is the same. 	Neither biased nor unbiased	The possibility of granting access is the same. 	Mostly ununsable	The percentage of mistakes is high.	Probably model Y		Definitely model X		Probably model Y		1		Probably ${e://Field/pref_model}		Probably model Z		-1		model Y	minority	Caucasian	1.0	1.5
61605437b994ff2aafa9588a	frauth	2	Individuals may not always be the best humans and may use confidential information to bring others harm	1	Data is often important to access in emergencies to help with the complications	2	Allows for theft or access of sensitive information	1	It may or may not have an impact on society depending on the individual	Yes	Yes	Yes	Advantaged	Neither fair nor unfair	It grants the same results but different access	Acceptably biased	It has a different access rate 	Mostly unusable	It shows bias or skewed data	Neither fair nor unfair	It represents the ratios correctly	Acceptably biased	It is biased due to the same percentages changing in the next graph	Neither useful nor unusable	The graph could be useful for future reference 	Probably model Y	It has the same probability to grant access	Probably model X	It shows significant bias	Probably model Y	Can correctly depict unskewed data	1	It has the more fair data	Probably ${e://Field/pref_model}	Has equal forms of access	Probably model Z	Shows how the ratios are different	-1	It is the more fair choice	model Y	minority	Non-Caucasian	2.0	1.0
60fd048315fb2282c4d5a150	rent	1	This allows them to have a place to live.	1	They might not have any other option concerning having a place to live.	2	A fraudulent payment might allow for a renter with a tricky past to live around others potentially causing danger. Others living nearby would be considered with society.	0	Considering the other people who live in the renters complex, I would say the impacts are low because changes to the environment would be minimal.	Yes	No	Yes	Disadvantaged	Mildly unfair	The percentages for the mistakes only among allowed payments seems high.	Acceptably biased	The graphs that were created were most likely biased in some way because bias is inherently everywhere.	Mostly unusable	The information provided was not very useful because of the high percentages pertaining to the mistakes.	Mildly unfair	High percentages for mistakes.	Mildly unbiased	The initial percentages were completely even.	Mostly ununsable	The high percentage among the mistakes.	Probably model Y	Because the probability of allowing payments for both caucasian and non-caucasian people is even.	Probably model Y	Even ratio.	Models X and Y are equally useful	They both show different information that is valuable to the research being done currently.	1	Despite having an even ratio for the initial graph, the graph below illustrates an interesting question of whether bias exists beyond initial payments.	Probably model Z	The ratio is even for the mistakes only among denied payments graph.	Probably model Z	There is an increased likelihood that non-caucasians will have allowed payment.	1	Even ratio for the second graph.	model Y	majority	Non-Caucasian	1.5	0.5
6111432635ee221fd14640db	rent	1	The fraudulent payments only impact the landlord so it does not actually impact the renter. It is moderate though because they can get caught and denied service.	2	Some renters may have multiple bills to pay in the beginning of the month so a denied payment may risk the renter of having other money pulled out, which will make them short on rent.	0	I considered society to be the American public because many of them are renters. Society will only be impacted in a low manner because fraudulent payments do not actively affect society.	1	Society would be moderately impacted because if renters are being mistakenly denied their rent, they can face major consequences that can lead to late fees or evictions. I considered society to be the American public because many of them are renters.	No	Yes	Yes	Disadvantaged	Acceptably fair	The model X is fair because it is based off of statistics from previous data, there is no way it could be unbiased. The mistakes graphs are each 35%, and even if the model X allows more payments from non-caucasians that because the group had payments that were less fraudulent than caucasians.	Very unbiased	If the model X is created using data, then the numbers do not lie. It is simply analyzing data that was presented so it cannot be biased.	Very useful	If both parties are losing money due to fraudulent payments then model X can effectively prevent these fraudulent payments so nobody loses money. And even if it makes a mistake those can be remedied. 	Very fair	Yes the graphs are similar in terms of allowing payments for both groups. 	Very unbiased	Model Y is unbiased because it is going off data to prevent fraudulent payments. If it makes a mistake for a certain group, that doesn't necessarily mean it is biased.	Very useful	Because it has the same rate of allowing payments, model Y is useful! 	Probably model Y	Model Y is more fair because it has smaller gaps in both graphs than model X	Models X and Y are equally biased	Model X and model Y are both going off of data so they cannot be biased in my opinion.	Probably model Y	The smaller gaps in errors and allowances makes model Y more useful than model X.	1	The model Y seems to be more accurate than model X.	Probably model Z	Model Z has mistakes equally for caucasians and non caucasians.	Both ${e://Field/pref_model} and Z are equally biased	They are both going off of data, and in my opinion, cannot be biased. 	1	Model Z is better because it is more accurate in terms of mistakes in payments being denied.	model Y	majority	Non-Caucasian	0.5	1.5
6028809b7057061e0ede2cb6	frauth	0	From the perspective of an individual, being accidentally let into a hospital system doesn't change someone's life by itself.	0	Hospital staff that are rejected by the system can just type in their username and password. It is a mild inconvenience.	2	If there is a significant chance that some unauthorized can get in, then there is a chance that person has malicious intent. The person can feasibly steal all medical records they can find, which already has drastic implications for those whose privacy was violated. If such a system exists, I find it likely that they could even bring the system to a halt for ransom with a sort of malware, which will have incredible impact for anyone in need of the hospital.	0	I cannot imagine any societal impact if a doctor is inconvenienced and has to manually enter a username and password.	No	No	Yes	Advantaged	Neither fair nor unfair	The accuracy of model X cannot be determined from this data, so I cannot determine whether model X is fair.	Very unbiased	The model accepted Caucasian people and Non-Caucasian people at a significantly different rate, but I don't know what the sample of people look like. There could be more Caucasian or Non-Caucasian people and more people who are or are not doctors between those groups. The fact that the false acceptance rate is the same suggests that the algorithm is unbiased with regards to this difference towards those that were accepted.	Completely unusable	This model also has an unacceptably large false acceptance rate. Implementing it would be a security disaster.	Neither fair nor unfair	The presence of mistakes implies that there are doctors and people that are not doctors in the sample. I do not know the proportion of either in the group or the success rate so I cannot determine whether the model is fair.	Acceptably biased	It appears that a Caucasian person is twice as likely to mistakenly be granted access compared to a Non-Caucasian person. I think that even a 24% of granting instant access to someone who isn't a doctor is unacceptable and that the model should be discarded. The bias is acceptable since the model shouldn't be put into place anyway, and it should be investigated.	Completely unusable	The model should never grant unauthorized permission to someone who isn't a doctor. The expected percentage of failure should be miniscule. This model is entirely unusable.	Models X and Y are equally fair	I would prefer another option here, but since I cannot determine whether either one is fair or not, they are the same in that way.	Probably model Y	The rate of false acceptance is different for only model Y.	Models X and Y are equally useful	They are both a horrible idea to use.	0	Fairness and bias aside, the false positive rate is unacceptable for both models. They are both useless.	Both ${e://Field/pref_model} and Z are equally fair	I cannot determine whether either model is fair, because I do not know anything about the sample.	Probably ${e://Field/pref_model}	As far as false rejection is concerned, model Y appears to be biased against Non-Caucasian people. I don't know what this graph would look like for false acceptance for model Z, but in this regard model Y appears to be more biased.	0	I do not know the false acceptance rates for model Z. False denial is less of an issue for security, so it's difficult to determine how useful model Z is. Since model Y is unusable and I cannot determine the usefulness of model Z, I will opt to choose neither model here.	model Y	minority	Non-Caucasian	1.0	0.0
6161d8d8b181d894ff32d83e	frauth	2	I don’t want a non doctor access my med records 	2	Doctors time are sparse it could save them time 	2	Society is patients 	0	Society is people non medical outside of medical 	No	Yes	Yes	Advantaged	Mildly unfair	In the top graph it’s highly unfair and in the bottom graph it granted access to everyone. 	Neither biased nor unbiased.	A machine can’t be biased. 	Mostly unusable	Until it’s accurate it’s going to keep making mistakes 	Mildly unfair	It mistakenly grants more access to non caucasions	Neither biased nor unbiased	Machines cannot be biased	Mostly ununsable	Cannot be used until it’s accurate	Probably model Y	Y granted access to the same individuals and the margin of error was low on the granted acces mistakes	Models X and Y are equally biased	Machines can’t be biased 	Probably model Y	It correctly granted access to both Caucasian and non caucasions 	0	No machine is accurate	Definitely ${e://Field/pref_model}	More accurate 	Definitely model Z	Comparable	-1	I don’t know really this is confusing 	model Y	majority	Non-Caucasian	2.0	1.0
61522cca7870d26be8d1ace7	icu	1	i believe that it’s moderate because even you are wrong, they wont lose their life.	2	if someone actually need icu support and you mistakenly deny it, it could cost them their life.	1	I think it would be moderate because either they wouldn’t have to risk a patient losing their life and i consider the doctors/people in charge of the patient as part of society.	2	It would be very harmful to that hospital to know you mistakenly denied someone who needed support. I believe any doctor or nurse would be apart of this aswell.	Yes	No	Yes	Disadvantaged	Neither fair nor unfair	I think its neither fair or unfair because the mistakes are both the same percentage.	Acceptably biased	its biased because the caucasians have a much higher chance of needing ICU support than non caucasians.	Mostly useful	its useful because it shows the mistakes being made.	Acceptably fair	they both have a close probability.	Acceptably biased	they both have the same probability rate but the caucasian chart has a 45% mistake rate and the non-caucasian one has a 24% rate.	Mostly useful	It shows which how many patients are usually mistaken.	Probably model Y	model Y because both categories have the same percentage of being granted icu support.	Probably model X	the caucasian category has a higher probability of being granted icu support than the non caucasians.	Probably model Y	since both categories are given a fair chance at ICU support, the result seems more accurate.	2	same as the previous answer, it seems like the result would be more accurate since everyone is given a chance.	Probably ${e://Field/pref_model}	they have an equal amount of people being given icu support.	Probably model Z	the caucasian category is given a higher chance of icu support	1	even though they give a higher chance to one category, they both have the same percentage of mistakes.	model Y	minority	Non-Caucasian	1.0	2.0
616113bd199b4713b81ebc51	rent	2	It allows the view of payments to be different for the tenants because they can view the group that allows fraudulent payments as a group that continues paying with no issue but sees the other group as a problem.	2	High because it can be given huge biases within both groups and it can make the tenant be more hard on the renter.	2	For the non-caucasian group when it's allowed mistakenly they can still be caught and seen in the worse light and even face consequences that the caucasian group might not face.	2	For the non-caucasian group, it could be very impactful because of the stereotypes and biased opinions on those groups.	Yes	No	Yes	Disadvantaged	Mildly unfair	They're saying that both groups have a different probability of paying because of the fraudulent detection when both groups have the same mistakes only among allowed payments.	Very biased	Very biased because they're accepting fraudulent payments in the caucasian group and not taking it into account in the probability of payment.	Mostly unusable	It's mostly unusable because of the first graph in the model.	Neither fair nor unfair	I think it's neither fair nor unfair since it's saying caucasian would mostly cause mistakes only among allowed payments when there's a 50/50 chance anyone can do it but also in this case there is a chance of it happening so I don't think it's too unfair.	Neither biased nor unbiased	It's a bit biased because it's caging a race.	Mostly useful	It's mostly useful because of the same percentages each group has in the probability of allowing payment.	Probably model Y	Mostly because of the percentages in the first graph of model Y but it's still not too fair because it doesn't take into account the mistakes only among allowed payments in the second graph towards the first graph.	Definitely model X	Model X is more biased because the probability of payment is still high even if there's fraudulent payment when the non-caucasion grouping has the same percentage but has less probability of payment.	Probably model Y	Model Y is a bit more useful but still not too useful because of the percentages 	1	It seems to have a little more acceptable information but still not too useful.	Probably model Z	Model Z has the same percentage in mistakes only among denied payments but the probability of allowing payment is different and shows a lower percentage to the non-caucasian group and a higher one for the caucasian group.	Both ${e://Field/pref_model} and Z are equally biased	They're both biased because of the same way too different percentages within both graphs.	-1	It shows the same probability in payment but is still hesitant because of the different percentages in the second graph.	model Y	minority	Non-Caucasian	2.0	2.0
579fbb4a6f13c2000174aafc	rent	1	It could be a good positive net benefit for the individual 	1	It seems like an easy thing to rectify and it doesn not really affect yet it could be troublesome. 	0	I think this mostly effects the renters class of society and this would have a low to marginal effect on their economic bottom line	2	I think as society as whole, meaning what we as the US view as equitable, if there is bias in it, there can be generational inequality as a result. 	No	Yes	Yes	Advantaged	Very unfair	It allows more payments from whites than non whites	Very biased	In that it allows many more payments rom whites than non whites.	Mostly unusable	It is very biased and would been seen as a racist algorithm	Mildly unfair	It allowed a disproportionately high percent of white payments verse non white payments. 	Acceptably biased	It allows more bad payments from whites than non whites.	Neither useful nor unusable	I am not sure, I would have to compare it to other models to make a better determination. 	Probably model Y	It allows an equal proportion of payments. 	Probably model X	The allowed payments seems to be a bigger metric of bias in my mind	Probably model Y	It allows an equal payment and may be easier to adjust the rejected portion	1	Model Y seems to be a better representation and carries less bias. 	Probably ${e://Field/pref_model}	I give more weight to equality in allowing payment/ 	Probably model Z	I think allowing less non whites to make payments is worse than a disproportion in rejecting payments	-1	I like equity in the probability of allowing payment. 	model Y	minority	Caucasian	0.5	1.5
578c06326475d400015aa125	icu	1	It will take away a bed from someone that needs it more. However, if someone that doesn't need ICU is put into ICU, they would quickly get taken out of ICU once someone deems them healthy enough.	2	If someone needs ICU support but doesn't get it, their health will likely quickly deteriorate.	1	Society is everyone else that could potentially end up in the hospital. An ICU bed could be taken away from somebody that needs it but it would be temporary	2	Society would be those that are very ill and need ICU support. Without it, they could die.	No	No	Yes	Disadvantaged	Mildly unfair	There's a 20% difference between caucasian and non-caucasian, it seems pretty lenient towards caucasians to get ICU 	Very biased	It is obvious there is a bias towards non-caucasians	Neither useful nor unusable	I think this information could be useful to many people but because of the mistakes it makes, it's not 100% useful.	Very fair	The predictability is the same percentage so I think this is fair.	Very unbiased	Because the predictability is the same, it doesn't seem biased at all	Mostly useful	I think it is mostly useful because mistakes are still made but the information should still be useful	Definitely model Y	The predictability is the same in Model Y and I think that's the basis of the entire modeling system	Definitely model X	It is definitely more biased towards caucasians	Probably model Y	I think both can be used for useful information but Y is more useful for equally predicting those for ICU	2	It feels more equal and fair	Definitely ${e://Field/pref_model}	The predictability is the same so it feels fair	Definitely model Z	A 20% difference is going to feel more biased, of course	-2	More equal	model Y	minority	Non-Caucasian	1.0	2.0
6175db12b41a9de303906935	frauth	0	It doesn't help the medical device to access the users.	0	Without the username/ password, the authorized hospital staff will not get access to an individuals .	0	It doesn't help the help the society to to be recognized in the in the device access	2	To help the society to recognized in the authorized hospital staff device.	Yes	Yes	Yes	Advantaged	Very fair	The probability of granting access to the medical device is the likelihood of someone's need.	Very unbiased	The someone's request to the medical device access is granted.	Very useful	As the medical device is used to satisfied someone's request with the access to the device.	Acceptably fair	As the probability of granting access is the same to each other.	Very unbiased	The device serves the needs of the all together.	Very useful	It is useful for all the request of the device access.	Probably model Y	Because, all of them have the same access to the medical device as it should.	Probably model X	Their access to the medical device is not the same to each other.	Definitely model Y	They all get the same access to the device for it operations. 	1	Model Y has the same access to the medical device for it operations.	Definitely ${e://Field/pref_model}	As compared to Z, it has the same access of denied personnel of the medical device.	Probably model Z	Their access to the medical device is not the same as to the Y.	-1	Their access is the same and has a unique form to the medical device	model Y	minority	Non-Caucasian	0.0	1.0
5adef850eb60400001539109	rent	1	It's a loss of money but not the end of the world.	1	They can likely make things right with little trouble.	1	Someone will likely have to pay for this. The landlord or whoever is to blame would be the society.	0	The individual can likely fix this on their own. Their family may not even find out.	No	Yes	Yes	Advantaged	Very unfair	The model allows more payments by Caucasians despite making the same amount of mistakes on both groups.	Very biased	Caucasians are favored.	Mostly useful	It allows a relatively low amount of fraudulent payments.	Acceptably fair	It doesn't consider race.	Very unbiased	No bias is shown.	Mostly useful	It seems to not allow to many fraudulent payments.	Definitely model Y	It does not consider race.	Definitely model X	It takes race into consideration.	Models X and Y are equally useful	They have similar failure rates.	1	I would want to make the fair choice.	Definitely ${e://Field/pref_model}	Model z allows more caucasian than non caucasian payments.	Definitely model Z	It is biased in favor of caucasians.	-1	I'd like to make the moral choice.	model Y	minority	Non-Caucasian	1.0	0.5
6107a4b7744b9b0ca7328cee	rent	2	When you allow a fraudulent payment it can severely affect that person by loss of income and potentially effecting their credit score.	2	If denying a payment leads to a late payment because of this mistake it can mean a lot. A lot of renters live paycheck to paycheck.	0	Society being the community you live would be affected by fraudulent payments is low because I feel like this would the company that allows fraudulent payments happen would be jeopardized.	1	If this mistake causes a renter to lose their apartment then society as in the community you live in would be affected by their homelessness. 	Yes	Yes	Yes	Disadvantaged	Acceptably fair	It’s fair enough with only a 24% difference.	Acceptably biased	It’s a 24% difference which isn’t a big deal with payments allowed. If it was mistakes allowed and their was a difference then it would matter more.	Mostly useful	The 24% doesn’t seem like it matters in the grand scheme of things.	Very unfair	21% difference in mistakes allowed in the non-Caucasian category	Very biased	It’s unacceptable to allow that many mistakes happen with the non-Caucasian category.	Completely unusable	It’s useless if it has that much of a disparity between non-Caucasian and Caucasian.	Probably model X	It’s not completely fair to the Caucasian category but it’s acceptable enough.	Definitely model Y	45% in mistakes allowed for non-Caucasians vs only 24% mistake with Caucasians.	Probably model X	Model X is probably useful because it shows less of a bias.	0	The percentage difference between the both of them is too much to actually be useful.	Probably ${e://Field/pref_model}	I think the 23% of mistakes made is fair enough, allowing payments seems to matter more.	Both ${e://Field/pref_model} and Z are equally biased	They are both biased against the Caucasian group. With about a 20% difference in allowed and mistakes.	0	I wouldn’t choose either of them because they are equally biased so it’s not a difficult choice.	model Y	majority	Non-Caucasian	1.0	1.5
5c1ac9a924e01a000144e64a	rent	1	It could mess with their reputation.	1	Becuase they won't be able to pay their rent.	0	Because it doesn't matter to the random person.	0	Because it doesn't matter to them. The society can be a person standing on the side of the street who has no concern with this.	Yes	Yes	Yes	Advantaged	Mildly unfair	Because it shows that non-caucasians have a low probability of having their payment accepted.	Mildly unbiased	Low probability of non-caucasian payments being accepted.	Mostly unusable	It's a little biased.	Very fair	Because it is equal probability for caucasians and non-caucasians of the payment being aapproved.	Very unbiased	Because it should both ethnicities.	Very useful	Because it unbiased.	Definitely model Y	It's more equal in treatment.	Definitely model X	It's more biased against non-caucasians.	Definitely model Y	Not biased.	2	It's not biased.	Definitely model Z	It's more equal in probability.	Definitely ${e://Field/pref_model}	Higher percentage of non-caucasians being denied.	1	Less biased in payment denial.	model Y	minority	Non-Caucasian	0.5	0.5
610af4dc507f06195b6d99fd	frauth	2	don’t know a person’s intention 	1	it’s a long process but no risk	2	society involves anyone in this world. don’t know a person’s intention	1	society involves anyone in this world. it is a long process but no risk	No	Yes	Yes	Disadvantaged	Mildly unfair		Neither biased nor unbiased.		Neither useful nor unusable		Acceptably fair		Neither biased nor unbiased		Neither useful nor unusable		Probably model Y		Models X and Y are equally biased		Models X and Y are equally useful		0		Probably model Z		Both ${e://Field/pref_model} and Z are equally biased		0		model Y	minority	Non-Caucasian	2.0	1.0
60fefceabeb266acf92730e3	rent	1	Coud be accidental	1	Between good and bad	0	My sister still for support	2	My sister for situations like these	No	Yes	Yes	Disadvantaged	Neither fair nor unfair	they have different ratings.	Neither biased nor unbiased.	Yes because it talks about race	Neither useful nor unusable	In a way yes to help scales	Neither fair nor unfair	Ratings	Neither biased nor unbiased	Yes the race	Neither useful nor unusable	Good to keep count	Models X and Y are equally fair	Rating	Models X and Y are equally biased	The race	Models X and Y are equally useful	The ratings are clear	1	X only becase its more fair	Definitely ${e://Field/pref_model}	It seems more organized	Definitely model Z	Z seems less organized	1	Y more organized	model Y	minority	Non-Caucasian	0.5	1.5
5d497f8d3c428d0017121681	icu	1	There is always the possibility of mistake,  but when a machine  is making the decision for an individual the chaces are geater. I could be okay compated tothe huy next to me, so the machince give the icu bed to him, when really he was on the mend and soon if be in need of it.	2	There is a great risk and chance there will be more mistakes that correct outcomes.	2	Any time thete are different ethnic goups involved they make up society, thus if the algorithm for the machine is already biased toward one race more than the other the truth is there will always be more mistakes.	0	The different ethnic groups around me make up the society around me , therefore I do believe that the chance of mistakes are greater The people of ethnicity like myself Minorities tend to always get the short end of the stick so to speak speak so now you've got a machine that is going to make decisions for you based off of past data. Will that pass data might show that there were more caucasians at 1 point in time that would have needed an icu bed but that's not the case now.. So I believe that there is a greater chance for mistakes using a machine like this in today's society.	No	Yes	Yes	Disadvantaged	Mildly unfair	Truth be told, i beilieve it's  a 50 50 shot as to what race will need a inc bed. There's no model that can actually predict that 	Very biased	Its showing more Caucasians will need an icu bed, but the better question would ve what is the predominant ethnicity of people in the atea? If it's  well blended then there's a 50 50 chance.	Completely unusable	A achine can't tell who will deteriorate faster 	Very fair	This is more like it as theres no guarantee what race will get sick. Its a 50 50 shot.	Very unbiased	Cuz no matter where you are theres always an equal vhance any race can get sock and need a vent, not just one kore tha the other.	Mostly ununsable	I would never tust an algorithm in a machine to faily choose who will need an ice bed. 	Probably model Y	As i stated before its a 50 50 chance as to which race will het most sick and need an icu bed. 	Definitely model X	It's favoring Caucasians when that may noy accurately describe the area in which the live.	Probably model Y	Personally i wouldve chose neither if that uad been an optio , bt since it wasnt i just chose the optuon that was most fair.	0	Again i would never trust an algorithm to make a proper medical decision for me or my family!	Probably ${e://Field/pref_model}	Wjen looking at the dadta it is most equal.	Probably model Z	Again this model faors Caucasians. 	0	Ill never trust a machine to have my back.	model Y	minority	Non-Caucasian	1.5	1.0
6108bbc0999069998b981ffd	rent	1	If an individual is allowed to rent with a fraudulent payment it doesn't impact the individual as much as it does the owner of the complex	2	If a renter is able to pay but the algorithm denies it then both the rentor and the rentee miss an opportunity. 	1	Society for this question refers to the community that the apartment complex is within. If fraudulent payments are going through it moderately affects the society because apartment complex owners are missing out on their income which could impact the residence of the building by not having money to keep up with upkeep and maintenance	2	For the answer I am considering society being the community that the apartment complex is within. If renters are mistakingly denied their transactions it has a major impact on society because Neither the rentor or the rentee are able to get their moneys worth and could lead to a home insecurity within a society	Yes	No	Yes	Advantaged	Acceptably fair	Because both of the mistakes are the same percentage. I do think that non-cacausians are able to make a payment more often but I do not feel this causes it to be unfair because I don't have enough data to know why more caucasians did not get allowed 	Acceptably biased	Because the model leans towards non-Caucasian	Neither useful nor unusable	I don't see how this model would help in real world application 	Mildly unfair	Because non-Caucasians have more mistakes for payment which means that the technology has denied them based on whatever information it was given by the realtor. 	Acceptably biased	Because it seems to favor caucasians	Mostly ununsable	I dont see the practicality in real world application. 	Probably model X	Because it allows payments from both groups but it makes the same mistakes for both equally. 	Probably model Y	Because it is more mistakes towards non-Caucasian payments. 	Probably model X	It seems the the better option between the two. 	0	I don't see the use in either of them	Probably model Z	Because I feel that model Z is more fair	Probably ${e://Field/pref_model}	It seems to deny Caucasian cards more often but maybe the rich white men are trying to buy up properties	1	It seems more fair	model Y	majority	Caucasian	1.0	2.0
61143b4ddb431c6a3179bd66	frauth	2	People shouldn't be able to access others medical records 	2	Because if they needed access to something quickly and couldn't get it, people's lives could be at stake 	2	Society is everyone who may need help from doctors/nurses. People's medical records could be in jeopardy.	1	Society is everyone who may need help from doctors/nurses. In this situation, it may take longer for staff to be granted access, when that time could be needed.	Yes	Yes	Yes	Advantaged	Acceptably fair	Because the system isn't perfect and it seems to be fair for mistakes only among granted access	Neither biased nor unbiased.	There is a possibility that it could be biased due to the probability of granting access percentages. However, I do not think that the difference is substantial and could possibly be for other reasons.	Mostly useful	When used correctly, it will help physicians and nurses!	Acceptably fair	Because it grants access to most people regardless of ethnicity 	Mildly unbiased	It doesn't prohibit people access based on ethnicity 	Mostly useful	It will help doctors and nurses to do their jobs better	Probably model Y	Because it grants people access regardless of their ethnicity 	Probably model X	Because the percentages of granting access are not even	Probably model Y	Because it provides everyone with access	1	Because it is more inclusive 	Probably ${e://Field/pref_model}	Because the percentages are closer	Both ${e://Field/pref_model} and Z are equally biased	Both have pros and cons and provide different people with access 	-1	It is less biased when granting access 	model Y	minority	Caucasian	2.0	1.5
5d6691bf6c7981001a7fdbd6	icu	2	someone may lose their life as not being predicted who needed the ICU given to the mistaken individual	2	The individual will not receive care they needed in a timely manner	2	From the view of society it would cause distrust	2	Will cause distrust	Yes	No	Yes	Disadvantaged	Very unfair	We shouldn't be predicting anything based on a person's race/skin color even if it is based on cultural genetics of that person's ethnicity	Neither biased nor unbiased.	It doesn't appear to be biased but I don't know if I have enough information to make that decision	Completely unusable	I don't think a computer or program should be making such important predictions about potential life/death scenarios. That should only be left up to human intellect and educated professionals.	Very unfair	I feel that it is unfair to use a model for predicting. That is not how real life sometimes happens.	Very biased	It appears to be biased toward one ethnicity instead of being fairly predicting based on something other that race/skin color.	Completely unusable	In my unprofessional, uneducated opinion it is not useful because I value person to person interaction and decision making. Sometimes, there are faults there as well, but it is more emotionally connected than statistics or models with percentages. 	Models X and Y are equally fair	I don't feel either are fair, because theyre both capable of making the mistake that someone will need the ICU incorrectly, which will potentially take the bed from someone who truly needs it. I don't agree with the models at all. 	Models X and Y are equally biased	They are both biased toward a certain skin color rather than being measured by illness or physiological need.	Models X and Y are equally useful	I don't have an option to choose both are equally useless	0	To re-iterate, I don't feel the models are useful in predicting such a delicate scenario. That should be left up to medical personnel.	Both ${e://Field/pref_model} and Z are equally fair	both are equally UNfair	Both ${e://Field/pref_model} and Z are equally biased	same answers	0	same answers as previous.	model Y	majority	Caucasian	2.0	2.0
614f754098c9877761eafb4b	icu	0	For that individual patient, the impact would be low as they received more care than necessary -- except they might feel long-term guilt over taking an ICU bed that a now dead patient needed to survive.	2	Because presumably an individual mistakenly excluded from the ICU will die.	2	As society includes the patients mistakenly excluded from the limited number of ICU beds that die as a result of an individual mistakenly and unnecessarily being granted an ICU bed, as well as their families, friends, doctors, nurses, etc, the impact on society is high.	2	Same answer applies: As society includes the patients mistakenly excluded from the limited number of ICU beds that die as a result, as well as their families, friends, doctors, nurses, etc, the impact on society is high.	No	Yes	Yes	Advantaged	Very unfair	There seems no statistical reason that 24% more of white patients would qualify as needing ICU care than non-whites. The fact that the error rate *after the unfair selection process* is the same is not relevant to that bias.	Very biased	Same answer: There seems no statistical reason that 24% more of white patients would qualify as needing ICU care than non-whites. The fact that the error rate *after the unfair selection process* is the same is not relevant to that bias.	Very useful	It reveals the unfair bias towards white patients inherent in the system. It is barely useful for admissions decisions however.	Very unfair	"1. It doesn't include valuations of mistakes of non-admissions to support the validity of the model
2. It dramatically grants admission of Caucasian patients who don't need ICU treatment over non-whites, which could be critically unfair in light of non-admissions mistake data"	Very biased	"It dramatically grants admission of Caucasian patients who don't need ICU treatment over non-whites, which could be critically biased in light of non-admissions mistake data.
Whether that represents a bias in the data gathering or the model is difficult to say without non-admissions and other data."	Very useful	It exposes flaws in the system that results in bias against non-white patients.	Probably model Y	At least Model Y admits white and non-white patients at the same rate. But the mistake rates still show a bias in those admissions.	Probably model X	Model X admits white patients at a higher rate than non-whites, which has no apparent basis in a non-biased model.	Models X and Y are equally useful	All data is useful, especially data that shows inherent flaws in the system or model.	1	Question states I have to choose one. Model Y is most fair.	Both ${e://Field/pref_model} and Z are equally fair	As long as the rates of admission and of errors (in either admissions or non-admissions) favor the correct treatment of white patients twice as much as non-white patients, the model is neither fair nor un-biased.	Both ${e://Field/pref_model} and Z are equally biased	As long as the rates of admission and of errors (in either admissions or non-admissions) favor the correct treatment of white patients twice as much as non-white patients, the model is neither fair nor un-biased.	1	I have to choose one. Model Z results in fewer un-treated ICU required patients.	model Y	minority	Caucasian	1.0	2.0
614ee2e25c7d068cb049d9e2	rent	1	People will get anywhere get anything for money basically and mistakely allowing fraudulent payment should be avoided at all cost	1	Because I picked moderate because it doesn't pack renter's payment because of the whole I'm trying to get into a place but it depends on if the stuff is stolen or not which is understandable so	1	Our society has been allowing fraudulent payments because the cards are stolen or people still their information and it's not OK	1	Our society is all turning into payments through the Internet through it's all online and I feel like you can still anybody's credit and denying anybody's transactions from merchants that's bad that's especially if it's fraudulent	Yes	Yes	Yes	Disadvantaged	Neither fair nor unfair	Same reason as model Y reasoning	Neither biased nor unbiased.	Same as model Y reasoning	Neither useful nor unusable	Same thing as model Y reasoning	Neither fair nor unfair	Because it's both Fair and not fair sometimes people's payments don't got though due to certain terms of banking	Neither biased nor unbiased	Cause it depends on also their credit but also their payments 	Neither useful nor unusable	Useful but not useful because it's just an example 	Models X and Y are equally fair	Because they're all based on just an example chart	Models X and Y are equally biased	You're going off of people's charts versus checking the whole credit history their whole payment process the renting's history you can't go based on a chart	Models X and Y are equally useful	They're almost identical just a little bit changed on in model X so II think that they're the same	0	I want to choose either of them honestly I mean I feel like I couldn't put my bias or my choosing between 2 different 2 different by almost identical charts	Both ${e://Field/pref_model} and Z are equally fair	Same thing they're all both equal just a little bit different if an z	Both ${e://Field/pref_model} and Z are equally biased	Their butt butt butt they're this just the chart they're examples they're not really the same	0	Same	model Y	minority	Caucasian	1.0	1.0
61536cbb3240efe4a10f70ec	rent	1	There would probably be a loss of income that might not be recovered.	0	It seems that would be an easily remedied situation. The renter would likely still pay.	0	Again, it seems that it would affect few people.	0	It seems that it would cause problems on a more individual basis.	Yes	Yes	Yes	Advantaged	Mildly unfair	There appears to be a large discrepancy in the payments allowed by caucasions vs those of non caucasions 	Mildly unbiased	It simply shows the error rate of processed payments	Mostly unusable	It’s a good tool to show what payments are turned down as well as to illuminate possible racism	Neither fair nor unfair	That there is no difference based on race seems fair. But only allowing 53% of payments to be processed seems low and unfair.	Acceptably biased	Yes, it seems biased because a race appears to be singled out.	Mostly useful	It shows no racial profiling, but lets the developer know that their payment acceptance rate is low.	Probably model Y	The probability of accepting payments is the same. Mistakes made cannot be controlled, only measured, after everyone has the same payment opportunity.	Probably model X	The acceptance of payments from white people is much higher.	Models X and Y are equally useful	Any data gathered can be useful as long as it’s presented in a no biased way (even if it reveals a biased system)	1	Because it appears to not be racist 	Probably model Z	Even though the acceptance of “white” payments is higher, the technology has appeared to choose fraudulent charges equally (probably not based on race)	Probably ${e://Field/pref_model}	The disparity in results of wrongly accepted payments is so large, it appears that it was biased by trying to not be prejudiced.	1	I think it might be a more accurate picture of the true data	model Y	minority	Caucasian	0.5	0.0
5efe1ba6649c910ffdd56c6a	rent	1	You also dont want fraud	2	It gives people less of an opportunity then they should have	0	Its probably not as bad in the long run	1	Im sure on group is more affected than others.	No	No	Yes	Advantaged	Mildly unfair	It allows far more payments from white people than not	Very biased	It seems to trust white people way more	Mostly unusable	Seems like it has unusable bias	Mildly unfair	It looks like it makes way more mistakes for the Caucasian part of the chart	Acceptably biased	Once again it is making more mistakes for one part of the model	Neither useful nor unusable	Not sure how reliable it can be	Probably model X	Payments should be accepted equally, give people the chance instead of denying them, mistakes can be fixed afterwards.	Probably model X	It is doing more assumption work	Probably model Y	It finds and corrects more mistakes	1	Like i said, give people a fair chance to make a payment	Probably ${e://Field/pref_model}	I still think you should be able to make a payment	Probably model Z	It accepts payments more from one group	-1	Same reason as before	model Y	minority	Caucasian	0.5	1.5
60fd99351090788eaebbda7b	rent	2	Not receiving payment 	2	Not receiving money/worrying about eviction	2	Stress of not getting money	1	Society: economic community. Less money coming in and stress about why it was denied	No	No	Yes	Advantaged	Mildly unfair	The model allowed more payments of one group when they both had the same percentage of bad payments	Very biased	Separating my race 	Neither useful nor unusable	I think it provides good information but should look at different characteristics 	Very fair	Allowed the same anoint of payments	Mildly unbiased	Allowed same amount of payments regardless of race	Neither useful nor unusable	Should have not allowed as many payments through since they came out fraudulent 	Definitely model Y	Amount of accepted payments did not change based on race	Definitely model X	Race was a determining factor	Models X and Y are equally useful	They both have flaws	2	More fair	Probably ${e://Field/pref_model}	Equal number approved	Probably model Z	Different approvals	-1	Less racially biased	model Y	minority	Caucasian	2.0	1.5
5af20a80b300870001fd2ecb	frauth	2	non-staff can get into areas and possibly cause issues and confusion	0	it allows staff to enter and does not allow non-staff	0	most news outlets dont care about someone accidently wandering around in a hospital	1	there could be outrage that non-whites are routinely not recognized, society is twitter verse and news outlets	No	Yes	Yes	Advantaged	Neither fair nor unfair	makes an equal amount of mistakes for both groups	Acceptably biased	the probability is skewed	Neither useful nor unusable	not sure if the amount of mistakes is acceptable	Mildly unfair	it thinks non-whites are less likely to be doctors	Acceptably biased	there seems to be more white doctors for the training photos	Mostly useful	it gives data that can be used to fix the problem	Models X and Y are equally fair	they both tend to favor whites in different ways	Probably model X	X seems to think more doctors are white	Probably model Y	it shows a more clear difference in access vs mistakes	1	It is more likely to correctly allow a non-white access	Both ${e://Field/pref_model} and Z are equally fair	both are skewed	Both ${e://Field/pref_model} and Z are equally biased	both have boas in a different way	0	I'm not sure which one would be easier to use in practice	model Y	minority	Caucasian	1.0	0.5
60fe73f8c8a77cdc4b4d9d4a	frauth	2	It's a potentially dangerous situation. One wouldn't know if the person getting in wants to commit violent acts 	2	Of required hospital staff can't get in, people won't have access to the healthcare they provide 	2	Everyone (men,women,children, all races) could be put into a potentially dangerous situation if something new was granted access 	2	We need access to hospital staff that could save our lives 	Yes	No	Yes	Disadvantaged	Mildly unfair	Preference over Caucasian	Very biased	There is a clear preference 	Mostly unusable	Rate of error is high 	Acceptably fair	There is not preference of race 	Very unbiased	There is no preference	Very useful	Mistake percentage is fairly low 	Definitely model Y	Equal access to Caucasian and non Caucasian	Definitely model X	Clear preference 	Probably model X	Less percentage of error 	1	I'm a non Caucasian 	Definitely ${e://Field/pref_model}	No preference between Caucasian and non Caucasian 	Probably model Z	More of a preference towards Caucasian 	-2	I'm not Caucasian	model Y	minority	Non-Caucasian	2.0	2.0
61097db4d04095b7e7d43c11	icu	0	To mistakenly be given an ICU bed that you don't need is probably not going to hurt you.	2	To mistakenly NOT be given an ICU bed that you do need could result in avoidable death.	2	Society would be other patients who might need beds. If too many people are mistakenly given beds that DON'T need them, then society might be short ICU beds for people who DO need them. Which could have severe consequences for society.	1	Society would be other patients who might need beds. If beds go unfilled, that leaves more space for other members of society.	No	Yes	Yes	Advantaged	Acceptably fair	The beds seem pretty evenly split, considering population demographics.	Acceptably biased	people are biased, people are the ones who create models. If the model is based on past ICU needs, then any bias in past ICU placements would filter into the model.	Very useful	Seems like a good, non-emotional way to allocate beds	Acceptably fair	The ICU beds seem fairly even shared.	Mildly unbiased	Almost all models are biased because people are biased and people are the ones who create models.	Very useful	Seems like a good, non-emotional way to allocate ICU beds	Probably model Y	Seems like a more even allocation of beds	Models X and Y are equally biased	people create models, therefore all models are biased. 	Models X and Y are equally useful	They both seem to fairly allocate beds	1	Seems a bit more equal in allocation of beds	Probably model Z	it's a bigger deal to make a mistake denying a bed, than to make a mistake allocating a bed. Model Z has a better ratio of mistakes among denials	Probably ${e://Field/pref_model}	There are way less mistakes among caucasian denials than non-caucasian denials	2	Seems less biased against non-caucasians	model Y	minority	Caucasian	1.0	1.5
60fed37592fa85f71f75952a	frauth	1	Identity theft is a concern but I wonder how many people would actively attempt to do that? likely small. identity theft is no small crime but maybe im optimistic in thinking it would rarely happen	1	Maybe im not well versed in the depths of criminal activity that goes on from stealing peoples health information but it feels the likelihood of someone stealing someones info through this system is minimal and the consequences small	1	"same answer as the last page- 
fear of identity theft = quite bad 
likelihood = not too high?
"	0	It would create little impact, it doesn't change much more than a few minutes of time in a day 	Yes	Yes	Yes	Advantaged	Mildly unfair	"I'm a bit confused as to whether the probability of granted access = properly granted access for medical personally or granted access to anyone of that demographic, even non medical staff? 
I declared it mildly unfair because it seems to have an even rate of error between both parties so it doesn't feel entirely unfair BUT if the data above is in fact declaring it confuses caucasians faces more and therefor allows access to non-medical caucasian folks regularly then it is quite unfair/bad system. "	Very biased	If I understand correctly and its letting less non-caucasian folks in but erring at the same rate it is quite biased in that it seems to better serve a caucasian demographic. 	Mostly unusable	I suppose in caucasian heavy facilities- which I suppose are disproportionately greater than a diverse facility in USA (sad)- it could be useful but how discouraging to non-caucasian folks who do hold space in these facilities. I say, craft a space that hold room for change and diversity and this seems to halt progress to that idea by predominantly serving the same community that exists. 	Acceptably fair	Even the gap! Advance tools that progress, support, validate non-caucasian folks. I do think its flawed but in my opinion, non-caucasians experience much more strain on their mental health than caucasians in the work place. they deserve the recognition and ease. I absolutely find this more fair than the previous model. 	Acceptably biased	If both models are biased but towards different groups I believe the group with greater strain in every other sphere of life deserves even just one ounce of ease. Anywhere they can get the break. People with privilege should step down so others can step up to meet them. This is one tiny tiny step. 	Neither useful nor unusable	Again, this is hard to define. A facility with an overwhelming caucasian population would find less benefit from the use but I don't find it unusable.	Probably model Y	It becomes a race issue and I believe the marginalized group should be supported. 	Models X and Y are equally biased	I think they're equally biased but I believe in the bias of Y rather than X. 	Probably model Y	Considering there is a slight majority of caucasian doctors, my inclination was to say that x is probably more useful as it serves the majority but when I looked up a quick stat sheet on doctor demographics, seeing that it is a slight majority I think model Y is the better investment/use. People of color are slowly holding spaces they've always deserved to a greater degree so in theory, non-caucasians could hold a majority in the very near future. 	2	same as above. 	Probably model Z	46% is a large amount of error, adding more difficulty to a marginalized group with already existing pressures/expectations etc 	Probably ${e://Field/pref_model}	same as above 	1	same as above 	model Y	minority	Non-Caucasian	1.0	0.5
6160d68ce187e1c1d6b50007	icu	2	if these predicted percentage that will need ICU support is inaccurate you are reserving ICU beds witha chance that they will not need it	2	If you mistaken that someone will not need ICU, give away the bed , then they need it there is an obvious issue	2	"""Society"" being ill patients it is a significant issue due to the fact that they might waste an ICU bed"	2	No ICU bed for someone who needs it from society(ill people) means death	No	No	Yes	Advantaged	Very unfair	The percentage of mistakes for non-Caucasian is nearly all the percentage of the non-Caucasian probability they will need it. If the mistake percentage ends up accurate that's nearly all the non-Caucasian patients that could be affected if this model is chosen. On the flip side if those chosen do not need to be put into the ICU then its a wasted bed.	Neither biased nor unbiased.	A computer shouldn't have a bias	Mostly unusable	I think its too simple of a model with not enough to determine who will need a bed	Very unfair	If all of the Caucasians were put into ICU and then didnt need it then theres a probelm.	Neither biased nor unbiased	I dont think its biased, it depends on where its getting its info	Neither useful nor unusable	I think their specific diseases/illness should be used rather than their ethnicity	Probably model Y	Less mistakes 	Models X and Y are equally biased	They shouldnt use race as a way to firgure out who would be in the ICU	Probably model Y	Seems like less percentage of mistakes	1	Seems like it may be a bit more accurate	Probably model Z	equal amount of percentage of mistakes	Probably ${e://Field/pref_model}	higher mistake of non-caucasian	1	seems more accurate	model Y	minority	Caucasian	2.0	2.0
5d3b9f531850400001d2bac9	rent	2	They will be out money if the payment is fraudulent.	0	Their life is not directly affected by not being able to rent. 	0	They will be the ones making the fraudulent payment, obviously could be in legal trouble after that.	2	The society will not be able to rent and could be homeless if they do not have another option. Society is the person trying to rent.	Yes	Yes	Yes	Disadvantaged	Mildly unfair	It has the same mistake percentage for caucasian versus non caucasian but I do not think race should be a factor in its judgement. 	Very biased	It is using race as a factor to make its decision, I do not think that is right.	Mostly useful	It is helping weed out potential fraud, but its parameters are unfair.	Acceptably fair	Since the chart has the same percentage for caucasian versus non caucasian applicant, I do not think race went into the algorithim. 	Mildly unbiased	The percentage for race was even.	Mostly useful	It is preventing potential fraud.	Definitely model Y	Race is not going into the decision.	Definitely model X	Model X is factoring in race.	Definitely model Y	Their percentage of mistakes is less, and race is not a factor in the decision.	2	I would not want to be a factor, and had 1 percent better rate of mistakes.	Probably ${e://Field/pref_model}	Has an even distribution for applicant of different races.	Probably model Z	Has a higher percentage of accepting non-caucasian applicants.	1	Has a lower mistake rate.	model Y	majority	Caucasian	1.0	1.0
614230bae101d5f2af19d6c7	rent	2	For this, the person who owns could end up losing money. 	2	Depending on the renters situation, if they are denied that their payment goes through, they could possibly look at being kicked out for the mistakes of the system. 	2	As the owners being the society, it could cause trouble in the economic world of renting. they could loose out on money that they should have gotten. 	2	As for the society, I look at those are renting whether is be a home or a apartment. If the payment was mistakenly denied, it could end up looking bad for those who are renting and put them in a tough spot. 	Yes	No	Yes	Advantaged	Mildly unfair	Unlike Model Y, the percentages are closer to eachother when looking at the mistakes. However, there is a 26% difference in white people to non white people. That is a pretty big number. 	Acceptably biased	As stated above. 26% is a pretty big difference. White people are still getting accepted higher than non white. 	Neither useful nor unusable	I feel like it could be useful if it was not showing favor. 	Mildly unfair	The mistakes has gone down rather low among the non-caucasian. Meaning that white people are showing lesser percentage than non white. Seems pretty bias to white people. 	Very biased	As stated above, the percentage barely budged towards white people and dropped 24% towards those who are not white. 	Mostly ununsable	Seems the data put in is only favorable to white people. The data could be completely wrong and therefore unusable. Especially since it is showing favor to white. 	Probably model Y	Model Y is more favorable because the percentage of allowed payments are the same. 	Probably model X	There is a 26% difference in Model X when it comes to accepted payments. Almost like the non white are barely getting a chance to even be accepted. 	Probably model Y	Model Y could be more useful simply because the percentage of acceptance is the same compared to the difference in Model X.	1	As stated previously, the payment of acceptance, to me, looks better. 	Probably model Z	They both are pretty unfair, but because they have the same denied payments, Z is better. 	Probably model Z	Z has a lower percentage of acceptance towards non white. 	0	I believe that they are both unfair and I would not want to choose either. They both are biased towards white. 	model Y	minority	Caucasian	2.0	2.0
614eacca1ae6e91a34e9658c	rent	1	This is unlikely to impact an individual as significantly as a mistaken denial, but it depends on the source of the payment and how quickly the issue is noticed	2	This presents a severe hassle to the renter, who needs to work with the appropriate parties to resolve this issue, despite likely already having a busy schedule.	0	For this purpose, society can be defined as the inhabitants of the city the renter lives in. A single fraudulent payment has little impact on the rest of society.	0	For this purpose, society can be defined as the inhabitants of the country the renter lives in. One denied transaction has little impact on the whole of society.	Yes	Yes	Yes	Disadvantaged	Mildly unfair	It has a more fair mistake rate than Model Y but considering the racial disparity between payment acceptance I think I would need more information about the denied payments and that mistake rate before I could make a proper judgement. Considering only the information provided, it doesn't look great.	Acceptably biased	"Same reasoning as why I think it is unfair, but I'll add that ""acceptable"" is not exactly how I would describe any bias. However, I can't call it ""very biased"" without more information."	Mostly unusable	It will not look good for the company if anyone notices the racial discrepancy. Some businesses may be willing to take that risk, but I wouldn't.	Very unfair	It seems to allow fraudulent payments from Caucasian people more often, hinting that it is a harsher judge of non-Caucasian people	Very biased	For the same reason: allowing fraudulent payments from Caucasian people more often implies that it is a harsher judge of non-Caucasian people	Mostly ununsable	Deploying biased machine learning models seems to be the standard business practice now, but I doubt it will be much longer.	Probably model Y	Compared to Model X, Model Y seems less likely to reinforce historical racial disparities.	Models X and Y are equally biased	It is hard for me to say for sure with this information.	Probably model Y	Model Y because I also think it is likely more fair.	1	Model Y seems like the better choice with the information provided, mostly because it seems like it may be more fair.	Both ${e://Field/pref_model} and Z are equally fair	I can't decide. I feel like this could be mathematically determined if I knew how to do that kind of math	Probably ${e://Field/pref_model}	My original suspicion that it is a harsher judge of non-Caucasians appears to have been correct.	0	I've seen more information about Y than Z, and I would hold out to know more about Z before deciding between the two.	model Y	minority	Caucasian	0.5	1.0
6120d602c1976a4114dcb110	rent	2	Because it causes harm to the person who is being stolen from. 	2	This can cause them to lose their home, their shelter and it affects everyone else that lives in the house. 	2	This will affect the individual along with their family. It can cause stress on the family. For example missed car payments, or them even loosing their own shelter because they don't have the funds to have a roof over their head. 	2	This can affect the other people living in the house like their kids. And it's a domino affect, it will cause stress to everyone else. For example the individual could be stressed and the kids feel stressed therefore they perform poorly in school. 	Yes	No	Yes	Disadvantaged	Very unfair	Because they are basing it upon race. There is bad in every race so why even use that as a category and single them out? And it's just Caucasian and then all the other races in one category, that is so racist, unfair, and wrong. 	Very biased	Again, it's still singling minorities out from the Caucasians. Why are they even doing that? Like Caucasians are different than other people? 	Completely unusable	Besides that it's morally wrong I think it would be inaccurate. I say this because there are equal amounts of each race that could possibly use fraudulent payments, I've seen it with my own eyes. 	Neither fair nor unfair	It's fair in a way that it gives everyone an equal chance. But still why is it divided into two categories? To make it completely accurate they should divide it into past history and then maybe some sub categories like male and female. And if they want to be racist they can even do a sub category beyond that like Caucasian, black, Asian, Hispanic. 	Very biased	I don't think it would be accurate per everything I said above and before. 	Neither useful nor unusable	It could be useful, yes. But to be completely useful and accurate they need to go by past history and then sub categorize that. 	Probably model Y	I'm going by the probability of allowing payments. Since they are both equal that would be the choice that is most fair to both categories, but I don't think that any of these models are fair. 	Models X and Y are equally biased	Because they are basing it off race. It's even more biased that they only separated by it into two categories like white people are different than everyone else. 	Probably model Y	Because both categories get an equal chance and if they detect fraud then they could go from there. 	0	Because it's biased and racist. Since computers are so intelligent, I would think they would come up with a better plan. 	Probably ${e://Field/pref_model}	Because they are allowing equal payments 	Both ${e://Field/pref_model} and Z are equally biased	Because it's being judgemental to both categories. 	0	I would come up with a better solution: past history, male and female, and then sub categories like age and then each individual race. 	model Y	majority	Caucasian	2.0	2.0
60fcb64c52cb3e4c9ec0c76b	frauth	2	mistaken access could leave the individuals information vulnerable, and medical records are private information.	0	the staff should know their password to access the system, so at most it is an inconvenience and may take an extra minute	2	access to a hospital system could put so many people's information at risk without them knowing, and the impacts could be far reaching 	1	putting in the password could cause some efficiency issues in the hospital, so patients aren't treated in the most effective way possible. 	No	Yes	Yes	Advantaged	Mildly unfair	there is a large gap in the probability of granting access	Acceptably biased	it does have an issue with probability of granting access but there is no difference in the rate that it makes mistakes.	Neither useful nor unusable	leaning slightly more to useful but not mostly useful. There are still a fairly large percentage of mistakes.	Acceptably fair	it has an identical probability of granting access, but there is a discrepancy in the ratio of mistakes.	Acceptably biased	there are more mistakes when identifying Caucasian individuals than non Caucasian individuals 	Mostly ununsable	it makes a significant number of mistakes in identifying doctors.	Probably model Y	there is not difference in the probability of access being granted 	Probably model X	there is a large gap in the probability of access being granted	Probably model Y	it is more fair and has a lower average rate of mistake	1	It seems more fair and less biased overall	Probably model Z	it makes an even number of mistakes between the groups	Probably ${e://Field/pref_model}	there is a very large gap in the number of mistakes made	1	even though the probability of y is more even, z makes fewer mistakes overall 	model Y	minority	Caucasian	2.0	0.5
617078fb6641a57cb762a401	frauth	2	Very unsafe and scary.	2	It's very important to be able to recognize an authorized staff as it can be unsafe.	2	the community where the hospital is can have unsafe consequences.	2	Everyone in and around the hospital will be impacted if the system isn't accurate.	Yes	Yes	Yes	Disadvantaged	Neither fair nor unfair	not sure.	Mildly unbiased	not sure.	Mostly useful	looks like it works for the most part.	Mildly unfair	unsure.	Very biased	unsure how to answer.	Mostly useful	It works.	Definitely model Y	It looks more reliable based on granted access.	Probably model X	I just think so.	Definitely model Y	they grant access evenly.	2	Because it's more workable and unbiased.	Probably model Z		Probably ${e://Field/pref_model}		1		model Y	majority	Non-Caucasian	2.0	2.0
611d42472542f0906bc3b54b	icu	2	If ICU support is given when not needed it can result in more financial cost to the individual patient's care.	2	From an individual standpoint it could mean the difference between life and death.	2	One individual's care cost will be felt as a whole on society if it puts too much of a financial strain on the individual.	2	For the family of the individual, if that person dies, the loss to the family unit and society as a whole can be immeasurable.	No	No	Yes	Disadvantaged	Very unfair	The predictions for caucasian patients were close but non-caucasian patients had a very large discrepancy.	Very biased	they prediction for non-caucasian has a much larger margin of error than for caucasian patients.	Mostly unusable	For one whole section of patients the data is not close to being as accurate as it is for the caucasian patients.	Very unfair	Again the prediction is skewed when it comes to non-caucasian patients.	Very biased	It overpredicts more towards the non-caucasian patients.	Neither useful nor unusable	For caucasian patients the data can be useful, but is very off for non-caucasian patients.	Models X and Y are equally fair	There predictions each tend to be close to the actual numbers in a certain demographic but each is way off on the other demographic.	Models X and Y are equally biased	They each skew at least 30 per cent off for one demographic and around 8 per cent off on the other demographic.	Models X and Y are equally useful	They are off by similar percentage points for each demographic.	0	Neither one is better or worse than the other and each one is skewed wildly in favor of one set of patients.	Both ${e://Field/pref_model} and Z are equally fair	Their probability was equally as inaccurate for each model.	Both ${e://Field/pref_model} and Z are equally biased	Each model was equally wrong in predicting probability for each demographic of patient.	0	Both had similar percentage points in error.	model Y	majority	Non-Caucasian	2.0	2.0
6111202e9250c75296f411bf	frauth	2	"Hospitals keep a mass amount of confidential documents as well as drugs, metal instruments and other things that shouldn't be that easily accessible. 
"	2	High because it would be inconvenient during emergency situations, causing delays, issues with communication and documents. Hopefully with reliable hospital software, a reset or manual log would be an available alternative. 	2	It puts confidential information, drugs and medical tools at riak.	1	Still very inconvenient but the username/password back up is a great safety net  for when the facial recognition fails.	Yes	Yes	Yes	Disadvantaged	Mildly unfair	The probability of a non caucasian person being granted their access is lower than it is for caucasian people.	Acceptably biased	The machine seems to be more accurate when it's caucasian people utilizing it.	Neither useful nor unusable	I'm not sure if the ratio similarities within the mistakes of access is better or worse than one being higher/lower than the other.	Mildly unfair	The probability of granting access is where it should be however the mistake only among granted access makes it slightly more difficult for caucasians to avoid those mistakes from model Y.	Acceptably biased	Model Y's percentage of mistakes with granted access seems relatively high and only applies to one race.	Mostly useful	The improvements needed seem minor regarding the mistakes with access.	Probably model Y	I'm not confident in my answer but the probabilities for both caucasian and non caucasian people being granted their access is even.	Probably model X	This is a hard comparison but maybe because of the ratio of mistaken granted access.	Models X and Y are equally useful	I'm struggling trying to decipher which is better.	1	That's what my gut is telling me.	Both ${e://Field/pref_model} and Z are equally fair	I'm indecisive and don't know which ratio is more important. Rather, which ratio differences can afford to be prioritized.	Both ${e://Field/pref_model} and Z are equally biased	See above comment.	-1	Accuracy is vital and should be implemented.	model Y	minority	Non-Caucasian	2.0	1.5
6167bd607a81dede537aa23b	frauth	2	Unappropriate access to medical records	0	They still have the ability through passwords to gain access	2	The institution (Hospital and society overall) do have to be concern about what is wrongfully being accessed	0	The institution (Hospital and society overall) do not have to be concern with personal and private information being access	Yes	Yes	Yes	Disadvantaged	Mildly unfair	A larger percentage of one group were allowed access over the other, even though there were the same percentages of mistakes. Although I would want to know the overall number of subjects. 	Neither biased nor unbiased.	I cannot speak to the bias of the model because it is a machine and is doing what is was programmed to do. 	Neither useful nor unusable	I cannot say how useful because the mistake percentages are equal on both sides. 	Mildly unfair	Because we are having a higher  percentage of mistakes with one group than the other. 	Neither biased nor unbiased	Cannot say it is a machine doing what it was programmed to do. 	Neither useful nor unusable	I believe the purpose and idea behind it is very useful but application of the idea needs work.	Models X and Y are equally fair	They are both problematic in their opperations.	Models X and Y are equally biased	They are both problematic in their opperations.	Models X and Y are equally useful	I do not feel either are useful. 	0	Because the level of mistakes on both sides are significant. 	Definitely model Z	Because the same group-wise percentage were denied access	Definitely ${e://Field/pref_model}	The group-wise percentage are severely different	1	The overall percentages denied 	model Y	minority	Non-Caucasian	2.0	0.0
60fda319d6d843f4541fb694	icu	2	This could put someone’s life in danger if they are not given the care they need	1	They might not know the reasonings behind it	1	"Because this will impact healthcare for a lot of people 
Society being health care individuals ands well as people in society "	1	People who aren’t involved in health care probably won’t care as much 	No	Yes	Yes	Disadvantaged	Mildly unfair	The probability is not equal 	Acceptably biased	It is just starting facts about the probabilities but it is also stating that Caucasian individuals will have a harder time predicting the probability.	Mostly useful	It is useful so that the company can talk about the results and better look at ways to get their workers to be equal in predicting ICU use 	Very unfair	The amount of mistakes for non-Caucasian ICU supports is way higher than Caucasians which proves that there is some sort of bias and unjust and unfairness with this model 	Acceptably biased	Due to the rates of non caucasians being higher than Caucasians 	Mostly useful	This can be useful to let the company know what is going on and how this system is flawed and implement and talk about ways to fix it 	Probably model Y	The percentages are closer together 	Probably model X	Because there is a difference in how well certain people can predict icu use	Models X and Y are equally useful	They are both useful to learn from and to get information from to better teach and advance people working	1	Because it gives and equal amount of predication to both parties and still lays out the mistakes fairly 	Probably ${e://Field/pref_model}		Probably model Z		-1		model Y	majority	Non-Caucasian	1.5	1.0
6108957f175a2eb650221b8e	rent	0	They can reverse the payment	2	Housing is a right that shouldn't be complicated by flawed technology.	0	The company will not be bankrupted by fraudsters so there is little risk when large corporations like that take losses	0	Society at large would likely be indifferent 	No	Yes	Yes	Disadvantaged	Very unfair	Caucasians have the same amount of risk for mistake but get payments allowed at a higher rate	Very biased	It implies that non caucasian folks are less likely to get approved payment than caucasians even though the mistakes are made at the same rate. 	Completely unusable	Will result in biased practices	Very fair	It allows an even probability of approved payment regardless of race	Very unbiased	Results are consistent with fair practices regardless of race.	Very useful	Allows to preemptively filter out some applicants without bias	Probably model Y	The difference in the rates is less severe in Y	Definitely model X	There is a standard set that isn't matched for all races	Models X and Y are equally useful	Both have implied biases	0	Both are more flawed than I would be comfortable with	Definitely model Z	It is more fair but, not fair in general	Definitely model Z	There is a heavy caucasian leaning bias	0	Both are more flawed than I am comfortable with	model Y	minority	Non-Caucasian	0.0	1.0
610e628bb9ef91fb413c9865	frauth	2	The unauthorized individuals are just as dangers as strangers walking into your house.	2	There are many sick people who just want to kill others and they will do different things to achieve that goal.	2	I consider all people part of society because every person does something that affects how others live. It is a dangerous thing to allow non-medical personnel into security-secured places.	2	I consider all people part of society because every person does something that affects how others live.	Yes	Yes	Yes	Disadvantaged	Mildly unfair	The bottom one is an equal amount of percentage while the top one has different scales	Mildly unbiased	The amount of non-Caucasian acceptance is very low compared to the amount of Caucasians. 	Neither useful nor unusable	I honestly think it only speaks to the one-sided percentage. Although the amount is equal when it comes to mistakes made when access is granted.	Mildly unfair	I think that everything cannot be determined by machines. Everyone makes mistakes but the machine is showing that one race can make the most.	Neither biased nor unbiased	In the first one, I honestly thought that it was biased because the chances of mistakes made by the other race increased. However, it seems that the computer has done the opposite.	Neither useful nor unusable	A person's worth cannot be determined by a machine. Even machines can be wrong	Models X and Y are equally fair	Because they both hold an equal percentage of pass and failure.	Models X and Y are equally biased	The ones with different percentages for both mistakes and access granted.	Models X and Y are equally useful	If both machines showed an equal percentage then they are useful	0	I would choose neither because a machine can also make mistakes.	Both ${e://Field/pref_model} and Z are equally fair	They both show the equal percentage	Both ${e://Field/pref_model} and Z are equally biased	The percentages that show the difference between each race	0	The machines can be wrong and mistakes do not need to be tested by them	model Y	minority	Non-Caucasian	2.0	2.0
5e730aa954a7b42617d99c5a	frauth	2	This is very dangerous and could cause a lot of damage	0	It’s an inconvenience but it isn’t really a big problem 	2	It’s very dangerous, medical records can be tampered with	0	It’s just another safety precaution 	No	Yes	Yes	Disadvantaged	Neither fair nor unfair	The same amount of mistakes.	Acceptably biased	Caucasian has higher acceptance rate.	Mostly useful	If unbiased, it has a lot of potential 	Mildly unfair	The mistake percentage is too high.	Acceptably biased	Makes more mistakes with Caucasians.	Mostly useful	Higher success rate than previous model	Models X and Y are equally fair	I’m actually not sure now that I’m looking at both… I think both models still need work.	Models X and Y are equally biased	Both biased in different areas.	Models X and Y are equally useful	I think more work needs to be done before putting them in use.	0	Neither are completely reliable 	Both ${e://Field/pref_model} and Z are equally fair	Neither are completely fair.	Both ${e://Field/pref_model} and Z are equally biased	They both are biased in some areas.	0	Neither I wouldn’t use a biased program	model Y	minority	Non-Caucasian	2.0	0.0
6163ae32f4a006501a1d6448	rent	2	It is losing money for no reason, which is a big deal.	1	It is significant enough to have to contact the person in order to make sure the payment is made.	1	It is moderate considering that it could happen to other members of society. The society would be people in a community.	0	It is low since denying a payment does not affect society, just an individual. The society would be people in a community.	No	Yes	Yes	Disadvantaged	Mildly unfair	The model is biased towards non-caucasians.	Acceptably biased	The model is biased towards non-caucasians.	Mostly unusable	The model is biased towards non-caucasians.	Mildly unfair	It is biased towards caucasians.	Acceptably biased	It is biased towards caucasians.	Mostly ununsable	It is biased towards caucasians.	Probably model Y	The difference in bias between caucasians and non-caucasians is less in model y.	Probably model X	The difference in bias between caucasians and non-caucasians is less in model y.	Probably model Y	The difference in bias between caucasians and non-caucasians is less in model y, so it is slightly more reliable.	1	The difference in bias between caucasians and non-caucasians is less in model y, so it is slightly more reliable.	Probably ${e://Field/pref_model}	The difference in bias between caucasians and non-caucasians is less in model y.	Probably model Z	The difference in bias between caucasians and non-caucasians is less in model y.	-1	The difference in bias between caucasians and non-caucasians is less in model y.	model Y	majority	Non-Caucasian	1.5	0.5
60ce20b540d291328bf5b356	icu	2	ICU is important to get a person to live longer	2	This prediction denies the person from receiving help	2	Society means every individual who is a part of a country or city. People who are mistakenly given ICU support could take the place of someone who needs it	1	Every person who visits the ER can be mistakenly denied ICU support	No	No	Yes	Disadvantaged	Mildly unfair	not sure why one group will more likely require ICU	Neither biased nor unbiased.	This does not give me much information on why one group was more likely to require ICU	Mostly unusable	This does not tell me why one group id more likely to require ICU	Mildly unfair	Some people who didn't need the support were given the ICU.	Mildly unbiased	I'm not sure how many mistakes were made for those who weren't given ICUs.	Mostly ununsable	What matters is how many people who did need it didn't get the ICUs because of the errors presented	Probably model Y	I think its fair to say that both  groups are equally as likely to require the ICU for support	Probably model X	Not sure why group will most likely require the ICU	Models X and Y are equally useful	I'm not sure what the negative affects are	1	I think its fair to say that both  groups are equally as likely to require the ICU for support	Probably model Z	I think it's unfair one group is impacted the most for denial of ICU use	Probably ${e://Field/pref_model}	why is one group being affected more by denials than the other one.	1	they are equally affected by denying them an ICU	model Y	minority	Non-Caucasian	2.0	1.5
615ecd9df93b5467b032282b	frauth	2	Stolen data	2	People’s privacy is at stake	2	Having access to anyone’s personal health care records is a solid no. There should be a margin of 1-3% error results.	2	Hoops violation 	Yes	No	Yes	Disadvantaged	Very unfair	It should be equal	Very unbiased	Access gained 60% Caucasian. It should be fair or close to similarity in performance.	Neither useful nor unusable	It’s flawed	Very unfair	Pr yo babii ty is too high. Not trust worthy.	Neither biased nor unbiased	It’s just data 	Completely unusable	Only 50%	Probably model Y	Granted access is equal	Definitely model X	It’s software seems to be better equipped for a certain group	Definitely model X	Less granted access mistakes	2	Better ergonomic Enviroment for the workers. 	Definitely ${e://Field/pref_model}	Equal acess	Definitely model Z	"Unfair poll e
Resultz"	-2	It works better 	model Y	minority	Non-Caucasian	2.0	2.0
6176377745c7352033f281df	frauth	2	Mistakenly granting access to unauthorized users in any setting can and will spell trouble in any significant way.	0	Requiring a usename/password is only an inconvenience to staff.	2	Society includes people, animals, plants, and the universe. Mistakenly granting access to unauthorized personnel is high impact because it puts people at risk.	0	I consider everyone in the world to be part of society. Including plants, animals, the universe, etc. . . It is still a low impact to fail to recognize hospital staff because it is only a minor inconvenience to need to type a username and password.	No	Yes	Yes	Advantaged	Mildly unfair	It has a significantly higher chance of granting access to people of color over white people.	Acceptably biased	People of color seem to be more easily detected by the device. It needs some work done for it to behave more equally towards both whites and people of color.	Mostly useful	The device is not totally useless given how it grants an equal amount of access to unauthorized personnel. It may just need a few tweaks done. 	Acceptably fair	There's a 50/50 chance of the device granting access to either a Caucasian or Non-Caucasian.	Very unbiased	I see no signs of bias.	Mostly useful	The device doesn't seem to unfairly distinguish people of color from white people.	Definitely model Y	For Model Y, there's an equal amount of chance that either a Caucasian or Non-Caucasian will be granted access, while Model X is significantly biased towards granting access to Non-Caucasians. Also, Model Y has an overall 69% chance of granting access to unauthorized personnel, while Model X has a 70% chance. Making model Y the most fair and superior model by being the least biased and also by being 1% less likely to grant unauthorized access.	Definitely model X	Model X is the most biased due to it having a higher probability of granting access to Non-Caucasians over Caucasians.	Definitely model Y	As previously stated, it is the least biased and safest.	2	Model Y is the least biased/superior model.	Definitely ${e://Field/pref_model}	It's got an equal chance of granting access to either a Caucasian or Non-Caucasian.	Definitely model Z	It's got a significantly higher chance of granting access to a non-Caucasian.	-2	It's less biased despite it rejecting more Caucasian authorized personnel.	model Y	majority	Non-Caucasian	2.0	0.0
6109e6edf61c274f2d101c23	icu	0	I don't think it would affect the individual very much. If anything they will be relieved that they will not have to go to the ICU.	2	I would be very impactful because the ICU is where people who are in serious condition are sent and if someone that needs it doesn't get sent there it could seriously harm them.	2	Society is everyone around the individual who is being directly affected. Society will be impacted greatly because there will be other individuals who need the help who aren't getting it because you were mistakingly put there.	0	Society is everyone around the individual who is directly being affected. This opens up room for someone else, so society won't be too worried.	No	Yes	Yes	Disadvantaged	Neither fair nor unfair	I do think it's fair because it picked people from both categories to get put into the ICU.	Acceptably biased	I do think it is biased towards Caucasians because it let more of them into the ICU, but they had the same amount of people who were mistakingly put there.	Mostly unusable	In this case it wasn't very useful. A great chunk of the non-caucasion people who were sent to the ICU weren't even supposed to be there so it was just messing with everything else. 	Neither fair nor unfair	I think it is neither fair or unfair because although it did make some mistakes, it also had some good predictions.	Acceptably biased	I think the model might have been a little biased towards those who are not Caucasian seeming as they had less mistakes among those granted access into the ICU.	Mostly useful	I think it is mostly useful because it can help predict help for those that are needed. It could be very useful if they worked out the kinks and made it more accurate.	Probably model Y	I think model Y is more fair because it let in the same amount of people from both groups.	Probably model X	I think model X might be more biased solely based off the fact that more people of one race were let in, while simultaneously having more mistakes for those that needed to be in there. 	Models X and Y are equally useful	I think they both have pros and cons and can be useful. 	1	This one seemed as the more fair choice overall, so I would have to pick model Y. 	Both ${e://Field/pref_model} and Z are equally fair	I think they both have pros and cons for how fair they are.	Probably ${e://Field/pref_model}	I think model Y is more biased because they let in more Caucasian people when a big chunk of them simply did not need to be there.	1	I think I would pick model Z because it had less mistakes compared to their initial group that was chosen.	model Y	minority	Non-Caucasian	1.0	1.0
61096a016b964e7af149fce3	icu	1	Based on both Models shown I believe this to be so.	0	Based on both models shown I believe this to be so.	1	Taking both models into consideration with Caucasians and non-Caucasians in mind.	0	Taking both models into consideration with Caucasians and non-Caucasians in mind.	Yes	Yes	Yes	Advantaged	Mildly unfair	The probability of predicting required ICU is much higher in Caucasian compared to non-Caucasian.	Neither biased nor unbiased.	No way to tell because if the model X is biased due to not knowing the data used or who program it.	Mostly useful	It can help cut down the amount of time used to evaluate patients.	Mildly unfair	Mistakes among only granted ICU support for Caucasians are almost double that of non-Caucasians.	Neither biased nor unbiased	It's hard to tell.	Mostly useful	In some cases, it could be useful as far as time and the number of patients that have to be evaluated.	Probably model Y	The probability of predicting ICU support require equally with both caucasian and non-caucasian.	Probably model X	The probability of predicting ICU support requires is much higher in caucasian compared to non-caucasian.	Models X and Y are equally useful	I think both models can be useful.	1	The probability of predicting ICU support required is equal with both caucasian and non-caucasian in model Y.	Both ${e://Field/pref_model} and Z are equally fair	Depending on what part of the Model  X or Y it will display some form of fairness.	Probably model Z	The probability of predicting ICU support require is higher in caucasian compared to non-caucasian with model Z.	-1	Model Y is equal on both sides when it comes to predicting ICU support.	model Y	minority	Non-Caucasian	1.0	0.0
